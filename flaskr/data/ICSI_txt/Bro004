Turn 0, B (Professor): OK .
Turn 1, C (PhD): Oh , I don't {disfmarker}
Turn 2, A (PhD): I think I 'm zero .
Turn 3, B (Professor): Wow ! Unprecedented .
Turn 4, C (PhD): Hello , hello , hello , hello .
Turn 5, E (PhD): Ah
Turn 6, F (Grad): Wh - what causes the crash ?
Turn 7, A (PhD): Did you fix something ?
Turn 8, C (PhD): Hello .
Turn 9, E (PhD): Five , five .
Turn 10, C (PhD): Hello , hello .
Turn 11, F (Grad): Oh , maybe it 's the turning {disfmarker} turning off and turning on of the mike , right ?
Turn 12, B (Professor): Uh , you think that 's you ? Oh .
Turn 13, C (PhD): Aaa - aaa - aaa .
Turn 14, F (Grad): Yeah , OK , mine 's working .
Turn 15, C (PhD): OK . That 's me .
Turn 16, B (Professor): OK . OK . So , um I guess we are {pause} um {pause} gonna do the digits at the end . Uh
Turn 17, D (PhD): Channel {disfmarker} channel three , yeah .
Turn 18, C (PhD): Channel two .
Turn 19, D (PhD): OK .
Turn 20, E (PhD): Mmm , channel five ? Doesn't work ?
Turn 21, B (Professor): Yeah , that 's the mike number there , uh {pause} Uh , mike number five , and {pause} channel {disfmarker} channel four .
Turn 22, C (PhD): Two .
Turn 23, A (PhD): Is it written on her sheet , I believe .
Turn 24, E (PhD): No ? Ah ,
Turn 25, D (PhD): Mike four .
Turn 26, F (Grad): Watch this .
Turn 27, E (PhD): era el cuatro .
Turn 28, F (Grad): Yep , that 's me .
Turn 29, E (PhD): Yeah .
Turn 30, A (PhD): But , channel
Turn 31, E (PhD): Yeah yeah yeah .
Turn 32, B (Professor): This is you .
Turn 33, E (PhD): OK . I saw that . Ah {disfmarker} yeah , it 's OK .
Turn 34, B (Professor): Yeah . And I 'm channel uh two I think ,
Turn 35, C (PhD): Ooo .
Turn 36, B (Professor): or channel {disfmarker}
Turn 37, C (PhD): I think I 'm channel two .
Turn 38, B (Professor): Oh , I 'm channel {disfmarker} must be channel one . Channel one ?
Turn 39, E (PhD): Channel {disfmarker} {vocalsound} I decided to talk about that .
Turn 40, B (Professor): Yes , OK . OK . So uh {pause} I also copied uh the results that we all got in the mail I think from uh {disfmarker} {pause} from OGI and we 'll go {disfmarker} go through them also . So where are we on {disfmarker} {pause} on uh {vocalsound} {pause} our runs ?
Turn 41, D (PhD): Uh so . {pause} uh {disfmarker} We {disfmarker} So {pause} As I was already said , we {disfmarker} we mainly focused on uh four kind of features .
Turn 42, B (Professor): Excuse me .
Turn 43, D (PhD): The PLP , the PLP with JRASTA , the MSG , and the MFCC from the baseline Aurora .
Turn 44, B (Professor): Mm - hmm .
Turn 45, D (PhD): Uh , and we focused for the {disfmarker} the test part on the English and the Italian . Um . We 've trained uh several neural networks on {disfmarker} so {disfmarker} on the TI - digits English {pause} and on the Italian data and also on the broad uh {pause} English uh French and uh Spanish databases . Mmm , so there 's our result tables here , for the tandem approach , and um , actually what we {disfmarker} we @ @ observed is that if the network is trained on the task data it works pretty well .
Turn 46, B (Professor): OK . Our {disfmarker} our uh {disfmarker} {pause} There 's a {disfmarker} {pause} We 're pausing for a photo {disfmarker}
Turn 47, C (PhD): Chicken on the grill . Try that corner .
Turn 48, A (PhD): How about over th from the front of the room ?
Turn 49, C (PhD): Yeah , it 's longer .
Turn 50, B (Professor): We 're pausing for a photo opportunity here . Uh . {vocalsound} Uh . So .
Turn 51, F (Grad): Oh wait wait wait wait wait . Wait .
Turn 52, C (PhD): Get out of the {disfmarker} Yeah .
Turn 53, F (Grad): Hold on . Hold on .
Turn 54, B (Professor): OK .
Turn 55, F (Grad): Let me give you a black screen .
Turn 56, B (Professor): He 's facing this way . What ? OK , this {disfmarker} this would be a {pause} good section for our silence detection .
Turn 57, F (Grad): OK .
Turn 58, C (PhD): Mm - hmm .
Turn 59, B (Professor): Um Oh .
Turn 60, F (Grad): Musical chairs everybody !
Turn 61, B (Professor): OK . So um , {pause} you were saying {pause} about the training data {disfmarker} Yeah .
Turn 62, D (PhD): Yeah , so if the network is trained on the task data um {pause} tandem works pretty well . And uh actually we have uh , results are similar Only on ,
Turn 63, A (PhD): Do you mean if it 's trained only on {disfmarker} On data from just that task ,
Turn 64, D (PhD): yeah .
Turn 65, A (PhD): that language ?
Turn 66, D (PhD): Just that task . But actually we didn't train network on {pause} uh both types of data I mean {pause} uh {pause} phonetically ba phonetically balanced uh data and task data .
Turn 67, A (PhD): Mmm .
Turn 68, D (PhD): We only did either task {disfmarker} task data or {pause} uh broad {pause} data .
Turn 69, A (PhD): Mm - hmm .
Turn 70, D (PhD): Um {pause} Yeah . So ,
Turn 71, B (Professor): So how {disfmarker} I mean {disfmarker} clearly it 's gonna be good then
Turn 72, A (PhD): So what 's th
Turn 73, B (Professor): but the question is how much {pause} worse is it {pause} if you have broad data ? I mean , {pause} my assump From what I saw from the earlier results , uh I guess last week , {pause} was that um , {pause} if you {pause} trained on one language and tested on another , say , that {pause} the results were {disfmarker} were relatively poor .
Turn 74, D (PhD): Mmm . Yeah .
Turn 75, B (Professor): But {disfmarker} but the question is if you train on one language {pause} but you have a broad coverage {pause} and then test in another , {pause} does that {disfmarker} {pause} is that improve things {pause} i c in comparison ?
Turn 76, D (PhD): If we use the same language ?
Turn 77, B (Professor): No , no , no . Different lang So {pause} um {pause} If you train on TI - digits {pause} and test on Italian digits , {pause} you do poorly , {pause} let 's say .
Turn 78, D (PhD): Mm - hmm .
Turn 79, B (Professor): I don't have the numbers in front of me ,
Turn 80, D (PhD): But {disfmarker} Yeah but I did not uh do that .
Turn 81, B (Professor): so I 'm just imagining . E So , you didn't train on {pause} TIMIT and test on {disfmarker} {pause} on Italian digits , say ?
Turn 82, D (PhD): We {disfmarker} No , we did four {disfmarker} four kind of {disfmarker} of testing , actually . The first testing is {pause} with task data {disfmarker} So , with nets trained on task data . So for Italian on the Italian speech @ @ . The second test is trained on a single language um with broad database , but the same language as the t task data .
Turn 83, B (Professor): OK .
Turn 84, D (PhD): But for Italian we choose Spanish which {pause} we assume is close to Italian . The third test is by using , um the three language database
Turn 85, B (Professor): W which in {disfmarker}
Turn 86, D (PhD): and the fourth is
Turn 87, B (Professor): It has three languages . That 's including the w the {disfmarker} {pause} the {disfmarker}
Turn 88, D (PhD): This includes {disfmarker}
Turn 89, B (Professor): the one that it 's {disfmarker}
Turn 90, D (PhD): Yeah .
Turn 91, A (PhD): In
Turn 92, D (PhD): But {pause} not digits . I mean it 's {disfmarker}
Turn 93, A (PhD): The three languages {pause} is not digits ,
Turn 94, B (Professor): Right .
Turn 95, A (PhD): it 's the broad {pause} data . OK .
Turn 96, D (PhD): Yeah And the fourth test is uh {pause} excluding from these three languages the language {pause} that is {pause} the task language .
Turn 97, B (Professor): Oh , OK , yeah , so , that is what I wanted to know .
Turn 98, D (PhD): Yeah .
Turn 99, B (Professor): I just wasn't saying it very well , I guess .
Turn 100, D (PhD): Uh , yeah . So um {pause} for uh TI - digits for ins example {pause} uh when we go from TI - digits training to {pause} TIMIT training {pause} uh we lose {pause} uh around ten percent , uh . The error rate increase u of {disfmarker} of {disfmarker} of ten percent , relative .
Turn 101, B (Professor): Relative . Right .
Turn 102, D (PhD): So this is not so bad . And then when we jump to the multilingual data it 's uh it become worse and , well Around uh , let 's say , {pause} twenty perc twenty percent further .
Turn 103, B (Professor): Ab - about how much ?
Turn 104, D (PhD): So . Yeah .
Turn 105, B (Professor): Twenty percent further ?
Turn 106, D (PhD): Twenty to {disfmarker} to thirty percent further . Yeah .
Turn 107, A (PhD): And so , remind me , the multilingual stuff is just the broad data . Right ? It 's not the digits .
Turn 108, D (PhD): Yeah .
Turn 109, A (PhD): So it 's the combination of {pause} two things there . It 's {pause} removing the {pause} task specific {pause} training and {pause} it 's adding other languages .
Turn 110, D (PhD): Yeah . Yeah .
Turn 111, A (PhD): OK .
Turn 112, D (PhD): But the first step is al already removing the task s specific from {disfmarker} from {disfmarker}
Turn 113, A (PhD): Already , right right right .
Turn 114, D (PhD): So .
Turn 115, A (PhD): So they were sort of building {pause} here ?
Turn 116, D (PhD): And we lose {disfmarker}
Turn 117, A (PhD): OK ?
Turn 118, D (PhD): Yeah . Uh {pause} So , basically when it 's trained on the {disfmarker} the multilingual broad data {pause} um or number {disfmarker} so , the {disfmarker} the {pause} ratio of our error rates uh with the {pause} baseline error rate is around {pause} uh one point one .
Turn 119, B (Professor): Yes . {vocalsound} And it 's something like one point three of {disfmarker} of the {pause} uh {disfmarker}
Turn 120, D (PhD): So .
Turn 121, B (Professor): I i if you compare everything to the first case at the baseline , you get something like one point one for the {disfmarker} for the using the same language but a different task , and something like one point three {pause} for three {disfmarker} three languages {pause} broad stuff .
Turn 122, D (PhD): No no no . Uh same language we are at uh {disfmarker} for at English at O point eight . So it improves , {pause} compared to the baseline . But {disfmarker} So . Le - let me .
Turn 123, B (Professor): I {disfmarker} I {disfmarker} I 'm sorry .
Turn 124, D (PhD): Tas - task data
Turn 125, B (Professor): I {disfmarker} I {disfmarker} I meant something different by baseline
Turn 126, D (PhD): we are u Yeah .
Turn 127, B (Professor): So let me {disfmarker} let me {disfmarker} Um , {pause} so , {pause} um {disfmarker}
Turn 128, D (PhD): Mmm .
Turn 129, B (Professor): OK , fine . Let 's {disfmarker} let 's use the conventional meaning of baseline .
Turn 130, D (PhD): Hmm .
Turn 131, B (Professor): I {disfmarker} I {disfmarker} By baseline here I meant {pause} uh using the task specific data .
Turn 132, D (PhD): Oh yeah , the f Yeah , OK .
Turn 133, B (Professor): But uh {disfmarker} {pause} uh , because that 's what you were just doing with this ten percent .
Turn 134, D (PhD): Yeah .
Turn 135, B (Professor): So I was just {disfmarker} I just trying to understand that .
Turn 136, D (PhD): Yeah . Sure .
Turn 137, B (Professor): So if we call {pause} a factor of w just one , just normalized to one , the word error rate {pause} that you have {pause} for using TI - digits as {disfmarker} as {pause} training and TI - digits as test ,
Turn 138, D (PhD): Mmm .
Turn 139, B (Professor): uh different words , I 'm sure ,
Turn 140, D (PhD): Mm - hmm .
Turn 141, B (Professor): but {disfmarker} {pause} but uh , uh the same {pause} task and so on .
Turn 142, D (PhD): Mm - hmm .
Turn 143, B (Professor): If we call that " one " , {pause} then what you 're saying is {pause} that the word error rate {pause} for the same language but using {pause} uh different training data than you 're testing on , say TIMIT and so forth , {pause} it 's one point one .
Turn 144, D (PhD): Mm - hmm . Yeah , it 's around one point one .
Turn 145, B (Professor): Right . And if it 's {disfmarker}
Turn 146, D (PhD): Yeah .
Turn 147, B (Professor): you {pause} do {pause} go to {pause} three languages including the English , {pause} it 's something like one point three . That 's what you were just saying , I think .
Turn 148, D (PhD): Ye Uh , more actually .
Turn 149, A (PhD): One point four ?
Turn 150, D (PhD): If I {disfmarker} Yeah .
Turn 151, A (PhD): So , it 's an additional thirty percent .
Turn 152, D (PhD): What would you say ? Around one point four
Turn 153, B (Professor): OK .
Turn 154, D (PhD): yeah .
Turn 155, B (Professor): And if you exclude {pause} English , {pause} from this combination , what 's that ?
Turn 156, D (PhD): If we exclude English , {pause} um {pause} there is {pause} not much difference with the {pause} data with English .
Turn 157, B (Professor): Aha !
Turn 158, D (PhD): So . Yeah .
Turn 159, B (Professor): That 's interesting . {pause} That 's interesting . Do you see ? Because {disfmarker} Uh ,
Turn 160, D (PhD): Uh .
Turn 161, B (Professor): so {disfmarker} No , that {disfmarker} that 's important . So what {disfmarker} what it 's saying here is just that " yes , there is a reduction {pause} in performance , {pause} when you don't {pause} um {pause} have the s {pause} when you don't have {pause} um
Turn 162, A (PhD): Task data .
Turn 163, B (Professor): Wait a minute , th th the {disfmarker}
Turn 164, D (PhD): Hmm .
Turn 165, B (Professor): No , actually {pause} it 's interesting . So it 's {disfmarker} So when you go to a different task , there 's actually not so {pause} different . It 's when you went to these {disfmarker} So what 's the difference between two and three ? Between the one point one case and the one point four case ? I 'm confused .
Turn 166, A (PhD): It 's multilingual .
Turn 167, D (PhD): Yeah . The only difference it 's {disfmarker} is that it 's multilingual {disfmarker} Um
Turn 168, B (Professor): Cuz in both {disfmarker} in both {disfmarker} both of those cases , you don't have the same task .
Turn 169, D (PhD): Yeah . Yeah sure .
Turn 170, B (Professor): So is {disfmarker} is the training data for the {disfmarker} for this one point four case {disfmarker} does it include the training data for the one point one case ?
Turn 171, D (PhD): Uh yeah .
Turn 172, F (Grad): Yeah , a fraction of it .
Turn 173, D (PhD): A part of it , yeah .
Turn 174, B (Professor): How m how much bigger is it ?
Turn 175, D (PhD): Um {pause} It 's two times ,
Turn 176, F (Grad): Yeah , um .
Turn 177, D (PhD): actually ? Yeah . Um . The English data {disfmarker} {pause} No , the multilingual databases are two times the {pause} broad English {pause} data . We just wanted to keep this , w well , not too huge . So .
Turn 178, B (Professor): So it 's two times , but it includes the {disfmarker} but it includes the broad English data .
Turn 179, D (PhD): I think so . Do you {disfmarker} Uh , Yeah .
Turn 180, B (Professor): And the broad English data is what you got this one point one {pause} with . So that 's TIMIT basically right ?
Turn 181, D (PhD): Yeah .
Turn 182, F (Grad): Mm - hmm .
Turn 183, B (Professor): So it 's band - limited TIMIT . This is all eight kilohertz sampling .
Turn 184, D (PhD): Mm - hmm .
Turn 185, F (Grad): Mm - hmm .
Turn 186, D (PhD): Yeah .
Turn 187, F (Grad): Downs Right .
Turn 188, B (Professor): So you have band - limited TIMIT , {pause} gave you uh almost as good as a result as using TI - digits {pause} on a TI - digits test . OK ?
Turn 189, D (PhD): Hmm ?
Turn 190, B (Professor): Um {pause} and {pause} um But , {pause} when you add in more training data but keep the neural net the same size , {pause} it {pause} um performs worse on the TI - digits . OK , now all of this is {disfmarker} {pause} This is noisy {pause} TI - digits , I assume ? Both training and test ?
Turn 191, D (PhD): 
Turn 192, B (Professor): Yeah . OK . Um OK . Well . {pause} We {disfmarker} we {disfmarker} we may just need to uh {disfmarker} So I mean it 's interesting that h going to a different {disfmarker} different task didn't seem to hurt us that much , and going to a different language um It doesn't seem to matter {disfmarker} The difference between three and four is not particularly great , so that means that {pause} whether you have the language in or not is not such a big deal .
Turn 193, D (PhD): Mmm .
Turn 194, B (Professor): It sounds like um {pause} uh {pause} we may need to have more {pause} of uh things that are similar to a target language or {disfmarker} I mean . {pause} You have the same number of parameters in the neural net , you haven't increased the size of the neural net , and maybe there 's just {disfmarker} {pause} just not enough {pause} complexity to it to represent {pause} the variab increased variability in the {disfmarker} in the training set . That {disfmarker} that could be . Um {pause} So , what about {disfmarker} So these are results with {pause} uh th {pause} that you 're describing now , that {pause} they are pretty similar for the different features or {disfmarker} {pause} or uh {disfmarker}
Turn 195, D (PhD): Uh , let me check . Uh .
Turn 196, B (Professor): Yeah .
Turn 197, D (PhD): So . This was for the PLP ,
Turn 198, B (Professor): Yeah .
Turn 199, D (PhD): Um . The {disfmarker} Yeah . For the PLP with JRASTA the {disfmarker} {pause} the {disfmarker} we {disfmarker} This is quite the same {pause} tendency , {pause} with a slight increase of the error rate , {pause} uh if we go to {disfmarker} to TIMIT . And then it 's {disfmarker} it gets worse with the multilingual . Um . Yeah . There {disfmarker} there is a difference actually with {disfmarker} b between PLP and JRASTA is that {pause} JRASTA {pause} seems to {pause} perform better with the highly mismatched {pause} condition {pause} but slightly {disfmarker} slightly worse {pause} for the well matched condition . Mmm .
Turn 200, B (Professor): I have a suggestion , actually , even though it 'll delay us slightly , would {disfmarker} would you mind {pause} running into the other room and making {pause} copies of this ? Cuz we 're all sort of {disfmarker} If we c if we could look at it , while we 're talking , I think it 'd be
Turn 201, D (PhD): Yeah , yeah . OK .
Turn 202, B (Professor): uh {disfmarker} {pause} Uh , I 'll {disfmarker} I 'll sing a song or dance or something while you {vocalsound} do it , too .
Turn 203, A (PhD): So um {disfmarker}
Turn 204, F (Grad): Alright .
Turn 205, A (PhD): Go ahead . Ah , while you 're gone I 'll ask s some of my questions .
Turn 206, B (Professor): Yeah .
Turn 207, A (PhD): Um .
Turn 208, B (Professor): Yeah . Uh , this way and just slightly to the left , yeah .
Turn 209, A (PhD): The um {disfmarker} What was {disfmarker} Was this number {pause} forty or {disfmarker} It was roughly the same as this one , {pause} he said ? When you had the two language versus the three language ?
Turn 210, B (Professor): Um . That 's what he was saying .
Turn 211, A (PhD): That 's where he removed English ,
Turn 212, F (Grad): Yeah .
Turn 213, A (PhD): right ?
Turn 214, B (Professor): Right .
Turn 215, F (Grad): It sometimes , actually , depends on what features you 're using .
Turn 216, B (Professor): Yeah . But {disfmarker} but i it sounds like {disfmarker}
Turn 217, F (Grad): Um , but {disfmarker} {vocalsound} {vocalsound} He {disfmarker} Mm - hmm .
Turn 218, B (Professor): I mean . That 's interesting because {pause} it {disfmarker} it seems like what it 's saying is not so much that you got hurt {pause} uh because {pause} you {pause} uh didn't have so much representation of English , because in the other case you don't get hurt any more , at least when {pause} it seemed like uh it {disfmarker} it might simply be a case that you have something that is just much more diverse ,
Turn 219, A (PhD): Mm - hmm .
Turn 220, B (Professor): but you have the same number of parameters representing it .
Turn 221, A (PhD): Mm - hmm . I wonder {disfmarker} were um all three of these nets {pause} using the same output ? This multi - language {pause} uh labelling ?
Turn 222, F (Grad): He was using uh sixty - four phonemes from {pause} SAMPA .
Turn 223, A (PhD): OK , OK .
Turn 224, F (Grad): Yeah .
Turn 225, A (PhD): So this would {disfmarker} {pause} From this you would say , " well , it doesn't really matter if we put Finnish {pause} into {pause} the training of the neural net , {pause} if there 's {pause} gonna be , {pause} you know , Finnish in the test data . " Right ?
Turn 226, B (Professor): Well , it 's {disfmarker} it sounds {disfmarker} {pause} I mean , we have to be careful , cuz we haven't gotten a good result yet .
Turn 227, A (PhD): Yeah .
Turn 228, B (Professor): And comparing different bad results can be {pause} tricky .
Turn 229, A (PhD): Hmm .
Turn 230, B (Professor): But I {disfmarker} I {disfmarker} I {disfmarker} {pause} I think it does suggest that it 's not so much uh {pause} uh cross {pause} language as cross type of speech .
Turn 231, A (PhD): Mm - hmm .
Turn 232, B (Professor): It 's {disfmarker} it 's um {disfmarker} {vocalsound} But we did {disfmarker} Oh yeah , the other thing I was asking him , though , is that I think that in the case {disfmarker} Yeah , you {disfmarker} you do have to be careful because of com compounded results . I think we got some earlier results {pause} in which you trained on one language and tested on another and you didn't have {pause} three , but you just had one {pause} language . So you trained on {pause} one type of digits and tested on another . Didn - Wasn't there something of that ? Where you , {pause} say , trained on Spanish and tested on {disfmarker} on TI - digits , or the other way around ? Something like that ?
Turn 233, E (PhD): No .
Turn 234, B (Professor): I thought there was something like that , {pause} that he showed me {pause} last week . We 'll have to wait till we get {disfmarker}
Turn 235, A (PhD): Yeah , that would be interesting .
Turn 236, B (Professor): Um , This may have been what I was asking before , Stephane , but {disfmarker} {pause} but , um , wasn't there something that you did , {pause} where you trained {pause} on one language and tested on another ? I mean no {disfmarker} no mixture but just {disfmarker}
Turn 237, F (Grad): I 'll get it for you .
Turn 238, D (PhD): Uh , no , no .
Turn 239, B (Professor): We 've never just trained on one lang
Turn 240, D (PhD): Training on a single language , you mean , and testing on the other one ?
Turn 241, B (Professor): Yeah .
Turn 242, D (PhD): Uh , no .
Turn 243, E (PhD): Not yet .
Turn 244, D (PhD): So the only {pause} task that 's similar to this is the training on two languages , and {comment} that {disfmarker}
Turn 245, B (Professor): But we 've done a bunch of things where we just trained on one language . Right ? I mean , you haven't {disfmarker} you haven't done all your tests on multiple languages .
Turn 246, D (PhD): Uh , No . Either thi this is test with {pause} uh the same language {pause} but from the broad data , or it 's test with {pause} uh different languages also from the broad data , excluding the {disfmarker} So , it 's {disfmarker} it 's three or {disfmarker} three and four .
Turn 247, E (PhD): The early experiment that {disfmarker}
Turn 248, A (PhD): Did you do different languages from digits ?
Turn 249, D (PhD): Uh . No . You mean {pause} training digits {pause} on one language and using the net {pause} to recognize on the other ?
Turn 250, A (PhD): Digits on another language ?
Turn 251, D (PhD): No .
Turn 252, B (Professor): See , I thought you showed me something like that last week . You had a {disfmarker} you had a little {disfmarker}
Turn 253, D (PhD): Uh , {pause} No , I don't think so .
Turn 254, B (Professor): Um What {disfmarker}
Turn 255, C (PhD): These numbers are uh {pause} ratio to baseline ?
Turn 256, B (Professor): So , I mean wha what 's the {disfmarker}
Turn 257, D (PhD): So .
Turn 258, B (Professor): This {disfmarker} this chart {disfmarker} this table that we 're looking at {pause} is um , show is all testing for TI - digits , or {disfmarker} ?
Turn 259, F (Grad): Bigger is worse .
Turn 260, D (PhD): So you have uh basically two {pause} uh parts .
Turn 261, F (Grad): This is error rate , I think .
Turn 262, C (PhD): Ratio .
Turn 263, F (Grad): No . {pause} No .
Turn 264, D (PhD): The upper part is for TI - digits
Turn 265, F (Grad): Yeah , yeah , yeah .
Turn 266, D (PhD): and it 's divided in three {pause} rows {pause} of four {disfmarker} four rows each .
Turn 267, F (Grad): Mm - hmm .
Turn 268, B (Professor): Yeah .
Turn 269, D (PhD): And the first four rows is well - matched , then the s the second group of four rows is mismatched , and {pause} finally highly mismatched . And then the lower part is for Italian and it 's the same {disfmarker} {pause} the same thing .
Turn 270, A (PhD): So , so the upper part is training {pause} TI - digits ?
Turn 271, D (PhD): So . It 's {disfmarker} it 's the HTK results , I mean . So it 's {pause} HTK training testings {pause} with different kind of features
Turn 272, A (PhD): Ah .
Turn 273, D (PhD): and what appears in the {pause} uh left column is {pause} the networks that are used for doing this .
Turn 274, B (Professor): Hmm .
Turn 275, D (PhD): So . Uh Yeah .
Turn 276, B (Professor): Well , What was is that i What was it that you had {pause} done {pause} last week when you showed {disfmarker} Do you remember ? Wh - when you showed me {pause} the {disfmarker} your table last week ?
Turn 277, D (PhD): It - It was part of these results . Mmm . Mmm .
Turn 278, A (PhD): So where is the baseline {pause} for the TI - digits {pause} located in here ?
Turn 279, D (PhD): You mean the HTK Aurora baseline ?
Turn 280, A (PhD): Yeah .
Turn 281, D (PhD): It 's uh the one hundred number . It 's , well , all these numbers are the ratio {pause} with respect to the baseline .
Turn 282, A (PhD): Ah ! Ah , OK , OK .
Turn 283, B (Professor): So this is word {disfmarker} word error rate , so a high number is bad .
Turn 284, D (PhD): Yeah , this is {pause} a word error rate ratio .
Turn 285, E (PhD): Yeah .
Turn 286, A (PhD): OK , I see .
Turn 287, D (PhD): Yeah . So , seventy point two means that {pause} we reduced the error rate uh by thirty {disfmarker} thirty percent .
Turn 288, A (PhD): OK , OK , gotcha .
Turn 289, D (PhD): So .
Turn 290, B (Professor): OK , {vocalsound} so if we take
Turn 291, D (PhD): Hmm .
Turn 292, B (Professor): uh um let 's see PLP {pause} uh with on - line {pause} normalization and {pause} delta - del so that 's this thing you have circled here {pause} in the second column ,
Turn 293, D (PhD): Yeah .
Turn 294, B (Professor): um {pause} and " multi - English " refers to what ?
Turn 295, D (PhD): To TIMIT . Mmm . Then you have {pause} uh MF , {pause} MS and ME which are for French , Spanish and English . And , yeah . Actually I {disfmarker} {pause} I uh forgot to say that {pause} the multilingual net are trained {pause} on {pause} uh {pause} features without the s derivatives uh but with {pause} increased frame numbers . Mmm . And we can {disfmarker} we can see on the first line of the table that it {disfmarker} it {disfmarker} {pause} it 's slightly {disfmarker} slightly worse when we don't use delta but it 's not {disfmarker} {pause} not that much .
Turn 296, B (Professor): Right . So w w So , I 'm sorry . I missed that . What 's MF , MS and ME ?
Turn 297, A (PhD): Multi - French , Multi - Spanish
Turn 298, D (PhD): So . Multi - French , Multi - Spanish , and Multi - English .
Turn 299, B (Professor): Uh OK . So , it 's {pause} uh {pause} broader vocabulary . Then {disfmarker} And {disfmarker}
Turn 300, D (PhD): Yeah .
Turn 301, B (Professor): OK so I think what I 'm {disfmarker} what I saw in your smaller chart that I was thinking of was {disfmarker} was {pause} there were some numbers I saw , I think , that included these multiple languages and it {disfmarker} and I was seeing {pause} that it got worse . I {disfmarker} I think that was all it was . You had some very limited results that {disfmarker} at that point
Turn 302, D (PhD): Yeah .
Turn 303, B (Professor): which showed {pause} having in these {disfmarker} these other languages . In fact it might have been just this last category , {pause} having two languages broad that were {disfmarker} where {disfmarker} where English was removed . So that was cross language and the {disfmarker} and the result was quite poor . What I {disfmarker} {pause} we hadn't seen yet was that if you added in the English , it 's still poor .
Turn 304, D (PhD): Yeah .
Turn 305, B (Professor): Uh {vocalsound} {vocalsound} Um now , what 's the noise condition {pause} um {pause} of the training data {disfmarker}
Turn 306, D (PhD): Still poor .
Turn 307, B (Professor): Well , I think this is what you were explaining . The noise condition is the same {disfmarker} It 's the same uh Aurora noises uh , in all these cases {pause} for the training .
Turn 308, D (PhD): Yeah . Yeah .
Turn 309, B (Professor): So there 's not a {pause} statistical {disfmarker} sta a strong st {pause} statistically different {pause} noise characteristic between {pause} uh the training and test
Turn 310, D (PhD): No these are the s s s same noises ,
Turn 311, B (Professor): and yet we 're seeing some kind of effect {disfmarker}
Turn 312, D (PhD): yeah . At least {disfmarker} at least for the first {disfmarker} {pause} for the well - matched ,
Turn 313, F (Grad): Well matched condition .
Turn 314, B (Professor): Right .
Turn 315, D (PhD): yeah .
Turn 316, B (Professor): So there 's some kind of a {disfmarker} a {disfmarker} an effect from having these {disfmarker} uh this broader coverage um Now I guess what we should try doing with this is try {pause} testing these on u this same sort of thing on {disfmarker} you probably must have this {pause} lined up to do . To try the same t {pause} with the exact same training , do testing on {pause} the other languages .
Turn 317, D (PhD): Mmm .
Turn 318, B (Professor): On {disfmarker} on um {disfmarker} So . Um , oh I well , wait a minute . You have this here , for the Italian . That 's right . OK , so , {pause} So .
Turn 319, D (PhD): Yeah . Yeah , so for the Italian the results are {vocalsound} uh {pause} stranger um {pause} Mmm . So what appears is that perhaps Spanish is {pause} not very close to Italian because uh , well , {pause} when using the {disfmarker} the network trained only on Spanish it 's {disfmarker} {pause} the error rate is {pause} almost uh twice {pause} the baseline error rate .
Turn 320, B (Professor): Mm - hmm .
Turn 321, D (PhD): Mmm . {vocalsound} Uh .
Turn 322, B (Professor): Well , I mean , let 's see . Is there any difference in {disfmarker} So it 's in {pause} the uh {disfmarker} So you 're saying that {pause} when you train on English {pause} and {pause} uh {pause} and {disfmarker} and test on {disfmarker}
Turn 323, D (PhD): Yeah .
Turn 324, B (Professor): No , you don't have training on English testing {disfmarker}
Turn 325, D (PhD): There {disfmarker} there is {disfmarker} another difference , is that the noise {disfmarker} the noises are different .
Turn 326, B (Professor): In {disfmarker} in what ?
Turn 327, D (PhD): Well , For {disfmarker} for the Italian part I mean the {pause} uh {pause} the um {pause} networks are trained with noise from {pause} Aurora {disfmarker} TI - digits ,
Turn 328, E (PhD): Aurora - two .
Turn 329, D (PhD): mmm .
Turn 330, B (Professor): And the noise is different in th
Turn 331, D (PhD): Yeah . And perhaps the noise are {pause} quite different from the noises {pause} in the speech that Italian .
Turn 332, B (Professor): Do we have any um {pause} test sets {pause} uh in {pause} any other language that um have the same noise as in {pause} the Aurora ?
Turn 333, D (PhD): And {disfmarker}
Turn 334, E (PhD): Mmm , no .
Turn 335, D (PhD): No .
Turn 336, A (PhD): Can I ask something real quick ? In {disfmarker} in the upper part {disfmarker} {pause} in the English {pause} stuff , {pause} it looks like the very best number is sixty point nine ? and that 's in the uh {disfmarker} {pause} the third {pause} section in the upper part under PLP JRASTA , sort of the middle column ?
Turn 337, D (PhD): Yeah .
Turn 338, A (PhD): I is that {pause} a noisy condition ?
Turn 339, D (PhD): Yeah .
Turn 340, A (PhD): So that 's matched training ? Is that what that is ?
Turn 341, D (PhD): It 's {disfmarker} no , the third part , so it 's uh {pause} highly mismatched . So . Training and {pause} test noise are different .
Turn 342, A (PhD): So {disfmarker} why do you get your best number in {disfmarker} Wouldn't you get your best number in the clean case ?
Turn 343, C (PhD): Well , it 's relative to the um {pause} baseline mismatching
Turn 344, D (PhD): Yeah .
Turn 345, A (PhD): Ah ,
Turn 346, D (PhD): Yeah . Yeah .
Turn 347, A (PhD): OK so these are not {disfmarker} OK , alright , I see .
Turn 348, C (PhD): Yeah .
Turn 349, A (PhD): OK . And then {disfmarker} so , in the {disfmarker} in the um {disfmarker} {pause} in the {pause} non - mismatched clean case , {pause} your best one was under MFCC ? That sixty - one point four ?
Turn 350, D (PhD): Yeah . {pause} But it 's not a clean case . It 's {pause} a noisy case but {pause} uh training and test noises are the same .
Turn 351, A (PhD): Oh ! So this upper third ?
Turn 352, D (PhD): So {disfmarker} Yeah .
Turn 353, A (PhD): Uh that 's still noisy ?
Turn 354, D (PhD): Yeah .
Turn 355, A (PhD): Ah , OK .
Turn 356, D (PhD): So it 's always noisy basically ,
Turn 357, A (PhD): Mm - hmm .
Turn 358, D (PhD): and , {pause} well , the {disfmarker}
Turn 359, A (PhD): I see .
Turn 360, D (PhD): Mmm .
Turn 361, B (Professor): OK ? Um {pause} So uh , I think this will take some {pause} looking at , thinking about . But , {pause} what is uh {disfmarker} what is currently running , that 's {disfmarker} uh , i that {disfmarker} just filling in the holes here or {disfmarker} or {disfmarker} ? {comment} {pause} pretty much ?
Turn 362, D (PhD): Uh , no we don't plan to fill the holes
Turn 363, B (Professor): OK .
Turn 364, D (PhD): but {pause} actually there is something important , is that {pause} um we made a lot of assumption concerning the on - line normalization and we just noticed {pause} uh recently that {pause} uh the {pause} approach that we were using {pause} was not {pause} uh {pause} leading to very good results {pause} when we {pause} used the straight features to HTK . Um {pause} {pause} Mmm . So basically d {pause} if you look at the {disfmarker} at the left of the table , {pause} the first uh row , {pause} with eighty - six , one hundred , and forty - three and seventy - five , these are the results we obtained for Italian {pause} uh with {pause} straight {pause} mmm , PLP features {pause} using on - line normalization .
Turn 365, B (Professor): Mm - hmm .
Turn 366, D (PhD): Mmm . And the , mmm {disfmarker} what 's {pause} in the table , just {pause} at the left of the PLP twelve {pause} on - line normalization column , so , the numbers seventy - nine , fifty - four and {pause} uh forty - two {pause} are the results obtained by uh Pratibha with {pause} uh his on - line normalization {disfmarker} uh her on - line normalization approach .
Turn 367, A (PhD): Where is that ? seventy - nine , fifty
Turn 368, B (Professor): Uh , it 's just sort of sitting right on the uh {disfmarker} the column line .
Turn 369, D (PhD): So .
Turn 370, E (PhD): Fifty - one ? This {disfmarker}
Turn 371, A (PhD): Oh I see , OK .
Turn 372, B (Professor): Uh . {pause} Yeah .
Turn 373, D (PhD): Just {disfmarker} uh Yeah . So these are the results of {pause} OGI with {pause} on - line normalization and straight features to HTK . And the previous result , eighty - six and so on , {pause} are with our {pause} features straight to HTK .
Turn 374, B (Professor): Yes . Yes .
Turn 375, D (PhD): So {pause} what we see that {disfmarker} is {disfmarker} there is that um {pause} uh the way we were doing this was not correct , but {pause} still {pause} the networks {pause} are very good . When we use the networks {pause} our number are better that {pause} uh Pratibha results .
Turn 376, E (PhD): We improve .
Turn 377, B (Professor): So , do you know what was wrong with the on - line normalization , or {disfmarker} ?
Turn 378, D (PhD): Yeah . There were diff there were different things and {pause} basically , {pause} the first thing is the mmm , {pause} alpha uh {pause} value . So , the recursion {pause} uh {pause} part . um , {pause} I used point five percent , {pause} which was the default value in the {disfmarker} {pause} in the programs here . And Pratibha used five percent .
Turn 379, B (Professor): Uh
Turn 380, D (PhD): So it adapts more {pause} quickly
Turn 381, B (Professor): Yes . Yeah .
Turn 382, D (PhD): Um , but , yeah . I assume that this was not important because {pause} uh previous results from {disfmarker} from Dan and {disfmarker} show that basically {pause} the {pause} both {disfmarker} both values g give the same {disfmarker} same {pause} uh results . It was true on uh {pause} TI - digits but it 's not true on Italian .
Turn 383, B (Professor): Mm - hmm .
Turn 384, D (PhD): Uh , second thing is the initialization of the {pause} stuff . Actually , {pause} uh what we were doing is to start the recursion from the beginning of the {pause} utterance . And using initial values that are the global mean and variances {pause} measured across the whole database .
Turn 385, B (Professor): Right . Right .
Turn 386, D (PhD): And Pratibha did something different is that he {disfmarker} uh she initialed the um values of the mean and variance {pause} by computing {pause} this on the {pause} twenty - five first frames of each utterance . Mmm . There were other minor differences , the fact that {pause} she used fifteen dissities instead s instead of thirteen , and that she used C - zero instead of log energy . Uh , but the main differences concerns the recursion . So . {pause} Uh , I changed the code uh and now we have a baseline that 's similar to the OGI baseline .
Turn 387, B (Professor): OK .
Turn 388, D (PhD): We {disfmarker} It {disfmarker} it 's slightly {pause} uh different because {pause} I don't exactly initialize the same way she does . Actually I start , {pause} mmm , I don't wait to a fifteen {disfmarker} twenty - five {disfmarker} twenty - five frames {pause} before computing a mean and the variance {pause} to e to {disfmarker} to start the recursion .
Turn 389, C (PhD): Mm - hmm .
Turn 390, B (Professor): Yeah .
Turn 391, D (PhD): I {disfmarker} I use the on - line scheme and only start the re recursion after the twenty - five {disfmarker} {pause} twenty - fifth frame . But , well it 's similar . So {pause} uh I retrained {pause} the networks with {pause} these {disfmarker} well , the {disfmarker} the {disfmarker} the networks are retaining with these new {pause} features .
Turn 392, B (Professor): Mm - hmm .
Turn 393, D (PhD): And , yeah .
Turn 394, B (Professor): OK .
Turn 395, D (PhD): So basically what I expect is that {pause} these numbers will a little bit go down but {pause} perhaps not {disfmarker} not so much
Turn 396, B (Professor): Right .
Turn 397, D (PhD): because {pause} I think the neural networks learn perhaps {pause} to {disfmarker}
Turn 398, B (Professor): Right .
Turn 399, D (PhD): even if the features are not {pause} normalized . It {disfmarker} it will learn how to normalize and {disfmarker}
Turn 400, B (Professor): OK , but I think that {pause} given the pressure of time we probably want to draw {disfmarker} because of that {pause} especially , we wanna draw some conclusions from this , do some reductions {pause} in what we 're looking at ,
Turn 401, D (PhD): Yeah .
Turn 402, B (Professor): and make some strong decisions for what we 're gonna do testing on before next week . So do you {disfmarker} are you {disfmarker} w did you have something going on , on the side , with uh multi - band {pause} or {disfmarker} on {disfmarker} on this ,
Turn 403, D (PhD): Yeah {vocalsound} I
Turn 404, B (Professor): or {disfmarker} ?
Turn 405, D (PhD): No , I {disfmarker} we plan to start this uh so , act actually we have discussed uh {pause} @ @ um , these {disfmarker} what we could do {pause} more as a {disfmarker} as a research and {disfmarker} {pause} and {pause} we were thinking perhaps that {pause} uh {pause} the way we use the tandem is not {disfmarker} Uh , well , there is basically perhaps a flaw in the {disfmarker} in the {disfmarker} the stuff because {pause} we {pause} trained the networks {disfmarker} If we trained the networks on the {disfmarker} on {pause} a language and a t or a specific {pause} task ,
Turn 406, B (Professor): Mm - hmm .
Turn 407, D (PhD): um , what we ask is {disfmarker} to the network {disfmarker} is to put the bound the decision boundaries somewhere in the space .
Turn 408, B (Professor): Mmm .
Turn 409, D (PhD): And uh {pause} mmm and ask the network to put one , {pause} at one side of the {disfmarker} for {disfmarker} for a particular phoneme at one side of the boundary {disfmarker} decision boundary and one for another phoneme at the other side . And {pause} so there is kind of reduction of the information there that 's not correct because if we change task {pause} and if the phonemes are not in the same context in the new task , {pause} obviously the {pause} decision boundaries are not {disfmarker} {pause} should not be at the same {pause} place .
Turn 410, B (Professor): I di
Turn 411, D (PhD): But the way the feature gives {disfmarker} The {disfmarker} the way the network gives the features is that it reduce completely the {disfmarker} {pause} it removes completely the information {disfmarker} {pause} a lot of information from the {disfmarker} the features {pause} by uh {pause} uh {pause} placing the decision boundaries at {pause} optimal places for {pause} one kind of {pause} data but {pause} this is not the case for another kind of data .
Turn 412, B (Professor): It 's a trade - off ,
Turn 413, D (PhD): So {disfmarker}
Turn 414, B (Professor): right ? Any - anyway go ahead .
Turn 415, D (PhD): Yeah . So uh what we were thinking about is perhaps {pause} um one way {pause} to solve this problem is increase the number of {pause} outputs of the neural networks . Doing something like , um {pause} um phonemes within context and , well , basically context dependent phonemes .
Turn 416, B (Professor): Maybe . I mean , I {disfmarker} I think {pause} you could make {pause} the same argument , it 'd be just as legitimate , {pause} for hybrid systems {pause} as well . Right .
Turn 417, D (PhD): Yeah but , we know that {disfmarker}
Turn 418, B (Professor): And in fact , {pause} th things get better with context dependent {pause} versions . Right ?
Turn 419, D (PhD): Ye - yeah but here it 's something different . We want to have features
Turn 420, B (Professor): Yeah .
Turn 421, D (PhD): uh well , {pause} um .
Turn 422, B (Professor): Yeah , but it 's still true {pause} that what you 're doing {pause} is you 're ignoring {disfmarker} you 're {disfmarker} you 're coming up with something to represent , {pause} whether it 's a distribution , {pause} probability distribution or features , you 're coming up with a set of variables {pause} that are representing {pause} uh , {pause} things that vary w over context .
Turn 423, D (PhD): Mm - hmm .
Turn 424, B (Professor): Uh , and you 're {pause} putting it all together , ignoring the differences in context . That {disfmarker} that 's true {pause} for the hybrid system , it 's true for a tandem system . So , for that reason , when you {disfmarker} in {disfmarker} in {disfmarker} in a hybrid system , {pause} when you incorporate context one way or another , {pause} you do get better scores .
Turn 425, D (PhD): Yeah .
Turn 426, B (Professor): OK ? But I {disfmarker} it 's {disfmarker} it 's a big deal {pause} to get that . I {disfmarker} I 'm {disfmarker} I 'm sort of {disfmarker} And once you {disfmarker} the other thing is that once you represent {disfmarker} start representing more and more context {pause} it is {pause} uh {pause} much more {pause} um specific {pause} to a particular task in language . So um Uh , the {disfmarker} {pause} the acoustics associated with {pause} uh a particular context , for instance you may have some kinds of contexts that will never occur {pause} in one language and will occur frequently in the other , so the qu the issue of getting enough training {pause} for a particular kind of context becomes harder . We already actually don't have a huge amount of training data um
Turn 427, D (PhD): Yeah , but {disfmarker} mmm , I mean , {pause} the {disfmarker} the way we {disfmarker} we do it now is that we have a neural network and {pause} basically {pause} the net network is trained almost to give binary decisions .
Turn 428, B (Professor): Right .
Turn 429, D (PhD): And {pause} uh {disfmarker} binary decisions about phonemes . Nnn {disfmarker} Uh It 's {disfmarker}
Turn 430, B (Professor): Almost . But I mean it {disfmarker} it {disfmarker} it does give a distribution .
Turn 431, D (PhD): Yeah .
Turn 432, B (Professor): It 's {disfmarker} and {disfmarker} and {pause} it is true that if there 's two phones that are very similar , {pause} that {pause} uh {pause} the {disfmarker} {pause} i it may prefer one but it will {pause} give a reasonably high value to the other , too .
Turn 433, D (PhD): Yeah . Yeah , sure but uh {pause} So basically it 's almost binary decisions and {pause} um the idea of using more {pause} classes is {pause} to {pause} get something that 's {pause} less binary decisions .
Turn 434, B (Professor): Oh no , but it would still be even more of a binary decision . It {disfmarker} it 'd be even more of one . Because then you would say {pause} that in {disfmarker} that this phone in this context is a one , {pause} but the same phone in a slightly different context is a zero .
Turn 435, D (PhD): But {disfmarker} yeah , but {disfmarker}
Turn 436, B (Professor): That would be even {disfmarker} even more distinct of a binary decision . I actually would have thought you 'd wanna go the other way and have fewer classes .
Turn 437, D (PhD): Yeah , but if {disfmarker}
Turn 438, B (Professor): Uh , I mean for instance , the {disfmarker} the thing I was arguing for before , but again which I don't think we have time to try , {pause} is something in which you would modify the code so you could train to have several outputs on and use articulatory features
Turn 439, D (PhD): Mmm . Mm - hmm .
Turn 440, B (Professor): cuz then that would {disfmarker} that would go {disfmarker} {pause} that would be much broader and cover many different situations . But if you go to very very fine categories , it 's very {pause} binary .
Turn 441, D (PhD): Mmm . Yeah , but I think {disfmarker} Yeah , perhaps you 're right , but you have more classes so {pause} you {disfmarker} you have more information in your features . So , {vocalsound} Um {pause} You have more information in the {pause} uh
Turn 442, B (Professor): Mm - hmm . True .
Turn 443, D (PhD): posteriors vector um which means that {disfmarker} But still the information is relevant
Turn 444, B (Professor): Mm - hmm .
Turn 445, D (PhD): because it 's {disfmarker} it 's information that helps to discriminate ,
Turn 446, B (Professor): Mm - hmm .
Turn 447, D (PhD): if it 's possible to be able to discriminate {pause} among the phonemes in context .
Turn 448, B (Professor): Well it 's {disfmarker} it 's {disfmarker} {pause} it 's an interesting thought .
Turn 449, D (PhD): But the {disfmarker}
Turn 450, B (Professor): I mean we {disfmarker} we could disagree about it at length
Turn 451, D (PhD): Mmm .
Turn 452, B (Professor): but the {disfmarker} the real thing is if you 're interested in it you 'll probably try it
Turn 453, D (PhD): Mmm .
Turn 454, B (Professor): and {disfmarker} {pause} and {pause} we 'll see . But {disfmarker} but what I 'm more concerned with now , as an operational level , is {pause} uh , you know ,
Turn 455, D (PhD): Mmm .
Turn 456, B (Professor): what do we do in four or five days ? Uh , and {disfmarker} {pause} so we have {pause} to be concerned {pause} with Are we gonna look at any combinations of things , you know once the nets get retrained so you have this problem out of it .
Turn 457, D (PhD): Mmm .
Turn 458, B (Professor): Um , are we going to look at {pause} multi - band ? Are we gonna look at combinations of things ? Uh , what questions are we gonna ask , uh now that , I mean , {pause} we should probably turn shortly to this O G I note . Um , how are we going to {pause} combine {pause} with what they 've been focusing on ? Uh , {pause} Uh we haven't been doing any of the L D A RASTA sort of thing .
Turn 459, D (PhD): Mm - hmm .
Turn 460, B (Professor): And they , although they don't talk about it in this note , um , {pause} there 's um , {pause} the issue of the {pause} um Mu law {pause} business {pause} uh {pause} versus the logarithm , um , {pause} so .
Turn 461, D (PhD): Mm - hmm .
Turn 462, B (Professor): So what i what is going on right now ? What 's right {disfmarker} you 've got {pause} nets retraining , Are there {disfmarker} is there {disfmarker} are there any H T K {pause} trainings {disfmarker} testings going on ?
Turn 463, D (PhD): N
Turn 464, E (PhD): I {disfmarker} I {disfmarker} I 'm trying the HTK with eh , {pause} PLP twelve on - line delta - delta and MSG filter {pause} together .
Turn 465, B (Professor): The combination , I see .
Turn 466, E (PhD): The combination , yeah . But I haven't result {vocalsound} at this moment .
Turn 467, B (Professor): MSG and {disfmarker} and PLP .
Turn 468, E (PhD): Yeah . 
Turn 469, B (Professor): And is this with the revised {pause} on - line normalization ?
Turn 470, E (PhD): Ye - Uh , with the old {pause} older ,
Turn 471, D (PhD): Yeah .
Turn 472, B (Professor): Old one . So it 's using all the nets for that
Turn 473, E (PhD): yeah .
Turn 474, B (Professor): but again we have the hope that it {disfmarker} {pause} We have the hope that it {disfmarker} {pause} maybe it 's not making too much difference ,
Turn 475, E (PhD): Yeah . But {pause} We can know soon .
Turn 476, B (Professor): but {disfmarker} but
Turn 477, E (PhD): Maybe .
Turn 478, B (Professor): yeah .
Turn 479, E (PhD): I don't know .
Turn 480, D (PhD): Yeah .
Turn 481, B (Professor): Uh , OK .
Turn 482, D (PhD): Uh so there is this combination , yeah . Working on combination obviously .
Turn 483, E (PhD): Mm - hmm .
Turn 484, D (PhD): Um , I will start work on multi - band . And {pause} we {pause} plan to work also on the idea of using both {pause} features {pause} and net outputs .
Turn 485, E (PhD): 
Turn 486, D (PhD): Um . And {pause} we think that {pause} with this approach perhaps {pause} we could reduce the number of outputs of the neural network . Um , So , get simpler networks , because we still have the features . So we have um {pause} come up with um {pause} different kind of {pause} broad phonetic categories . And we have {disfmarker} Basically we have three {pause} types of broad phonetic classes . Well , something using place of articulation which {disfmarker} which leads to {pause} nine , I think , {pause} broad classes . Uh , another which is based on manner , which is {disfmarker} is also something like nine classes . And then , {pause} something that combine both , and we have {pause} twenty f {pause} twenty - five ?
Turn 487, F (Grad): Twenty - seven .
Turn 488, D (PhD): Twenty - seven broad classes . So like , uh , oh , I don't know , like back vowels , front vowels .
Turn 489, B (Professor): So what you do {disfmarker} um I just wanna understand
Turn 490, D (PhD): Um For the moments we do not {disfmarker} don't have nets ,
Turn 491, B (Professor): so {pause} You have two net or three nets ? Was this ? How many {disfmarker} how many nets do you have ? No nets .
Turn 492, D (PhD): I mean , {pause} It 's just {disfmarker} Were we just changing {pause} the labels to retrain nets {pause} with fewer out outputs .
Turn 493, E (PhD): Begin to work in this . We are @ @ .
Turn 494, B (Professor): Right . But {disfmarker} but I didn't understand {disfmarker}
Turn 495, D (PhD): And then {disfmarker} Mm - hmm .
Turn 496, B (Professor): Uh . {pause} the software currently just has {disfmarker} uh a {disfmarker} allows for I think , the one {disfmarker} one hot output . So you 're having multiple nets and combining them , or {disfmarker} ? Uh , how are you {disfmarker} how are you coming up with {disfmarker} If you say {pause} uh {pause} If you have a place {pause} characteristic and a manner characteristic , how do you {disfmarker}
Turn 497, D (PhD): It - It 's the single net ,
Turn 498, A (PhD): I think they have one output .
Turn 499, D (PhD): yeah .
Turn 500, B (Professor): Oh , it 's just one net .
Turn 501, D (PhD): It 's one net with {pause} um {pause} twenty - seven outputs
Turn 502, E (PhD): Yeah .
Turn 503, F (Grad): mm - hmm
Turn 504, D (PhD): if we have twenty - seven classes ,
Turn 505, B (Professor): I see . I see , OK .
Turn 506, D (PhD): yeah . So it 's {disfmarker} Well , it 's basically a standard net with fewer {pause} classes .
Turn 507, B (Professor): So you 're sort of going the other way of what you were saying a bit ago instead of {disfmarker} yeah .
Turn 508, D (PhD): Yeah , but I think {disfmarker} Yeah . B b including the features , yeah .
Turn 509, F (Grad): But including the features .
Turn 510, E (PhD): Yeah .
Turn 511, D (PhD): I don't think this {pause} will work {pause} alone . I think it will get worse because Well , I believe the effect that {disfmarker} of {disfmarker} of too reducing too much the information is {pause} basically {disfmarker} basically what happens
Turn 512, B (Professor): Uh - huh .
Turn 513, D (PhD): and {disfmarker}
Turn 514, B (Professor): But you think if you include that {pause} plus the other features ,
Turn 515, D (PhD): but {disfmarker} Yeah , because {pause} there is perhaps one important thing that the net {pause} brings , and OGI show showed that , is {pause} the distinction between {pause} sp speech and silence Because these nets are trained on well - controlled condition . I mean the labels are obtained on clean speech , and we add noise after . So this is one thing And But perhaps , something intermediary using also {pause} some broad classes could {disfmarker} could bring so much more information . Uh .
Turn 516, B (Professor): So {disfmarker} so again then we have these broad classes and {disfmarker} well , somewhat broad . I mean , it 's twenty - seven instead of sixty - four , {pause} basically . And you have the original features .
Turn 517, D (PhD): Yeah .
Turn 518, B (Professor): Which are PLP , or something .
Turn 519, D (PhD): Yeah .
Turn 520, B (Professor): And then uh , just to remind me , all of that goes {pause} into {disfmarker} uh , that all of that is transformed by uh , uh , K - KL or something , or {disfmarker} ?
Turn 521, D (PhD): Mm - hmm . There will probably be ,
Turn 522, E (PhD): Mu .
Turn 523, D (PhD): yeah , one single KL to transform everything
Turn 524, B (Professor): Right .
Turn 525, D (PhD): or {vocalsound} {pause} uh ,
Turn 526, E (PhD): No transform the PLP
Turn 527, D (PhD): per
Turn 528, E (PhD): and only transform the other I 'm not sure .
Turn 529, B (Professor): Well no ,
Turn 530, D (PhD): This is {pause} still something {pause} that
Turn 531, B (Professor): I think {disfmarker} I see .
Turn 532, D (PhD): yeah , we {pause} don't know {disfmarker}
Turn 533, B (Professor): So there 's a question of whether you would {disfmarker}
Turn 534, E (PhD): Two e @ @ it 's one .
Turn 535, D (PhD): Yeah .
Turn 536, B (Professor): Right . Whether you would transform together or just one . Yeah . Might wanna try it both ways . But that 's interesting . So that 's something that you 're {disfmarker} you haven't trained yet but are preparing to train , and {disfmarker}
Turn 537, D (PhD): Yeah .
Turn 538, B (Professor): Yeah . Um {pause} {pause} Yeah , so I think Hynek will be here Monday .
Turn 539, D (PhD): Mmm .
Turn 540, B (Professor):  Monday or Tuesday . So
Turn 541, D (PhD): Uh , yeah .
Turn 542, B (Professor): So I think , you know , we need to {pause} choose the {disfmarker} choose the experiments carefully , so we can get uh key {disfmarker} {pause} key questions answered {pause} uh before then
Turn 543, D (PhD): Mm - hmm .
Turn 544, B (Professor): and {pause} leave other ones aside even if it {pause} leaves incomplete {pause} tables {vocalsound} {pause} someplace , uh {pause} uh , it 's {disfmarker} it 's really time to {disfmarker} {pause} time to choose .
Turn 545, D (PhD): Mm - hmm .
Turn 546, B (Professor): Um , let me pass this out , {pause} by the way . Um These are {disfmarker} Did {disfmarker} did {disfmarker} {pause} did I interrupt you ?
Turn 547, E (PhD): Yeah , I have one .
Turn 548, B (Professor): Were there other things that you wanted to {disfmarker}
Turn 549, D (PhD): Uh , no . I don't think so .
Turn 550, E (PhD): 
Turn 551, D (PhD): Yeah , I have one .
Turn 552, G (Grad): Oh , thanks .
Turn 553, B (Professor): Ah ! {pause} OK . {pause} OK , we have {pause} lots of them .
Turn 554, E (PhD): We have one . 
Turn 555, B (Professor): OK , so {vocalsound} um , Something I asked {disfmarker} So they 're {disfmarker} they 're doing {pause} the {disfmarker} the VAD I guess they mean voice activity detection So again , it 's the silence {disfmarker} So they 've just trained up a net {pause} which has two outputs , I believe . Um {vocalsound} I asked uh {pause} Hynek whether {disfmarker} I haven't talked to Sunil {disfmarker} I asked Hynek whether {pause} they compared that to {pause} just taking the nets we already had {pause} and summing up the probabilities .
Turn 556, D (PhD): Mm - hmm .
Turn 557, B (Professor): Uh . {pause} To get the speech {disfmarker} voice activity detection , or else just using the silence , {pause} if there 's only one {pause} silence output . Um {pause} And , he didn't think they had , um . But on the other hand , maybe they can get by with a smaller net and {pause} maybe {pause} sometimes you don't run the other , maybe there 's a computational advantage to having a separate net , anyway .
Turn 558, D (PhD): Mm - hmm .
Turn 559, B (Professor): So um Their uh {disfmarker} {pause} the results look pretty good . Um , {pause} I mean , not uniformly .
Turn 560, D (PhD): Yeah .
Turn 561, B (Professor): I mean , there 's a {disfmarker} an example or two {pause} that you can find , where it made it slightly worse , but {pause} uh in {disfmarker} in all but a couple {pause} examples .
Turn 562, D (PhD): Mmm .
Turn 563, B (Professor): Uh .
Turn 564, E (PhD): But they have a question of the result . Um how are trained the {disfmarker} the LDA filter ? How obtained the LDA filter ?
Turn 565, D (PhD): Mmm .
Turn 566, B (Professor): I I 'm sorry . I don't understand your question .
Turn 567, E (PhD): Yes , um the LDA filter {pause} needs some {pause} training set {pause} to obtain the filter . Maybe I don't know exactly how {pause} they are obtained .
Turn 568, B (Professor): It 's on {pause} training .
Turn 569, E (PhD): Training , with the training test of each {disfmarker} You understand me ?
Turn 570, B (Professor): No .
Turn 571, E (PhD): Yeah , uh for example , {pause} LDA filter {pause} need a set of {disfmarker} {pause} a set of training {pause} to obtain the filter .
Turn 572, B (Professor): Yes .
Turn 573, E (PhD): And maybe {pause} for the Italian , for the TD {pause} TE on for Finnish , these filter are {disfmarker} are obtained with their own training set .
Turn 574, B (Professor): Yes , I don't know . That 's {disfmarker} that 's {disfmarker} so that 's a {disfmarker} that 's a very good question , then {disfmarker} now that it {disfmarker} {pause} I understand it . It 's " yeah , where does the LDA come from ? " In the {disfmarker} In {pause} earlier experiments , they had taken LDA {pause} from a completely different database , right ?
Turn 575, E (PhD): Yeah . Yeah , because maybe it the same situation that the neural network training with their own
Turn 576, D (PhD): Mmm .
Turn 577, E (PhD): set .
Turn 578, B (Professor): So that 's a good question . Where does it come from ? Yeah , I don't know . Um , {pause} but uh to tell you the {pause} truth , I wasn't actually looking at the LDA so much when I {disfmarker} I was looking at it I was {pause} mostly thinking about the {disfmarker} {pause} the VAD . And um , it ap {pause} it ap Oh what does {disfmarker} what does ASP ? Oh that 's {disfmarker}
Turn 579, D (PhD): The features , yeah . Yeah .
Turn 580, E (PhD): I don't understand also
Turn 581, B (Professor): It says " baseline ASP " .
Turn 582, E (PhD): what is {disfmarker} {pause} what is the difference between ASP and uh baseline over ?
Turn 583, C (PhD): ASP .
Turn 584, D (PhD): Yeah , I don't know .
Turn 585, E (PhD): This is {disfmarker}
Turn 586, B (Professor): Anybody know {pause} any {disfmarker}
Turn 587, C (PhD): Oh . There it is .
Turn 588, B (Professor): Um Cuz there 's " baseline Aurora " {pause} above it .
Turn 589, C (PhD): Mm - hmm .
Turn 590, B (Professor): And it 's {disfmarker} This is mostly better than baseline , although in some cases it 's a little worse , in a couple cases .
Turn 591, C (PhD): Well , it says baseline ASP is twenty - three mill {pause} minus thirteen .
Turn 592, E (PhD): Yeah .
Turn 593, B (Professor): Yeah , it says what it is . But I don't how that 's different {pause} from {disfmarker}
Turn 594, C (PhD): From the baseline . {comment} OK .
Turn 595, B (Professor): I think this was {disfmarker} {pause} I think this is the same point we were at when {disfmarker} when we were up in Oregon .
Turn 596, E (PhD): Yeah .
Turn 597, D (PhD): I think {disfmarker} {pause} I think it 's the C - zero {disfmarker} using C - zero instead of log energy .
Turn 598, E (PhD): Ah , OK , mm - hmm .
Turn 599, D (PhD): Yeah , it 's this .
Turn 600, B (Professor): Oh . OK .
Turn 601, E (PhD): yeah .
Turn 602, D (PhD): It should be that , yeah .
Turn 603, A (PhD): They s they say in here that the VAD is not used as an additional feature .
Turn 604, B (Professor): Shouldn't it be {disfmarker}
Turn 605, D (PhD): Because {disfmarker}
Turn 606, A (PhD): Does {disfmarker} does anybody know how they 're using it ?
Turn 607, B (Professor): Yeah . So {disfmarker} so what they 're doing here is , {pause} i
Turn 608, D (PhD): Yeah .
Turn 609, B (Professor): if you look down at the block diagram , {pause} um , {pause} they estimate {disfmarker} they get a {disfmarker} {pause} they get an estimate {pause} of whether it 's speech or silence ,
Turn 610, A (PhD): But that {disfmarker}
Turn 611, B (Professor): and then they have a median filter of it .
Turn 612, A (PhD): Mm - hmm .
Turn 613, B (Professor): And so um , {pause} basically they 're trying to find stretches . The median filter is enforcing a {disfmarker} i it having some continuity .
Turn 614, A (PhD): Mm - hmm .
Turn 615, B (Professor): You find stretches where the {pause} combination of the {pause} frame wise VAD and the {disfmarker} {pause} the median filter say that there 's a stretch of silence . And then it 's going through and just throwing the data away .
Turn 616, C (PhD): Hmm .
Turn 617, B (Professor): Right ? So um {disfmarker}
Turn 618, A (PhD): So it 's {disfmarker} it 's {disfmarker} I don't understand . You mean it 's throwing out frames ? Before {disfmarker}
Turn 619, B (Professor): It 's throwing out chunks of frames , yeah . There 's {disfmarker} the {disfmarker} the median filter is enforcing that it 's not gonna be single cases of frames , or isolated frames .
Turn 620, A (PhD): Yeah .
Turn 621, B (Professor): So it 's throwing out frames and the thing is {pause} um , {pause} what I don't understand is how they 're doing this with H T
Turn 622, A (PhD): Yeah , that 's what I was just gonna ask .
Turn 623, B (Professor): This is {disfmarker}
Turn 624, A (PhD): How can you just throw out frames ?
Turn 625, B (Professor): Yeah . Well , you {disfmarker} you can ,
Turn 626, D (PhD): i
Turn 627, B (Professor): right ? I mean y you {disfmarker} you {disfmarker}
Turn 628, D (PhD): Yeah .
Turn 629, B (Professor): it stretches again . For single frames I think it would be pretty hard .
Turn 630, A (PhD): Yeah .
Turn 631, B (Professor): But if you say speech starts here , speech ends there .
Turn 632, A (PhD): Mm - hmm .
Turn 633, B (Professor): Right ?
Turn 634, C (PhD): Huh .
Turn 635, D (PhD): Yeah . Yeah , you can basically remove the {disfmarker} the frames from the feature {disfmarker} feature files .
Turn 636, B (Professor): Yeah . Yeah , so I mean in the {disfmarker} i i in the {disfmarker} in the decoding , you 're saying that we 're gonna decode from here to here .
Turn 637, D (PhD): I t
Turn 638, A (PhD): Mm - hmm .
Turn 639, B (Professor): I think they 're {disfmarker} they 're {disfmarker} they 're treating it , {pause} you know , like uh {disfmarker} well , it 's not isolated word , but {disfmarker} but connected , you know , the {disfmarker} the {disfmarker}
Turn 640, A (PhD): In the text they say that this {disfmarker} this is a tentative block diagram of a possible configuration we could think of . So that sort of sounds like they 're not doing that yet .
Turn 641, B (Professor): Well . {pause} No they {disfmarker} they have numbers though , right ? So I think they 're {disfmarker} they 're doing something like that . I think that they 're {disfmarker} they 're {disfmarker} I think what I mean by tha that is they 're trying to come up with a block diagram that 's plausible for the standard . In other words , it 's {disfmarker} uh {disfmarker} I mean from the point of view of {disfmarker} of uh reducing the number of bits you have to transmit it 's not a bad idea to detect silence anyway .
Turn 642, A (PhD): Yeah . Yeah . I 'm just wondering what exactly did they do up in this table if it wasn't this .
Turn 643, B (Professor): Um . But it 's {disfmarker} the thing is it 's that {disfmarker} that {disfmarker} that 's {disfmarker} that 's I {disfmarker} I {disfmarker} Certainly it would be tricky about it intrans in transmitting voice , {pause} uh uh for listening to , is that these kinds of things {pause} uh cut {pause} speech off a lot .
Turn 644, A (PhD): Mm - hmm .
Turn 645, B (Professor): Right ? And so {pause} um
Turn 646, A (PhD): Plus it 's gonna introduce delays .
Turn 647, B (Professor): It does introduce delays but they 're claiming that it 's {disfmarker} it 's within the {disfmarker} {pause} the boundaries of it .
Turn 648, A (PhD): Mmm .
Turn 649, B (Professor): And the LDA introduces delays , and b {pause} what he 's suggesting this here is a parallel path so that it doesn't introduce {pause} uh , any more delay . I it introduces two hundred milliseconds of delay but at the same {pause} time the LDA {pause} down here {disfmarker} I don't know {disfmarker} Wh what 's the difference between TLDA and SLDA ?
Turn 650, C (PhD): Temporal and spectral .
Turn 651, B (Professor): Ah , thank you .
Turn 652, E (PhD): Temporal LDA .
Turn 653, B (Professor): Yeah , you would know that .
Turn 654, C (PhD): Yeah
Turn 655, B (Professor): So um . The temporal LDA does in fact include the same {disfmarker} so that {disfmarker} I think he {disfmarker} well , by {disfmarker} by saying this is a b a tentative block di diagram I think means {pause} if you construct it this way , this {disfmarker} this delay would work in that way
Turn 656, A (PhD): Ah .
Turn 657, B (Professor): and then it 'd be OK . They {disfmarker} they clearly did actually remove {pause} silent sections in order {disfmarker} because they {pause} got these {pause} word error rate {pause} results . So um I think that it 's {disfmarker} it 's nice to do that in this because in fact , it 's gonna give a better word error result and therefore will help within an evaluation . Whereas to whether this would actually be in a final standard , I don't know . Um . Uh , as you know , part of the problem with evaluation right now is that the {pause} word models are pretty bad and nobody wants {disfmarker} {pause} has {disfmarker} has approached improving them . So {pause} it 's possible that a lot of the problems {pause} with so many insertions and so forth would go away if they were better word models {pause} to begin with . So {pause} this might just be a temporary thing . But {disfmarker} But , on the other hand , and maybe {disfmarker} maybe it 's a decent idea . So um The question we 're gonna wanna go {pause} through next week when Hynek shows up I guess is given that we 've been {disfmarker} if you look at what we 've been trying , we 're uh looking at {pause} uh , by then I guess , combinations of features and multi - band Uh , and we 've been looking at {pause} cross - language , cross {pause} task {pause} issues . And they 've been not so much looking at {pause} the cross task uh multiple language issues . But they 've been looking at uh {disfmarker} {pause} at these issues . At the on - line normalization and the uh {pause} voice activity detection . And I guess when he comes here we 're gonna have to start deciding about {pause} um what do we choose {pause} from what we 've looked at {pause} to um blend with {pause} some group of things in what they 've looked at And once we choose that , {pause} how do we split up the {pause} effort ? Uh , because we still have {disfmarker} even once we choose , {pause} we 've still got {pause} uh another {pause} month or so , I mean there 's holidays in the way , but {disfmarker} but uh {pause} I think the evaluation data comes January thirty - first so there 's still a fair amount of time {pause} to do things together it 's just that they probably should be somewhat more coherent between the two sites {pause} in that {disfmarker} that amount of time .
Turn 658, A (PhD): When they removed the silence frames , did they insert some kind of a marker so that the recognizer knows it 's {disfmarker} {pause} knows when it 's time to back trace or something ?
Turn 659, B (Professor): Well , see they , I {disfmarker} I think they 're Um . I don't know the {disfmarker} {pause} the specifics of how they 're doing it . They 're {disfmarker} {pause} they 're getting around the way the recognizer works because they 're not allowed to {pause} um , change the scripts {pause} for the recognizer , {pause} I believe .
Turn 660, A (PhD): Oh , right . Maybe they 're just inserting some nummy frames or something ?
Turn 661, B (Professor): So . Uh . Uh , you know that 's what I had thought . But I don't {disfmarker} I don't think they are .
Turn 662, A (PhD): Hmm .
Turn 663, B (Professor): I mean that 's {disfmarker} sort of what {disfmarker} the way I had imagined would happen is that on the other side , yeah you p put some low level noise or something . Probably don't want all zeros .
Turn 664, A (PhD): Hmm .
Turn 665, B (Professor): Most recognizers don't like zeros but {vocalsound} but {pause} you know , {pause} put some epsilon in or some rand
Turn 666, A (PhD): Yeah .
Turn 667, B (Professor): sorry epsilon random variable {pause} in or something .
Turn 668, A (PhD): Some constant vector . I mean i w Or something {disfmarker}
Turn 669, B (Professor): Maybe not a constant but it doesn't , uh {disfmarker} don't like to divide by the variance of that , but I mean it 's
Turn 670, A (PhD): That 's right . But something that {disfmarker} what I mean is something that is {pause} very distinguishable from {pause} speech .
Turn 671, B (Professor): Mm - hmm .
Turn 672, A (PhD): So that the {disfmarker} the silence model in HTK will always pick it up .
Turn 673, B (Professor): Yeah . So I {disfmarker} I {disfmarker} that 's what I thought they would do . or else , uh {pause} uh maybe there is some indicator to tell it to start and stop , I don't know .
Turn 674, A (PhD): Hmm .
Turn 675, B (Professor): But whatever they did , I mean they have to play within the rules of this specific evaluation .
Turn 676, A (PhD): Yeah .
Turn 677, B (Professor): We c we can find out .
Turn 678, A (PhD): Cuz you gotta do something . Otherwise , if it 's just a bunch of speech , stuck together {disfmarker}
Turn 679, B (Professor): No they 're {disfmarker}
Turn 680, A (PhD): Yeah .
Turn 681, B (Professor): It would do badly
Turn 682, A (PhD): Yeah , right .
Turn 683, B (Professor): and it didn't so badly , right ? So they did something .
Turn 684, A (PhD): Yeah , yeah .
Turn 685, B (Professor): Yeah . Uh . So , OK , So I think {pause} this brings me up to date a bit . It hopefully brings other {pause} people up to date a bit . And um Um {pause} I think {disfmarker} Uh , I wanna look at these numbers off - line a little bit and think about it and {disfmarker} {pause} and talk with everybody uh , {pause} outside of this meeting . Um , but uh No I mean it sounds like {disfmarker} I mean {pause} there {disfmarker} there {disfmarker} there are the usual number of {disfmarker} of {pause} little {disfmarker} little problems and bugs and so forth but it sounds like they 're getting ironed out . And now we 're {pause} seem to be kind of in a position to actually {pause} uh , {pause} look at stuff and {disfmarker} and {disfmarker} and compare things . So I think that 's {disfmarker} that 's pretty good . Um {pause} I don't know what the {disfmarker} One of the things I wonder about , {pause} coming back to the first results you talked about , is {disfmarker} is {pause} how much , {pause} uh {pause} things could be helped {pause} by more parameters . And uh {disfmarker} {pause} And uh how many more parameters we can afford to have , {vocalsound} {pause} in terms of the uh computational limits . Because anyway when we go to {pause} twice as much data {pause} and have the same number of parameters , particularly when it 's twice as much data and it 's quite diverse , um , I wonder if having twice as many parameters would help .
Turn 686, D (PhD): Mm - hmm .
Turn 687, B (Professor): Uh , just have a bigger hidden layer . Uh But {disfmarker} I doubt it would {pause} help by forty per cent . But {vocalsound} {pause} but uh
Turn 688, D (PhD): Yeah .
Turn 689, B (Professor): Just curious . How are we doing on the {pause} resources ? Disk , and {disfmarker}
Turn 690, D (PhD): I think we 're alright ,
Turn 691, B (Professor): OK .
Turn 692, D (PhD): um , {pause} not much problems with that .
Turn 693, B (Professor): Computation ?
Turn 694, D (PhD): It 's OK .
Turn 695, B (Professor): We {disfmarker}
Turn 696, D (PhD): Well this table took uh {pause} more than five days to get back .
Turn 697, B (Professor): Yeah . Yeah , well .
Turn 698, D (PhD): But {disfmarker} Yeah .
Turn 699, B (Professor): Are {disfmarker} were you folks using Gin ? That 's a {disfmarker} that just died , you know ?
Turn 700, D (PhD): Mmm , no . You were using Gin {comment} perhaps , yeah ? No .
Turn 701, E (PhD): No .
Turn 702, B (Professor): No ? Oh , that 's good .
Turn 703, F (Grad): It just died .
Turn 704, B (Professor): OK . Yeah , {pause} we 're gonna get a replacement {pause} server that 'll be a faster server , {pause} actually .
Turn 705, E (PhD): Yes .
Turn 706, B (Professor): That 'll be {disfmarker} It 's a {pause} seven hundred fifty megahertz uh SUN
Turn 707, D (PhD): Hmm . {comment} Mm - hmm .
Turn 708, B (Professor): uh {pause} But it won't be installed for {pause} a little while .
Turn 709, C (PhD): Tonic .
Turn 710, B (Professor): U Go ahead .
Turn 711, G (Grad): Do we {disfmarker} Do we have that big new IBM machine the , I think in th
Turn 712, B (Professor): We have the {pause} little tiny IBM machine {vocalsound} {pause} that might someday grow up to be a big {pause} IBM machine . It 's got s slots for eight , uh IBM was donating five , I think we only got two so far , processors . We had originally hoped we were getting eight hundred megahertz processors . They ended up being five fifty . So instead of having eight processors that were eight hundred megahertz , we ended up with two {pause} that are five hundred and fifty megahertz . And more are supposed to come soon and there 's only a moderate amount of dat of memory . So I don't think {pause} anybody has been sufficiently excited by it to {pause} spend much time {pause} uh {pause} with it , but uh {vocalsound} Hopefully , {pause} they 'll get us some more {pause} parts , soon and {disfmarker} Uh , yeah , I think that 'll be {disfmarker} once we get it populated , {pause} that 'll be a nice machine . I mean we will ultimately get eight processors in there . And uh {disfmarker} and uh a nice amount of memory . Uh so it 'll be a pr pretty fast Linux machine .
Turn 713, G (Grad): And if we can do things on Linux , {pause} some of the machines we have going already , like Swede ?
Turn 714, B (Professor): Mm - hmm .
Turn 715, G (Grad): Um It seems pretty fast .
Turn 716, B (Professor): Mm - hmm .
Turn 717, G (Grad): But {disfmarker} I think Fudge is pretty fast too .
Turn 718, B (Professor): Yeah , I mean you can check with uh {pause} Dave Johnson . I mean , it {disfmarker} it 's {disfmarker} {pause} I think the machine is just sitting there . And it does have two processors , you know and {disfmarker} {pause} Somebody could do {disfmarker} {pause} you know , uh , check out {pause} uh the multi - threading {pause} libraries . And {pause} I mean i it 's possible that the {disfmarker} I mean , I guess the prudent thing to do would be for somebody to do the work on {disfmarker} {pause} on getting our code running {pause} on that machine with two processors {pause} even though there aren't five or eight . There 's {disfmarker} there 's {disfmarker} there 's gonna be debugging hassles and then we 'd be set for when we did have five or eight , to have it really be useful . But . {pause} Notice how I said somebody and {vocalsound} turned my head your direction . That 's one thing you don't get in these recordings . You don't get the {disfmarker} {pause} don't get the visuals but {disfmarker}
Turn 719, G (Grad): I is it um {pause} mostly um the neural network trainings that are {pause} um slowing us down or the HTK runs that are slowing us down ?
Turn 720, B (Professor): Uh , I think yes . Uh , {vocalsound} Isn't that right ? I mean I think you 're {disfmarker} you 're sort of held up by both , right ? If the {disfmarker} if the neural net trainings were a hundred times faster {pause} you still wouldn't {pause} be anything {disfmarker} running through these a hundred times faster because you 'd {pause} be stuck by the HTK trainings ,
Turn 721, D (PhD): Mmm .
Turn 722, B (Professor): right ?
Turn 723, D (PhD): Yeah .
Turn 724, B (Professor): But if the HTK {disfmarker} I mean I think they 're both {disfmarker} It sounded like they were roughly equal ? Is that about right ?
Turn 725, D (PhD): Yeah .
Turn 726, B (Professor): Yeah .
Turn 727, G (Grad): Because , um {pause} I think that 'll be running Linux , and Sw - Swede and Fudge are already running Linux so , {pause} um I could try to get {pause} um the train the neural network trainings or the HTK stuff running under Linux , and to start with I 'm {pause} wondering which one I should pick first .
Turn 728, B (Professor): Uh , probably the neural net cuz it 's probably {disfmarker} it {disfmarker} it 's {disfmarker} {pause} it 's um {disfmarker} Well , I {disfmarker} I don't know . They both {disfmarker} HTK we use for {pause} um {pause} this Aurora stuff Um {pause} Um , I think {pause} It 's not clear yet what we 're gonna use {pause} for trainings uh {disfmarker} Well , {pause} there 's the trainings uh {disfmarker} is it the training that takes the time , or the decoding ? Uh , is it about equal {pause} between the two ? For {disfmarker} for Aurora ?
Turn 729, D (PhD): For HTK ?
Turn 730, B (Professor): For {disfmarker} Yeah . For the Aurora ?
Turn 731, D (PhD): Uh Training is longer .
Turn 732, B (Professor): OK .
Turn 733, D (PhD): Yeah .
Turn 734, B (Professor): OK . Well , I don't know how we can {disfmarker} I don't know how to {disfmarker} Do we have HTK source ? Is that {disfmarker} Yeah .
Turn 735, D (PhD): Mmm .
Turn 736, B (Professor): You would think that would fairly trivially {disfmarker} the training would , anyway , th the testing {pause} uh I don't {disfmarker} I don't {pause} think would {pause} parallelize all that well . But I think {pause} that {pause} you could {pause} certainly do d um , {pause} distributed , sort of {disfmarker} {pause} Ah , no , it 's the {disfmarker} {pause} each individual {pause} sentence is pretty tricky to parallelize . But you could split up the sentences in a test set .
Turn 737, A (PhD): They have a {disfmarker} they have a thing for doing that and th they have for awhile , in H T And you can parallelize the training .
Turn 738, B (Professor): Yeah ?
Turn 739, A (PhD): And run it on several machines
Turn 740, B (Professor): Aha !
Turn 741, A (PhD): and it just basically keeps counts . And there 's something {disfmarker} {pause} a final {pause} thing that you run and it accumulates all the counts together .
Turn 742, B (Professor): I see .
Turn 743, D (PhD): Mmm .
Turn 744, A (PhD): I don't what their scripts are {pause} set up to do for the Aurora stuff , but {disfmarker}
Turn 745, D (PhD): Yeah .
Turn 746, B (Professor): Something that we haven't really settled on yet is other than {pause} this Aurora stuff , {pause} uh what do we do , large vocabulary {pause} training slash testing {pause} for uh tandem systems . Cuz we hadn't really done much with tandem systems for larger stuff . Cuz we had this one collaboration with CMU and we used SPHINX . Uh , we 're also gonna be collaborating with SRI and we have their {disfmarker} have theirs . Um {pause} So {pause} I don't know Um . So I {disfmarker} I think the {disfmarker} the advantage of going with the neural net thing is that we 're gonna use the neural net trainings , no matter what , for a lot of the things we 're doing ,
Turn 747, G (Grad): OK .
Turn 748, B (Professor): whereas , w exactly which HMM {disfmarker} Gaussian - mixture - based HMM thing we use is gonna depend uh So with that , maybe we should uh {vocalsound} go to our {nonvocalsound} digit recitation task . And , it 's about eleven fifty . Canned . Uh , I can {disfmarker} I can start over here . Great , uh , could you give Adam a call . Tell him to He 's at two nine seven seven .
Turn 749, F (Grad): Oh .
Turn 750, B (Professor): OK . I think we can {vocalsound} @ @ You know Herve 's coming tomorrow , right ? Herve will be giving a talk , yeah , talk at eleven . Did uh , did everybody sign these consent Er everybody Has everyone signed a consent form before , on previous meetings ? You don't have to do it again each time Yes . microphones off
