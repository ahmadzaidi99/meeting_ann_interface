{"metadata": {"meeting_name": "Bro023"}, "turns": [{"turn": 1, "name": "PhD", "id": "A", "contribution": " OK , we 're going ."}, {"turn": 2, "name": "PhD", "id": "D", "contribution": " Damn ."}, {"turn": 3, "name": "Professor", "id": "C", "contribution": " And uh Hans - uh , Hans - Guenter will be here , um , I think by next {disfmarker} next Tuesday or so ."}, {"turn": 4, "name": "PhD", "id": "B", "contribution": " Oh , OK ."}, {"turn": 5, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 6, "name": "Professor", "id": "C", "contribution": " So he 's {disfmarker} he 's going to be here for about three weeks ,"}, {"turn": 7, "name": "PhD", "id": "B", "contribution": " Oh ! That 's nice ."}, {"turn": 8, "name": "PhD", "id": "A", "contribution": " Just for a visit ?"}, {"turn": 9, "name": "Professor", "id": "C", "contribution": " and , uh {disfmarker} Uh , we 'll see ."}, {"turn": 10, "name": "PhD", "id": "A", "contribution": " Huh ."}, {"turn": 11, "name": "Professor", "id": "C", "contribution": " We might {disfmarker} might end up with some longer collaboration or something ."}, {"turn": 12, "name": "PhD", "id": "A", "contribution": " Cool ."}, {"turn": 13, "name": "Professor", "id": "C", "contribution": " So he 's gonna look in on everything we 're doing"}, {"turn": 14, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 15, "name": "Professor", "id": "C", "contribution": " and give us his {disfmarker} his thoughts . And so it 'll be another {disfmarker} another good person looking at things ."}, {"turn": 16, "name": "PhD", "id": "B", "contribution": " Oh . Hmm ."}, {"turn": 17, "name": "Grad", "id": "E", "contribution": " Th - that 's his spectral subtraction group ?"}, {"turn": 18, "name": "Professor", "id": "C", "contribution": " Yeah ,"}, {"turn": 19, "name": "Grad", "id": "E", "contribution": " Is that right ?"}, {"turn": 20, "name": "Professor", "id": "C", "contribution": " yeah ."}, {"turn": 21, "name": "Grad", "id": "E", "contribution": " Oh , OK . So I guess I should probably talk to him a bit too ?"}, {"turn": 22, "name": "Professor", "id": "C", "contribution": " Oh , yeah . Yeah . Yeah . No , he 'll be around for three weeks . He 's , uh , um , very , very , easygoing , easy to talk to , and , uh , very interested in everything ."}, {"turn": 23, "name": "PhD", "id": "A", "contribution": " Really nice guy ."}, {"turn": 24, "name": "Professor", "id": "C", "contribution": " Yeah , yeah ."}, {"turn": 25, "name": "PhD", "id": "B", "contribution": " Yeah , we met him in Amsterdam ."}, {"turn": 26, "name": "Professor", "id": "C", "contribution": " Yeah , yeah , he 's been here before ."}, {"turn": 27, "name": "PhD", "id": "B", "contribution": " Oh , OK ."}, {"turn": 28, "name": "Professor", "id": "C", "contribution": " I mean , he 's {disfmarker} he 's {disfmarker} he 's {disfmarker} he 's {disfmarker}"}, {"turn": 29, "name": "PhD", "id": "A", "contribution": " Wh - Back when I was a grad student he was here for a , uh , uh {disfmarker} a year or {comment} n six months ."}, {"turn": 30, "name": "PhD", "id": "B", "contribution": " I haven't noticed him ."}, {"turn": 31, "name": "Professor", "id": "C", "contribution": " N nine months ."}, {"turn": 32, "name": "PhD", "id": "A", "contribution": " Something like that ."}, {"turn": 33, "name": "Professor", "id": "C", "contribution": " Something like that ."}, {"turn": 34, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 35, "name": "Professor", "id": "C", "contribution": " Yeah . Yeah . He 's {disfmarker} he 's done a couple stays here ."}, {"turn": 36, "name": "PhD", "id": "B", "contribution": " Hmm ."}, {"turn": 37, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 38, "name": "PhD", "id": "A", "contribution": " So , um , {vocalsound} {comment} I guess we got lots to catch up on . And we haven't met for a couple of weeks . We didn't meet last week , Morgan . Um , I went around and talked to everybody , and it seemed like they {disfmarker} they had some new results but rather than them coming up and telling me I figured we should just wait a week and they can tell both {disfmarker} you know , all of us . So , um , why don't we {disfmarker} why don't we start with you , Dave , and then , um , we can go on ."}, {"turn": 39, "name": "Grad", "id": "E", "contribution": " Oh , OK ."}, {"turn": 40, "name": "PhD", "id": "A", "contribution": " So ."}, {"turn": 41, "name": "Grad", "id": "E", "contribution": " So , um , since we 're looking at putting this , um {disfmarker} mean log m magnitude spectral subtraction , um , into the SmartKom system , I I did a test seeing if , um , it would work using past only {comment} and plus the present to calculate the mean . So , I did a test , um , {vocalsound} where I used twelve seconds from the past and the present frame to , um , calculate the mean . And {disfmarker}"}, {"turn": 42, "name": "PhD", "id": "A", "contribution": " Twelve seconds {disfmarker} Twelve {disfmarker} twelve seconds back from the current {pause} frame , is that what you mean ?"}, {"turn": 43, "name": "Grad", "id": "E", "contribution": " Uh {disfmarker} Twelve seconds , um , counting back from the end of the current frame ,"}, {"turn": 44, "name": "PhD", "id": "A", "contribution": " OK , OK ."}, {"turn": 45, "name": "Grad", "id": "E", "contribution": " yeah . So it was , um , twen I think it was twenty - one frames and that worked out to about twelve seconds ."}, {"turn": 46, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 47, "name": "Grad", "id": "E", "contribution": " And compared to , um , do using a twelve second centered window , I think there was a drop in performance but it was just a slight drop ."}, {"turn": 48, "name": "PhD", "id": "A", "contribution": " Hmm !"}, {"turn": 49, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 50, "name": "Grad", "id": "E", "contribution": " Is {disfmarker} is that right ?"}, {"turn": 51, "name": "Professor", "id": "C", "contribution": " Um , yeah , I mean , it was pretty {disfmarker} it was pretty tiny . Yeah ."}, {"turn": 52, "name": "Grad", "id": "E", "contribution": " Uh - huh . So that was encouraging . And , um , that {disfmarker} that {disfmarker} um , that 's encouraging for {disfmarker} for the idea of using it in an interactive system like And , um , another issue I 'm {disfmarker} I 'm thinking about is in the SmartKom system . So say twe twelve seconds in the earlier test seemed like a good length of time , but what happens if you have less than twelve seconds ? And , um {disfmarker} So I w bef before , um {disfmarker} Back in May , I did some experiments using , say , two seconds , or four seconds , or six seconds . In those I trained the models using mean subtraction with the means calculated over two seconds , or four seconds , or six seconds . And , um , here , I was curious , what if I trained the models using twelve seconds but I f I gave it a situation where the test set I was {disfmarker} subtracted using two seconds , or four seconds , or six seconds . And , um {disfmarker} So I did that for about three different conditions . And , um {disfmarker} I mean , I th I think it was , um , four se I think {disfmarker} I think it was , um , something like four seconds and , um , six seconds , and eight seconds . Something like that . And it seems like it {disfmarker} it {disfmarker} it hurts compared to if you actually train the models {comment} using th that same length of time but it {disfmarker} it doesn't hurt that much . Um , u usually less than point five percent , although I think I did see one where it was a point eight percent or so rise in word error rate . But this is , um , w where , um , even if I train on the , uh , model , and mean subtracted it with the same length of time as in the test , it {disfmarker} the word error rate is around , um , ten percent or nine percent . So it doesn't seem like that big a d a difference ."}, {"turn": 53, "name": "Professor", "id": "C", "contribution": " But it {disfmarker} but looking at it the other way , isn't it {disfmarker} what you 're saying that it didn't help you to have the longer time for training , if you were going to have a short time for {disfmarker}"}, {"turn": 54, "name": "Grad", "id": "E", "contribution": " That {disfmarker} that 's true . Um ,"}, {"turn": 55, "name": "Professor", "id": "C", "contribution": " I mean , why would you do it , if you knew that you were going to have short windows in testing ."}, {"turn": 56, "name": "Grad", "id": "E", "contribution": " Wa"}, {"turn": 57, "name": "PhD", "id": "A", "contribution": " Yeah , it seems like for your {disfmarker} I mean , in normal situations you would never get twelve seconds of speech , right ? I 'm not {disfmarker} e u"}, {"turn": 58, "name": "PhD", "id": "B", "contribution": " You need twelve seconds in the past to estimate , right ? Or l or you 're looking at six sec {disfmarker} seconds in future and six in {disfmarker}"}, {"turn": 59, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 60, "name": "Grad", "id": "E", "contribution": " Um , t twelve s"}, {"turn": 61, "name": "Professor", "id": "C", "contribution": " No , total ."}, {"turn": 62, "name": "Grad", "id": "E", "contribution": " N n uh {disfmarker} For the test it 's just twelve seconds in the past ."}, {"turn": 63, "name": "PhD", "id": "B", "contribution": " No , it 's all {disfmarker} Oh , OK ."}, {"turn": 64, "name": "PhD", "id": "A", "contribution": " Is this twelve seconds of {disfmarker} uh , regardless of speech or silence ? Or twelve seconds of speech ?"}, {"turn": 65, "name": "Grad", "id": "E", "contribution": " Of {disfmarker} of speech ."}, {"turn": 66, "name": "PhD", "id": "A", "contribution": " OK ."}, {"turn": 67, "name": "PhD", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 68, "name": "Professor", "id": "C", "contribution": " The other thing , um , which maybe relates a little bit to something else we 've talked about in terms of windowing and so on is , that , um , I wonder if you trained with twelve seconds , and then when you were two seconds in you used two seconds , and when you were four seconds in , you used four seconds , and when you were six {disfmarker} and you basically build up to the twelve seconds . So that if you have very long utterances you have the best ,"}, {"turn": 69, "name": "Grad", "id": "E", "contribution": " Yeah ."}, {"turn": 70, "name": "Professor", "id": "C", "contribution": " but if you have shorter utterances you use what you can ."}, {"turn": 71, "name": "Grad", "id": "E", "contribution": " Right . And that 's actually what we 're planning to do in"}, {"turn": 72, "name": "Professor", "id": "C", "contribution": " OK . Yeah ."}, {"turn": 73, "name": "Grad", "id": "E", "contribution": " But {disfmarker} s so I g So I guess the que the question I was trying to get at with those experiments is , \" does it matter what models you use ? Does it matter how much time y you use to calculate the mean when you were , um , tra doing the training data ? \""}, {"turn": 74, "name": "Professor", "id": "C", "contribution": " Right . But I mean the other thing is that that 's {disfmarker} I mean , the other way of looking at this , going back to , uh , mean cepstral subtraction versus RASTA kind of things , is that you could look at mean cepstral subtraction , especially the way you 're doing it , uh , as being a kind of filter . And so , the other thing is just to design a filter . You know , basically you 're {disfmarker} you 're {disfmarker} you 're doing a high - pass filter or a band - pass filter of some sort and {disfmarker} and just design a filter . And then , you know , a filter will have a certain behavior and you loo can look at the start up behavior when you start up with nothing ."}, {"turn": 75, "name": "Grad", "id": "E", "contribution": " Mm - hmm ."}, {"turn": 76, "name": "Professor", "id": "C", "contribution": " And {disfmarker} and , you know , it will , uh , if you have an IIR filter for instance , it will , um , uh , not behave in the steady - state way that you would like it to behave until you get a long enough period , but , um , uh , by just constraining yourself to have your filter be only a subtraction of the mean , you 're kind of , you know , tying your hands behind your back because there 's {disfmarker} filters have all sorts of be temporal and spectral behaviors ."}, {"turn": 77, "name": "Grad", "id": "E", "contribution": " Mm - hmm ."}, {"turn": 78, "name": "Professor", "id": "C", "contribution": " And the only thing , you know , consistent that we know about is that you want to get rid of the very low frequency component ."}, {"turn": 79, "name": "Grad", "id": "E", "contribution": " Hmm ."}, {"turn": 80, "name": "PhD", "id": "B", "contribution": " But do you really want to calculate the mean ? And you neglect all the silence regions {comment} or you just use everything that 's twelve seconds , and {disfmarker}"}, {"turn": 81, "name": "Grad", "id": "E", "contribution": " Um , you {disfmarker} do you mean in my tests so far ?"}, {"turn": 82, "name": "PhD", "id": "B", "contribution": " Ye - yeah ."}, {"turn": 83, "name": "Grad", "id": "E", "contribution": " Most of the silence has been cut out ."}, {"turn": 84, "name": "PhD", "id": "B", "contribution": " OK ."}, {"turn": 85, "name": "Grad", "id": "E", "contribution": " Just {disfmarker} There 's just inter - word silences ."}, {"turn": 86, "name": "PhD", "id": "B", "contribution": " Mm - hmm . And they are , like , pretty short . Shor"}, {"turn": 87, "name": "Grad", "id": "E", "contribution": " Pretty short ."}, {"turn": 88, "name": "PhD", "id": "B", "contribution": " Yeah , OK ."}, {"turn": 89, "name": "Grad", "id": "E", "contribution": " Yeah ."}, {"turn": 90, "name": "PhD", "id": "B", "contribution": " Yeah . Mm - hmm . So you really need a lot of speech to estimate the mean of it ."}, {"turn": 91, "name": "Grad", "id": "E", "contribution": " Well , if I only use six seconds , it still works pretty well ."}, {"turn": 92, "name": "PhD", "id": "B", "contribution": " Yeah . Yeah . Uh - huh ."}, {"turn": 93, "name": "Grad", "id": "E", "contribution": " I saw in my test before . I was trying twelve seconds cuz that was the best {pause} in my test before"}, {"turn": 94, "name": "PhD", "id": "B", "contribution": " OK ."}, {"turn": 95, "name": "Grad", "id": "E", "contribution": " and that increasing past twelve seconds didn't seem to help ."}, {"turn": 96, "name": "PhD", "id": "B", "contribution": " Hmm . Huh ."}, {"turn": 97, "name": "Grad", "id": "E", "contribution": " th um , yeah , I guess it 's something I need to play with more to decide how to set that up for the SmartKom system . Like , may maybe if I trained on six seconds it would work better when I only had two seconds or four seconds , and {disfmarker}"}, {"turn": 98, "name": "Professor", "id": "C", "contribution": " Yeah . Yeah . And , um {disfmarker}"}, {"turn": 99, "name": "Grad", "id": "E", "contribution": " OK ."}, {"turn": 100, "name": "Professor", "id": "C", "contribution": " Yeah , and again , if you take this filtering perspective and if you essentially have it build up over time . I mean , if you computed means over two and then over four , and over six , essentially what you 're getting at is a kind of , uh , ramp up of a filter anyway . And so you may {disfmarker} may just want to think of it as a filter . But , uh , if you do that , then , um , in practice somebody using the SmartKom system , one would think {comment} {disfmarker} if they 're using it for a while , it means that their first utterance , instead of , you know , getting , uh , a forty percent error rate reduction , they 'll get a {disfmarker} uh , over what , uh , you 'd get without this , uh , um , policy , uh , you get thirty percent . And then the second utterance that you give , they get the full {disfmarker} you know , uh , full benefit of it if it 's this ongoing thing ."}, {"turn": 101, "name": "PhD", "id": "A", "contribution": " Oh , so you {disfmarker} you cache the utterances ? That 's how you get your , uh {disfmarker}"}, {"turn": 102, "name": "Professor", "id": "C", "contribution": " Well , I 'm saying in practice , yeah ,"}, {"turn": 103, "name": "Grad", "id": "E", "contribution": " M"}, {"turn": 104, "name": "PhD", "id": "A", "contribution": " Ah . OK ."}, {"turn": 105, "name": "Professor", "id": "C", "contribution": " that 's {disfmarker} If somebody 's using a system to ask for directions or something ,"}, {"turn": 106, "name": "PhD", "id": "A", "contribution": " OK ."}, {"turn": 107, "name": "Professor", "id": "C", "contribution": " you know , they 'll say something first . And {disfmarker} and to begin with if it doesn't get them quite right , ma m maybe they 'll come back and say , \" excuse me ? \""}, {"turn": 108, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 109, "name": "Professor", "id": "C", "contribution": " uh , or some {disfmarker} I mean it should have some policy like that anyway ."}, {"turn": 110, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 111, "name": "Professor", "id": "C", "contribution": " And {disfmarker} and , uh , uh , in any event they might ask a second question . And it 's not like what he 's doing doesn't , uh , improve things . It does improve things , just not as much as he would like . And so , uh , there 's a higher probability of it making an error , uh , in the first utterance ."}, {"turn": 112, "name": "PhD", "id": "A", "contribution": " What would be really cool is if you could have {disfmarker} uh , this probably {disfmarker} users would never like this {disfmarker} but if you had {disfmarker} could have a system where , {vocalsound} before they began to use it they had to introduce themselves , verbally ."}, {"turn": 113, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 114, "name": "PhD", "id": "A", "contribution": " You know . \" Hi , my name is so - and - so ,"}, {"turn": 115, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 116, "name": "PhD", "id": "A", "contribution": " I 'm from blah - blah - blah . \" And you could use that initial speech to do all these adaptations and {disfmarker}"}, {"turn": 117, "name": "Professor", "id": "C", "contribution": " Right ."}, {"turn": 118, "name": "Grad", "id": "E", "contribution": " Mm - hmm ."}, {"turn": 119, "name": "Professor", "id": "C", "contribution": " Oh , the other thing I guess which {disfmarker} which , uh , I don't know much about {disfmarker} as much as I should about the rest of the system but {disfmarker} but , um , couldn't you , uh , if you {disfmarker} if you sort of did a first pass I don't know what kind of , uh , uh , capability we have at the moment for {disfmarker} for doing second passes on {disfmarker} on , uh , uh , some kind of little {disfmarker} small lattice , or a graph , or confusion network , or something . But if you did first pass with , um , the {disfmarker} with {disfmarker} either without the mean sub subtraction or with a {disfmarker} a very short time one , and then , um , once you , uh , actually had the whole utterance in , if you did , um , the , uh , uh , longer time version then , based on everything that you had , um , and then at that point only used it to distinguish between , you know , top N , um , possible utterances or something , you {disfmarker} you might {disfmarker} it might not take very much time . I mean , I know in the large vocabulary stu uh , uh , systems , people were evaluating on in the past , some people really pushed everything in to make it in one pass but other people didn't and had multiple passes . And , um , the argument , um , against multiple passes was u u has often been \" but we want to this to be r you know {disfmarker} have a nice interactive response \" . And the counterargument to that which , say , uh , BBN I think had , {comment} was \" yeah , but our second responses are {disfmarker} second , uh , passes and third passes are really , really fast \" ."}, {"turn": 120, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 121, "name": "Professor", "id": "C", "contribution": " So , um , if {disfmarker} if your second pass takes a millisecond who cares ? Um ."}, {"turn": 122, "name": "Grad", "id": "E", "contribution": " S so , um , the {disfmarker} the idea of the second pass would be waiting till you have more recorded speech ? Or {disfmarker} ?"}, {"turn": 123, "name": "Professor", "id": "C", "contribution": " Yeah , so if it turned out to be a problem , that you didn't have enough speech because you need a longer {disfmarker} longer window to do this processing , then , uh , one tactic is {disfmarker} you know , looking at the larger system and not just at the front - end stuff {comment} {disfmarker} is to take in , um , the speech with some simpler mechanism or shorter time mechanism ,"}, {"turn": 124, "name": "Grad", "id": "E", "contribution": " Mm - hmm ."}, {"turn": 125, "name": "Professor", "id": "C", "contribution": " um , do the best you can , and come up with some al possible alternates of what might have been said . And , uh , either in the form of an N - best list or in the form of a lattice , or {disfmarker} or confusion network , or whatever ."}, {"turn": 126, "name": "Grad", "id": "E", "contribution": " Mm - hmm ."}, {"turn": 127, "name": "Professor", "id": "C", "contribution": " And then the decoding of that is much , much faster or can be much , much faster if it isn't a big bushy network . And you can decode that now with speech that you 've actually processed using this longer time , uh , subtraction ."}, {"turn": 128, "name": "Grad", "id": "E", "contribution": " Mmm ."}, {"turn": 129, "name": "Professor", "id": "C", "contribution": " So I mean , it 's {disfmarker} it 's common that people do this sort of thing where they do more things that are more complex or require looking over more time , whatever , in some kind of second pass ."}, {"turn": 130, "name": "Grad", "id": "E", "contribution": " Mm - hmm . OK ."}, {"turn": 131, "name": "Professor", "id": "C", "contribution": " um , and again , if the second pass is really , really fast {disfmarker} Uh , another one I 've heard of is {disfmarker} is in {disfmarker} in connected digit stuff , um , going back and l and through backtrace and finding regions that are considered to be a d a digit , but , uh , which have very low energy ."}, {"turn": 132, "name": "Grad", "id": "E", "contribution": " Mm - hmm . OK ."}, {"turn": 133, "name": "Professor", "id": "C", "contribution": " So , uh {disfmarker} I mean , there 's lots of things you can do in second passes , at all sorts of levels . Anyway , I 'm throwing too many things out . But ."}, {"turn": 134, "name": "PhD", "id": "A", "contribution": " So is that , uh {disfmarker} that it ?"}, {"turn": 135, "name": "Grad", "id": "E", "contribution": " I guess that 's it ."}, {"turn": 136, "name": "PhD", "id": "A", "contribution": " OK , uh , do you wanna go , Sunil ?"}, {"turn": 137, "name": "PhD", "id": "B", "contribution": " Yep . Um , so , the last two weeks was , like {disfmarker} So I 've been working on that Wiener filtering . And , uh , found that , uh , s single {disfmarker} like , I just do a s normal Wiener filtering , like the standard method of Wiener filtering . And that doesn't actually give me any improvement over like {disfmarker} I mean , uh , b it actually improves over the baseline but it 's not like {disfmarker} it doesn't meet something like fifty percent or something . So , I 've been playing with the v"}, {"turn": 138, "name": "PhD", "id": "A", "contribution": " Improves over the base line MFCC system ? Yeah ."}, {"turn": 139, "name": "PhD", "id": "B", "contribution": " Yeah . Yeah . Yeah . So , um {disfmarker} So that 's {disfmarker} The improvement is somewhere around , like , thirty percent over the baseline ."}, {"turn": 140, "name": "Professor", "id": "C", "contribution": " Is that using {disfmarker} in combination with something else ?"}, {"turn": 141, "name": "PhD", "id": "B", "contribution": " No , just {disfmarker} just one stage Wiener filter"}, {"turn": 142, "name": "Professor", "id": "C", "contribution": " With {disfmarker} with a {disfmarker}"}, {"turn": 143, "name": "PhD", "id": "B", "contribution": " which is a standard Wiener filter ."}, {"turn": 144, "name": "Professor", "id": "C", "contribution": " No , no , but I mean in combination with our on - line normalization or with the LDA ?"}, {"turn": 145, "name": "PhD", "id": "B", "contribution": " Yeah , yeah , yeah , yeah . So I just plug in the Wiener filtering ."}, {"turn": 146, "name": "Professor", "id": "C", "contribution": " Oh , OK ."}, {"turn": 147, "name": "PhD", "id": "B", "contribution": " I mean , in the s in our system , where {disfmarker}"}, {"turn": 148, "name": "PhD", "id": "A", "contribution": " Oh , OK ."}, {"turn": 149, "name": "PhD", "id": "B", "contribution": " So , I di i di"}, {"turn": 150, "name": "Professor", "id": "C", "contribution": " So , does it g does that mean it gets worse ? Or {disfmarker} ?"}, {"turn": 151, "name": "PhD", "id": "B", "contribution": " No . It actually improves over the baseline of not having a Wiener filter in the whole system . Like I have an LDA f LDA plus on - line normalization , and then I plug in the Wiener filter in that ,"}, {"turn": 152, "name": "Professor", "id": "C", "contribution": " Yeah ?"}, {"turn": 153, "name": "PhD", "id": "B", "contribution": " so it improves over not having the Wiener filter . So it improves but it {disfmarker} it doesn't take it like be beyond like thirty percent over the baseline . So {disfmarker}"}, {"turn": 154, "name": "Professor", "id": "C", "contribution": " But that 's what I 'm confused about , cuz I think {disfmarker} I thought that our system was more like forty percent without the Wiener filtering ."}, {"turn": 155, "name": "PhD", "id": "B", "contribution": " No , it 's like , uh ,"}, {"turn": 156, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 157, "name": "PhD", "id": "A", "contribution": " Is this with the v new VAD ?"}, {"turn": 158, "name": "PhD", "id": "B", "contribution": " well , these are not {disfmarker} No , it 's the old VAD . So my baseline was , {vocalsound} uh , {vocalsound} nine {disfmarker} This is like {disfmarker} w the baseline is ninety - five point six eight , and eighty - nine , and {disfmarker}"}, {"turn": 159, "name": "Professor", "id": "C", "contribution": " So I mean , if you can do all these in word errors it 's a lot {disfmarker} a lot easier actually ."}, {"turn": 160, "name": "PhD", "id": "B", "contribution": " What was that ? Sorry ?"}, {"turn": 161, "name": "Professor", "id": "C", "contribution": " If you do all these in word error rates it 's a lot easier , right ?"}, {"turn": 162, "name": "PhD", "id": "B", "contribution": " Oh , OK , OK , OK . Errors , right , I don't have ."}, {"turn": 163, "name": "Professor", "id": "C", "contribution": " OK , cuz then you can figure out the percentages ."}, {"turn": 164, "name": "PhD", "id": "B", "contribution": " It 's all accuracies ."}, {"turn": 165, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 166, "name": "PhD", "id": "D", "contribution": " The baseline is something similar to a w I mean , the t the {disfmarker} the baseline that you are talking about is the MFCC baseline , right ?"}, {"turn": 167, "name": "PhD", "id": "B", "contribution": " The t yeah , there are two baselines ."}, {"turn": 168, "name": "PhD", "id": "D", "contribution": " Or {disfmarker} ?"}, {"turn": 169, "name": "PhD", "id": "B", "contribution": " OK . So the baseline {disfmarker} One baseline is MFCC baseline that {disfmarker} When I said thirty percent improvement it 's like MFCC baseline ."}, {"turn": 170, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 171, "name": "Professor", "id": "C", "contribution": " So {disfmarker} so {disfmarker} so what 's it start on ? The MFCC baseline is {disfmarker} is what ? Is at what level ?"}, {"turn": 172, "name": "PhD", "id": "B", "contribution": " It 's the {disfmarker} it 's just the mel frequency and that 's it ."}, {"turn": 173, "name": "Professor", "id": "C", "contribution": " No , what 's {disfmarker} what 's the number ?"}, {"turn": 174, "name": "PhD", "id": "B", "contribution": " Uh , so I I don't have that number here . OK , OK , OK , I have it here . Uh , it 's the VAD plus the baseline actually . I 'm talking about the {disfmarker} the MFCC plus I do a frame dropping on it . So that 's like {disfmarker} the word error rate is like four point three . Like {disfmarker} Ten point seven ."}, {"turn": 175, "name": "Professor", "id": "C", "contribution": " Four point three . What 's ten point seven ?"}, {"turn": 176, "name": "PhD", "id": "B", "contribution": " It 's a medium misma OK , sorry . There 's a well ma well matched , medium mismatched , and a high matched ."}, {"turn": 177, "name": "Professor", "id": "C", "contribution": " Ah ."}, {"turn": 178, "name": "PhD", "id": "B", "contribution": " So I don't have the {disfmarker} like the {disfmarker}"}, {"turn": 179, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 180, "name": "PhD", "id": "B", "contribution": " So {disfmarker}"}, {"turn": 181, "name": "Professor", "id": "C", "contribution": " OK , four point three , ten point seven ,"}, {"turn": 182, "name": "PhD", "id": "B", "contribution": " And forty forty ."}, {"turn": 183, "name": "Professor", "id": "C", "contribution": " and {disfmarker}"}, {"turn": 184, "name": "PhD", "id": "B", "contribution": " Forty percent is the high mismatch ."}, {"turn": 185, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 186, "name": "PhD", "id": "B", "contribution": " And that becomes like four point three {disfmarker}"}, {"turn": 187, "name": "Professor", "id": "C", "contribution": " Not changed ."}, {"turn": 188, "name": "PhD", "id": "B", "contribution": " Yeah , it 's like ten point one . Still the same . And the high mismatch is like eighteen point five ."}, {"turn": 189, "name": "Professor", "id": "C", "contribution": " Eighteen point five ."}, {"turn": 190, "name": "PhD", "id": "B", "contribution": " Five ."}, {"turn": 191, "name": "Professor", "id": "C", "contribution": " And what were you just describing ?"}, {"turn": 192, "name": "PhD", "id": "B", "contribution": " Oh , the one is {disfmarker} this one is just the baseline plus the , uh , Wiener filter plugged into it ."}, {"turn": 193, "name": "Professor", "id": "C", "contribution": " But where 's the , uh , on - line normalization and so on ?"}, {"turn": 194, "name": "PhD", "id": "B", "contribution": " Oh , OK . So {disfmarker} Sorry . So , with the {disfmarker} with the on - line normalization , the performance was , um , ten {disfmarker} OK , so it 's like four point three . Uh , and again , that 's the ba the ten point , uh , four and twenty point one . That was with on - line normalization and LDA . So the h well matched has like literally not changed by adding on - line or LDA on it . But the {disfmarker} I mean , even the medium mismatch is pretty much the same . And the high mismatch was improved by twenty percent absolute ."}, {"turn": 195, "name": "Professor", "id": "C", "contribution": " OK , and what kind of number {disfmarker} an and what are we talking about here ?"}, {"turn": 196, "name": "PhD", "id": "B", "contribution": " It 's the It - it 's Italian ."}, {"turn": 197, "name": "Professor", "id": "C", "contribution": " Is this TI - digits"}, {"turn": 198, "name": "PhD", "id": "B", "contribution": " I 'm talking about Italian ,"}, {"turn": 199, "name": "Professor", "id": "C", "contribution": " or {disfmarker} Italian ?"}, {"turn": 200, "name": "PhD", "id": "B", "contribution": " yeah ."}, {"turn": 201, "name": "Professor", "id": "C", "contribution": " And what did {disfmarker} So , what was the , um , uh , corresponding number , say , for , um , uh , the Alcatel system for instance ?"}, {"turn": 202, "name": "PhD", "id": "B", "contribution": " Mmm ."}, {"turn": 203, "name": "Professor", "id": "C", "contribution": " Do you know ?"}, {"turn": 204, "name": "PhD", "id": "D", "contribution": " Yeah , so it looks to be , um {disfmarker}"}, {"turn": 205, "name": "PhD", "id": "B", "contribution": " You have it ?"}, {"turn": 206, "name": "PhD", "id": "D", "contribution": " Yep , it 's three point four , uh , eight point , uh , seven , and , uh , thirteen point seven ."}, {"turn": 207, "name": "PhD", "id": "B", "contribution": " Yep ."}, {"turn": 208, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 209, "name": "PhD", "id": "B", "contribution": " So {disfmarker} Thanks ."}, {"turn": 210, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 211, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 212, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 213, "name": "PhD", "id": "B", "contribution": " So , uh , this is the single stage Wiener filter , with {disfmarker} The noise estimation was based on first ten frames ."}, {"turn": 214, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 215, "name": "PhD", "id": "B", "contribution": " Actually I started with {disfmarker} using the VAD to estimate the noise and then I found that it works {disfmarker} it doesn't work for Finnish and Spanish because the VAD endpoints are not good to estimate the noise because it cuts into the speech sometimes , so I end up overestimating the noise and getting a worse result . So it works only for Italian by u for {disfmarker} using a VAD to estimate noise ."}, {"turn": 216, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 217, "name": "PhD", "id": "B", "contribution": " It works for Italian because the VAD was trained on Italian ."}, {"turn": 218, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 219, "name": "PhD", "id": "B", "contribution": " So , uh {disfmarker} so this was , uh {disfmarker} And so this was giving {disfmarker} um , this {disfmarker} this was like not improving a lot on this baseline of not having the Wiener filter on it . And , so , uh , I ran this stuff with one more stage of Wiener filtering on it but the second time , what I did was I {disfmarker} estimated the new Wiener filter based on the cleaned up speech , and did , uh , smoothing in the frequency to {disfmarker} to reduce the variance {disfmarker}"}, {"turn": 220, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 221, "name": "PhD", "id": "B", "contribution": " I mean , I have {disfmarker} I 've {disfmarker} I 've observed there are , like , a lot of bumps in the frequency when I do this Wiener filtering which is more like a musical noise or something . And so by adding another stage of Wiener filtering , the results on the SpeechDat - Car was like , um {disfmarker} So , I still don't have the word error rate . I 'm sorry about it . But the overall improvement was like fifty - six point four six . This was again using ten frames of noise estimate and two stage of Wiener filtering . And the rest is like the LDA plu and the on - line normalization all remaining the same . Uh , so this was , like , compared to , uh , uh {disfmarker} Fifty - seven is what you got by using the French Telecom system , right ?"}, {"turn": 222, "name": "PhD", "id": "D", "contribution": " No , I don't think so ."}, {"turn": 223, "name": "PhD", "id": "B", "contribution": " Y i"}, {"turn": 224, "name": "PhD", "id": "D", "contribution": " Is it on Italian ?"}, {"turn": 225, "name": "PhD", "id": "B", "contribution": " No , this is over the whole SpeechDat - Car . So {disfmarker}"}, {"turn": 226, "name": "PhD", "id": "D", "contribution": " Oh , yeah , fifty - seven {disfmarker}"}, {"turn": 227, "name": "PhD", "id": "B", "contribution": " point {disfmarker}"}, {"turn": 228, "name": "PhD", "id": "D", "contribution": " Right ."}, {"turn": 229, "name": "PhD", "id": "B", "contribution": " Yeah , so the new {disfmarker} the new Wiener filtering schema is like {disfmarker} some fifty - six point four six which is like one percent still less than what you got using the French Telecom system ."}, {"turn": 230, "name": "PhD", "id": "D", "contribution": " Uh - huh . Mm - hmm ."}, {"turn": 231, "name": "Professor", "id": "C", "contribution": " But it 's a pretty similar number in any event ."}, {"turn": 232, "name": "PhD", "id": "B", "contribution": " It 's very similar ."}, {"turn": 233, "name": "Professor", "id": "C", "contribution": " Yeah . But again , you 're {disfmarker} you 're more or less doing what they were doing , right ?"}, {"turn": 234, "name": "PhD", "id": "B", "contribution": " It 's {disfmarker} it 's different in a sense like I 'm actually cleaning up the cleaned up spectrum which they 're not doing . They 're d what they 're doing is , they have two stage {disfmarker} stages of estimating the Wiener filter , but {disfmarker} the final filter , what they do is they {disfmarker} they take it to their time domain by doing an inverse Fourier transform ."}, {"turn": 235, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 236, "name": "PhD", "id": "B", "contribution": " And they filter the original signal using that fil filter ,"}, {"turn": 237, "name": "Professor", "id": "C", "contribution": " Uh - huh ."}, {"turn": 238, "name": "PhD", "id": "B", "contribution": " which is like final filter is acting on the input noisy speech rather than on the cleaned up . So this is more like I 'm doing Wiener filter twice , but the only thing is that the second time I 'm actually smoothing the filter and then cleaning up the cleaned up spectrum first level . And so that {disfmarker} that 's {disfmarker} that 's what the difference is ."}, {"turn": 239, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 240, "name": "PhD", "id": "B", "contribution": " And actually I tried it on s the original clean {disfmarker} I mean , the original spectrum where , like , I {disfmarker} the second time I estimate the filter but actually clean up the noisy speech rather the c s first {disfmarker} output of the first stage and that doesn't {disfmarker} seems to be a {disfmarker} giving , I mean , that much improvement . I {disfmarker} I didn didn't run it for the whole case . And {disfmarker} and what I t what I tried was , by using the same thing but {disfmarker} Uh , so we actually found that the VAD is very , like , crucial . I mean , just by changing the VAD itself gives you the {disfmarker} a lot of improvement"}, {"turn": 241, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 242, "name": "PhD", "id": "B", "contribution": " by instead of using the current VAD , if you just take up the VAD output from the channel zero , {comment} when {disfmarker} instead of using channel zero and channel one , because that was the p that was the reason why I was not getting a lot of improvement for estimating {comment} the noise . So I just used the channel zero VAD to estimate the noise so that it gives me some reliable mar markers for this noise estimation ."}, {"turn": 243, "name": "Professor", "id": "C", "contribution": " What 's a channel zero VAD ?"}, {"turn": 244, "name": "PhD", "id": "B", "contribution": " Um ,"}, {"turn": 245, "name": "Professor", "id": "C", "contribution": " I 'm {disfmarker} I 'm confused about that ."}, {"turn": 246, "name": "PhD", "id": "B", "contribution": " so , it 's like {disfmarker}"}, {"turn": 247, "name": "PhD", "id": "D", "contribution": " So it 's the close - talking microphone ."}, {"turn": 248, "name": "PhD", "id": "B", "contribution": " Yeah , the close - talking without {disfmarker}"}, {"turn": 249, "name": "Professor", "id": "C", "contribution": " Oh , oh , oh , oh ."}, {"turn": 250, "name": "PhD", "id": "B", "contribution": " So because the channel zero and channel one are like the same speech , but only w I mean , the same endpoints ."}, {"turn": 251, "name": "Professor", "id": "C", "contribution": ""}, {"turn": 252, "name": "PhD", "id": "B", "contribution": " But the only thing is that the speech is very noisy for channel one , so you can actually use the output of the channel zero for channel one for the VAD . I mean , that 's like a cheating method ."}, {"turn": 253, "name": "Professor", "id": "C", "contribution": " Right . I mean , so a are they going to pro What are they doing to do , do we know yet ? about {disfmarker} as far as what they 're {disfmarker} what the rules are going to be and what we can use ?"}, {"turn": 254, "name": "PhD", "id": "D", "contribution": " Yeah , so actually I received a {disfmarker} a new document , describing this ."}, {"turn": 255, "name": "PhD", "id": "B", "contribution": " Yeah , that 's {disfmarker}"}, {"turn": 256, "name": "PhD", "id": "D", "contribution": " And what they did finally is to , mmm , uh , not to align the utterances but to perform recognition , um , only on the close - talking microphone ,"}, {"turn": 257, "name": "PhD", "id": "B", "contribution": " Which is the channel zero ."}, {"turn": 258, "name": "PhD", "id": "D", "contribution": " and to take the result of the recognition to get the boundaries uh , of speech ."}, {"turn": 259, "name": "Professor", "id": "C", "contribution": " So it 's not like that 's being done in one place or one time ."}, {"turn": 260, "name": "PhD", "id": "D", "contribution": " And {disfmarker}"}, {"turn": 261, "name": "Professor", "id": "C", "contribution": " That 's {disfmarker} that 's just a rule and we 'd {disfmarker} you {disfmarker} you were permitted to do that . Is {disfmarker} is that it ?"}, {"turn": 262, "name": "PhD", "id": "D", "contribution": " Uh , I think they will send , um , files but we {disfmarker} we don't {disfmarker} Well , apparently {disfmarker}"}, {"turn": 263, "name": "Professor", "id": "C", "contribution": " Oh , so they will send files so everybody will have the same boundaries to work with ?"}, {"turn": 264, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah ."}, {"turn": 265, "name": "PhD", "id": "B", "contribution": " But actually their alignment actually is not seems to be improving in like on all cases ."}, {"turn": 266, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 267, "name": "PhD", "id": "D", "contribution": " Oh , i Yeah , so what happened here is that , um , the overall improvement that they have with this method {disfmarker} So {disfmarker} Well , to be more precise , what they have is , they have these alignments and then they drop the beginning silence and {disfmarker} and the end silence but they keep , uh , two hundred milliseconds before speech and two hundred after speech . And they keep the speech pauses also . Um , and the overall improvement over the MFCC baseline So , when they just , uh , add this frame dropping in addition it 's r uh , forty percent , right ?"}, {"turn": 268, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 269, "name": "PhD", "id": "D", "contribution": " Fourteen percent , I mean ."}, {"turn": 270, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 271, "name": "PhD", "id": "B", "contribution": " Yeah , which is {disfmarker}"}, {"turn": 272, "name": "PhD", "id": "D", "contribution": " Um , which is , um , t which is the overall improvement . But in some cases it doesn't improve at all . Like , uh , y do you remember which case ?"}, {"turn": 273, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 274, "name": "PhD", "id": "B", "contribution": " It gives like negative {disfmarker} Well , in {disfmarker} in like some Italian and TI - digits ,"}, {"turn": 275, "name": "PhD", "id": "D", "contribution": " Yeah , some @ @ ."}, {"turn": 276, "name": "PhD", "id": "B", "contribution": " right ?"}, {"turn": 277, "name": "PhD", "id": "D", "contribution": " Right ."}, {"turn": 278, "name": "PhD", "id": "B", "contribution": " Yeah . So by using the endpointed speech , actually it 's worse than the baseline in some instances , which could be due to the word pattern ."}, {"turn": 279, "name": "PhD", "id": "D", "contribution": " Mmm . Yeah ."}, {"turn": 280, "name": "Professor", "id": "C", "contribution": " Yeah ,"}, {"turn": 281, "name": "PhD", "id": "D", "contribution": " And {disfmarker} Yeah , the other thing also is that fourteen percent is less than what you obtain using a real VAD ."}, {"turn": 282, "name": "PhD", "id": "B", "contribution": " Yeah , our neural net {disfmarker}"}, {"turn": 283, "name": "PhD", "id": "D", "contribution": " So with without cheating like this ."}, {"turn": 284, "name": "PhD", "id": "B", "contribution": " Yeah , yeah ."}, {"turn": 285, "name": "PhD", "id": "D", "contribution": " So {disfmarker} Uh {disfmarker} So I think this shows that there is still work {disfmarker} Uh , well , working on the VAD is still {disfmarker} still important I think ."}, {"turn": 286, "name": "Professor", "id": "C", "contribution": " Yeah , c"}, {"turn": 287, "name": "PhD", "id": "D", "contribution": " Uh {disfmarker}"}, {"turn": 288, "name": "PhD", "id": "A", "contribution": " Can I ask just a {disfmarker} a high level question ? Can you just say like one or two sentences about Wiener filtering and why {disfmarker} why are people doing that ?"}, {"turn": 289, "name": "PhD", "id": "B", "contribution": " Hmm ."}, {"turn": 290, "name": "PhD", "id": "A", "contribution": " What 's {disfmarker} what 's the deal with that ?"}, {"turn": 291, "name": "PhD", "id": "B", "contribution": " OK , so the Wiener filter , it 's {disfmarker} it 's like {disfmarker} it 's like you try to minimize {disfmarker} I mean , so the basic principle of Wiener filter is like you try to minimize the , uh , d uh , difference between the noisy signal and the clean signal if you have two channels . Like let 's say you have a clean t signal and you have an additional channel where you know what is the noisy signal ."}, {"turn": 292, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 293, "name": "PhD", "id": "B", "contribution": " And then you try to minimize the error between these two ."}, {"turn": 294, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 295, "name": "PhD", "id": "B", "contribution": " So that 's the basic principle . And you get {disfmarker} you can do that {disfmarker} I mean , if {disfmarker} if you have only a c noisy signal , at a level which you , you w try to estimate the noise from the w assuming that the first few frames are noise or if you have a w voice activity detector , uh , you estimate the noise spectrum ."}, {"turn": 296, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 297, "name": "PhD", "id": "B", "contribution": " And then you {disfmarker}"}, {"turn": 298, "name": "PhD", "id": "A", "contribution": " Do you assume the noise is the same ?"}, {"turn": 299, "name": "PhD", "id": "B", "contribution": " Yeah . in {disfmarker} yeah , after the speech starts ."}, {"turn": 300, "name": "PhD", "id": "A", "contribution": " Uh - huh ."}, {"turn": 301, "name": "PhD", "id": "B", "contribution": " So {disfmarker} but that 's not the case in , uh , many {disfmarker} many of our cases but it works reasonably well ."}, {"turn": 302, "name": "PhD", "id": "A", "contribution": " I see ."}, {"turn": 303, "name": "PhD", "id": "B", "contribution": " And {disfmarker} and then you What you do is you , uh b fff . So again , I can write down some of these eq Oh , OK . Yeah . And then you do this {disfmarker} uh , this is the transfer function of the Wiener filter , so \" SF \" is a clean speech spectrum , power spectrum"}, {"turn": 304, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 305, "name": "PhD", "id": "B", "contribution": " And \" N \" is the noisy power spectrum . And so this is the transfer function ."}, {"turn": 306, "name": "Professor", "id": "C", "contribution": " Right"}, {"turn": 307, "name": "PhD", "id": "B", "contribution": " And ,"}, {"turn": 308, "name": "Professor", "id": "C", "contribution": " actually , I guess {disfmarker}"}, {"turn": 309, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 310, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 311, "name": "PhD", "id": "B", "contribution": " And then you multiply your noisy power spectrum with this . You get an estimate of the clean power spectrum ."}, {"turn": 312, "name": "PhD", "id": "A", "contribution": " I see . OK ."}, {"turn": 313, "name": "PhD", "id": "B", "contribution": " So {disfmarker} but the thing is that you have to estimate the SF from the noisy spectrum , what you have . So you estimate the NF from the initial noise portions and then you subtract that from the current noisy spectrum to get an estimate of the SF . So sometimes that becomes zero because you do you don't have a true estimate of the noise . So the f filter will have like sometimes zeros in it"}, {"turn": 314, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 315, "name": "PhD", "id": "B", "contribution": " because some frequency values will be zeroed out because of that . And that creates a lot of discontinuities across the spectrum because @ @ the filter . So , uh , so {disfmarker} that 's what {disfmarker} that was just the first stage of Wiener filtering that I tried ."}, {"turn": 316, "name": "PhD", "id": "A", "contribution": " So is this , um , basically s uh , similar to just regular spectral subtraction ?"}, {"turn": 317, "name": "PhD", "id": "B", "contribution": " It {disfmarker}"}, {"turn": 318, "name": "Professor", "id": "C", "contribution": " It 's all pretty related ,"}, {"turn": 319, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 320, "name": "Professor", "id": "C", "contribution": " yeah . It 's {disfmarker} it 's {disfmarker} there 's a di there 's a whole class of techniques where you try in some sense to minimize the noise ."}, {"turn": 321, "name": "PhD", "id": "A", "contribution": " Uh - huh ."}, {"turn": 322, "name": "Professor", "id": "C", "contribution": " And it 's typically a mean square sense , uh {disfmarker} uh {disfmarker} uh , i in {disfmarker} in {disfmarker} in some way . And , uh {disfmarker} uh , spectral subtraction is {disfmarker} is , uh {disfmarker} uh , one approach to it ."}, {"turn": 323, "name": "PhD", "id": "A", "contribution": " Do people use the Wiener filtering in combination with the spectral subtraction typically , or is i are they sort of competing techniques ?"}, {"turn": 324, "name": "PhD", "id": "B", "contribution": " Not seen . They are very s similar techniques ."}, {"turn": 325, "name": "PhD", "id": "A", "contribution": " Yeah . O oh , OK ."}, {"turn": 326, "name": "PhD", "id": "B", "contribution": " So it 's like I haven't seen anybody using s Wiener filter with spectral subtraction ."}, {"turn": 327, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 328, "name": "PhD", "id": "A", "contribution": " I see , I see ."}, {"turn": 329, "name": "Professor", "id": "C", "contribution": " I mean , in the long run you 're doing the same thing"}, {"turn": 330, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 331, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 332, "name": "Professor", "id": "C", "contribution": " but y but there you make different approximations , and {disfmarker} in spectral subtraction , for instance , there 's a {disfmarker} a {disfmarker} an estimation factor ."}, {"turn": 333, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 334, "name": "Professor", "id": "C", "contribution": " You sometimes will figure out what the noise is and you 'll multiply that noise spectrum times some constant and subtract that rather than {disfmarker} and sometimes people {disfmarker} even though this really should be in the power domain , sometimes people s work in the magnitude domain because it {disfmarker} it {disfmarker} it works better ."}, {"turn": 335, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 336, "name": "Professor", "id": "C", "contribution": " And , uh , uh , you know ."}, {"turn": 337, "name": "PhD", "id": "A", "contribution": " So why did you choose , uh , Wiener filtering over some other {disfmarker} one of these other techniques ?"}, {"turn": 338, "name": "PhD", "id": "B", "contribution": " Uh , the reason was , like , we had this choice of using spectral subtraction , Wiener filtering , and there was one more thing which I which I 'm trying , is this sub space approach . So , Stephane is working on spectral subtraction ."}, {"turn": 339, "name": "PhD", "id": "A", "contribution": " Oh , OK ."}, {"turn": 340, "name": "PhD", "id": "B", "contribution": " So I picked up {disfmarker}"}, {"turn": 341, "name": "PhD", "id": "A", "contribution": " So you 're sort of trying @ @ them all ."}, {"turn": 342, "name": "PhD", "id": "B", "contribution": " Y Yeah ,"}, {"turn": 343, "name": "PhD", "id": "A", "contribution": " Ah ,"}, {"turn": 344, "name": "PhD", "id": "B", "contribution": " we just wanted to have a few noise production {disfmarker} compensation techniques"}, {"turn": 345, "name": "PhD", "id": "A", "contribution": " I see . Oh , OK ."}, {"turn": 346, "name": "PhD", "id": "B", "contribution": " and then pick some from that {disfmarker}"}, {"turn": 347, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 348, "name": "PhD", "id": "B", "contribution": " pick one ."}, {"turn": 349, "name": "Professor", "id": "C", "contribution": " I m I mean {disfmarker} yeah , I mean , there 's Car - Carmen 's working on another , on the vector Taylor series ."}, {"turn": 350, "name": "PhD", "id": "B", "contribution": " VA Yeah , VAD . w Yeah ."}, {"turn": 351, "name": "Professor", "id": "C", "contribution": " So they were just kind of trying to cover a bunch of different things with this task and see , you know , what are {disfmarker} what are the issues for each of them ."}, {"turn": 352, "name": "PhD", "id": "A", "contribution": " Ah , OK . That makes sense ."}, {"turn": 353, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 354, "name": "PhD", "id": "A", "contribution": " Yeah . Mm - hmm . Mm - hmm ."}, {"turn": 355, "name": "Professor", "id": "C", "contribution": " Um ."}, {"turn": 356, "name": "PhD", "id": "A", "contribution": " Cool , thanks ."}, {"turn": 357, "name": "PhD", "id": "B", "contribution": " So {disfmarker} so one of {disfmarker} one of the things that I tried , like I said , was to remove those zeros in the fri filter by doing some smoothing of the filter ."}, {"turn": 358, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 359, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 360, "name": "PhD", "id": "B", "contribution": " Like , you estimate the edge of square and then you do a f smoothing across the frequency so that those zeros get , like , flattened out ."}, {"turn": 361, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 362, "name": "PhD", "id": "B", "contribution": " And that doesn't seems to be improving by trying it on the first time . So what I did was like I p did this and then you {disfmarker} I plugged in the {disfmarker} one more {disfmarker} the same thing but with the smoothed filter the second time ."}, {"turn": 363, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 364, "name": "PhD", "id": "B", "contribution": " And that seems to be working ."}, {"turn": 365, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 366, "name": "PhD", "id": "B", "contribution": " So that 's where I got like fifty - six point five percent improvement on SpeechDat - Car with that . And {disfmarker} So the other thing what I tried was I used still the ten frames of noise estimate but I used this channel zero VAD to drop the frames . So I 'm not {disfmarker} still not estimating . And that has taken the performance to like sixty - seven percent in SpeechDat - Car , which is {disfmarker} which {disfmarker} which like sort of shows that by using a proper VAD you can just take it to further , better levels . And {disfmarker} So ."}, {"turn": 367, "name": "PhD", "id": "A", "contribution": " So that 's sort of like , you know , best - case performance ?"}, {"turn": 368, "name": "PhD", "id": "B", "contribution": " Yeah , so far I 've seen sixty - seven {disfmarker} I mean , no , I haven't seen s like sixty - seven percent . And , uh , using the channel zero VAD to estimate the noise also seems to be improving but I don't have the results for all the cases with that . So I used channel zero VAD to estimate noise as a lesser 2 x frame , which is like , {vocalsound} everywhere I use the channel zero VAD . And that seems to be the best combination , uh , rather than using a few frames to estimate and then drop a channel ."}, {"turn": 369, "name": "Professor", "id": "C", "contribution": " So I 'm {disfmarker} I 'm still a little confused . Is that channel zero information going to be accessible during this test ."}, {"turn": 370, "name": "PhD", "id": "B", "contribution": " Nnn , no . This is just to test whether we can really improve by using a better VAD ."}, {"turn": 371, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 372, "name": "PhD", "id": "B", "contribution": " So ,"}, {"turn": 373, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 374, "name": "PhD", "id": "B", "contribution": " I mean {disfmarker} So this is like the noise compensation f is fixed"}, {"turn": 375, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 376, "name": "PhD", "id": "B", "contribution": " but you make a better decision on the endpoints . That 's , like {disfmarker} seems to be {disfmarker}"}, {"turn": 377, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 378, "name": "PhD", "id": "B", "contribution": " so we c so I mean , which {disfmarker} which means , like , by using this technique what we improve just the VAD"}, {"turn": 379, "name": "Professor", "id": "C", "contribution": " Yes ."}, {"turn": 380, "name": "PhD", "id": "B", "contribution": " we can just take the performance by another ten percent or better ."}, {"turn": 381, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 382, "name": "PhD", "id": "B", "contribution": " So , that {disfmarker} that was just the , uh , reason for doing that experiment . And , w um {disfmarker} Yeah , but this {disfmarker} all these things , I have to still try it on the TI - digits , which is like I 'm just running . And there seems to be not improving a {disfmarker} a lot on the TI - digits , so I 'm like investigating that , why it 's not . And , um , um {disfmarker} Well after that . So , uh {disfmarker} so the other {disfmarker} the other thing is {disfmarker} like I 've been {disfmarker} I 'm doing all this stuff on the power spectrum . So {disfmarker} Tried this stuff on the mel as well {disfmarker} mel and the magnitude , and mel magnitude , and all those things . But it seems to be the power spectrum seems to be getting the best result . So , one of {disfmarker} one of reasons I thought like doing the averaging , after the filtering using the mel filter bank , that seems to be maybe helping rather than trying it on the mel filter ba filtered outputs ."}, {"turn": 383, "name": "Professor", "id": "C", "contribution": " Mm - hmm . Mm - hmm ."}, {"turn": 384, "name": "PhD", "id": "B", "contribution": " So just th"}, {"turn": 385, "name": "Professor", "id": "C", "contribution": " Ma Makes sense ."}, {"turn": 386, "name": "PhD", "id": "B", "contribution": " Yeah , th that 's {disfmarker} that 's the only thing that I could think of why {disfmarker} why it 's giving improvement on the mel . And , yep . So that 's it ."}, {"turn": 387, "name": "Professor", "id": "C", "contribution": " Uh , how about the subspace stuff ?"}, {"turn": 388, "name": "PhD", "id": "B", "contribution": " Subspace , {comment} I 'm {disfmarker} I 'm like {disfmarker} that 's still in {disfmarker} a little bit in the back burner because I 've been p putting a lot effort on this to make it work , on tuning things and other stuff ."}, {"turn": 389, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 390, "name": "PhD", "id": "B", "contribution": " So I was like going parallely but not much of improvement . I 'm just {disfmarker} have some skeletons ready , need some more time for it ."}, {"turn": 391, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 392, "name": "PhD", "id": "B", "contribution": " Mmm ."}, {"turn": 393, "name": "PhD", "id": "A", "contribution": " Tha - that it ?"}, {"turn": 394, "name": "PhD", "id": "B", "contribution": " Yep . Yep ."}, {"turn": 395, "name": "PhD", "id": "A", "contribution": " Cool . Do you wanna go , Stephane ?"}, {"turn": 396, "name": "PhD", "id": "D", "contribution": " Uh , yeah . So , {vocalsound} I 've been , uh , working still on the spectral subtraction . Um , So to r to remind you {vocalsound} {vocalsound} a little bit of {disfmarker} of what I did before , is just {vocalsound} to apply some spectral subtraction with an overestimation factor also to get , um , an estimate of the noise , uh , spectrum , and subtract this estimation of the noise spectrum from the , uh , signal spectrum , {comment} but subtracting more when the SNR is {disfmarker} is , uh , low , which is a technique that it 's often used ."}, {"turn": 397, "name": "PhD", "id": "A", "contribution": " \" Subtracting more \" , meaning {disfmarker} ?"}, {"turn": 398, "name": "PhD", "id": "D", "contribution": " So you overestimate the noise spectrum . You multiply the noise spectrum by a factor , uh , which depends on the SNR ."}, {"turn": 399, "name": "PhD", "id": "A", "contribution": " Oh , OK . I see ."}, {"turn": 400, "name": "PhD", "id": "D", "contribution": " So , above twenty DB , it 's one , so you just subtract the noise ."}, {"turn": 401, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 402, "name": "PhD", "id": "D", "contribution": " And then it 's b Generally {disfmarker} Well , I use , actually , a linear , uh , function of the SNR ,"}, {"turn": 403, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 404, "name": "PhD", "id": "D", "contribution": " which is bounded to , like , two or three , {comment} when the SNR is below zero DB ."}, {"turn": 405, "name": "PhD", "id": "A", "contribution": " Mm - hmm . Mm - hmm ."}, {"turn": 406, "name": "PhD", "id": "D", "contribution": " Um , doing just this , uh , either on the FFT bins or on the mel bands , um , t doesn't yield any improvement"}, {"turn": 407, "name": "Professor", "id": "C", "contribution": " Oh ! Um , uh , what are you doing with negative , uh , powers ?"}, {"turn": 408, "name": "PhD", "id": "D", "contribution": " o Yeah . So there is also a threshold , of course , because after subtraction you can have negative energies ,"}, {"turn": 409, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 410, "name": "PhD", "id": "D", "contribution": " and {disfmarker} So what I {disfmarker} I just do is to put , uh {disfmarker} to {disfmarker} to add {disfmarker} to put the threshold first and then to add a small amount of noise , which right now is speech - shaped . Um {disfmarker}"}, {"turn": 411, "name": "PhD", "id": "A", "contribution": " Speech - shaped ?"}, {"turn": 412, "name": "PhD", "id": "D", "contribution": " Yeah , so it 's {disfmarker} a it has the overall {disfmarker} overall energy , uh {disfmarker} pow it has the overall power spectrum of speech . So with a bump around one kilohertz ."}, {"turn": 413, "name": "PhD", "id": "A", "contribution": " So when y when you talk about there being something less than zero after subtracting the noise , is that at a particular frequency bin ?"}, {"turn": 414, "name": "PhD", "id": "D", "contribution": " i Uh - huh . Yeah ."}, {"turn": 415, "name": "PhD", "id": "A", "contribution": " OK ."}, {"turn": 416, "name": "PhD", "id": "D", "contribution": " There can be frequency bins with negative values ."}, {"turn": 417, "name": "PhD", "id": "A", "contribution": " And so when you say you 're adding something that has the overall shape of speech , is that in a {disfmarker} in a particular frequency bin ? Or you 're adding something across all the frequencies when you get these negatives ?"}, {"turn": 418, "name": "PhD", "id": "D", "contribution": " For each frequencies I a I 'm adding some , uh , noise , but the a the amount of {disfmarker} the amount of noise I add is not the same for all the frequency bins ."}, {"turn": 419, "name": "PhD", "id": "A", "contribution": " Ah ! OK . I gotcha . Right ."}, {"turn": 420, "name": "PhD", "id": "D", "contribution": " Uh . Right now I don't think if it makes sense to add something that 's speech - shaped , because then you have silence portion that have some spectra similar to the sp the overall speech spectra ."}, {"turn": 421, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 422, "name": "PhD", "id": "D", "contribution": " But {disfmarker} Yeah . So this is something I can still work on ,"}, {"turn": 423, "name": "PhD", "id": "A", "contribution": " So what does that mean ?"}, {"turn": 424, "name": "PhD", "id": "D", "contribution": " but {disfmarker} Hmm ."}, {"turn": 425, "name": "PhD", "id": "A", "contribution": " I 'm trying to understand what it means when you do the spectral subtraction and you get a negative . It means that at that particular frequency range you subtracted more energy than there was actually {disfmarker}"}, {"turn": 426, "name": "PhD", "id": "D", "contribution": " That means that {disfmarker} Mm - hmm . Yeah . So {disfmarker} so yeah , you have an {disfmarker} an estimation of the noise spectrum , but sometimes , of course , it 's {disfmarker} as the noise is not perfectly stationary , sometimes this estimation can be , uh , too small , so you don't subtract enough . But sometimes it can be too large also . If {disfmarker} if the noise , uh , energy in this particular frequency band drops for some reason ."}, {"turn": 427, "name": "PhD", "id": "A", "contribution": " Mm - hmm . Mm - hmm ."}, {"turn": 428, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 429, "name": "PhD", "id": "A", "contribution": " So in {disfmarker} in an ideal word i world {comment} if the noise were always the same , then , when you subtracted it the worst that i you would get would be a zero . I mean , the lowest you would get would be a zero , cuz i if there was no other energy there you 're just subtracting exactly the noise ."}, {"turn": 430, "name": "Professor", "id": "C", "contribution": " Right ."}, {"turn": 431, "name": "PhD", "id": "D", "contribution": " Mm - hmm ,"}, {"turn": 432, "name": "Professor", "id": "C", "contribution": " Yep , there 's all {disfmarker} there 's all sorts of , uh , deviations from the ideal here ."}, {"turn": 433, "name": "PhD", "id": "D", "contribution": " yeah ."}, {"turn": 434, "name": "Professor", "id": "C", "contribution": " I mean , for instance , you 're {disfmarker} you 're talking about the signal and noise , um , at a particular point . And even if something is sort of stationary in ster terms of statistics , there 's no guarantee that any particular instantiation or piece of it is exactly a particular number or bounded by a particular range ."}, {"turn": 435, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 436, "name": "Professor", "id": "C", "contribution": " So , you 're figuring out from some chunk of {disfmarker} of {disfmarker} of the signal what you think the noise is . Then you 're subtracting that from another chunk ,"}, {"turn": 437, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 438, "name": "Professor", "id": "C", "contribution": " and there 's absolutely no reason to think that you 'd know that it wouldn't , uh , be negative in some places ."}, {"turn": 439, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Hmm ."}, {"turn": 440, "name": "Professor", "id": "C", "contribution": " Uh , on the other hand that just means that in some sense you 've made a mistake because you certainly have stra subtracted a bigger number than is due to the noise ."}, {"turn": 441, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 442, "name": "Professor", "id": "C", "contribution": " Um {disfmarker} Also , we speak {disfmarker} the whole {disfmarker} where all this stuff comes from is from an assumption that signal and noise are uncorrelated . And that certainly makes sense in s in {disfmarker} in a statistical interpretation , that , you know , over , um , all possible realizations that they 're uncorrelated"}, {"turn": 443, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 444, "name": "Professor", "id": "C", "contribution": " or assuming , uh , ergodicity that i that i um , across time , uh , it 's uncorrelated . But if you just look at {disfmarker} a quarter second , uh , and you cross - multiply the two things , uh , you could very well , uh , end up with something that sums to something that 's not zero . So in fact , the two signals could have some relation to one another . And so there 's all sorts of deviations from ideal in this . And {disfmarker} and given all that , you could definitely end up with something that 's negative . But if down the road you 're making use of something as if it is a power spectrum , um , then it can be bad to have something negative . Now , the other thing I wonder about actually is , what if you left it negative ? What happens ?"}, {"turn": 445, "name": "PhD", "id": "B", "contribution": " Is that the log ?"}, {"turn": 446, "name": "Professor", "id": "C", "contribution": " I mean , because {disfmarker} Um , are you taking the log before you add them up to the mel ?"}, {"turn": 447, "name": "PhD", "id": "B", "contribution": " After that . No , after ."}, {"turn": 448, "name": "Professor", "id": "C", "contribution": " Right . So the thing is , I wonder how {disfmarker} if you put your thresholds after that , I wonder how often you would end up with , uh {disfmarker} with negative values ."}, {"turn": 449, "name": "PhD", "id": "B", "contribution": " But you will {disfmarker} But you end up reducing some neighboring frequency bins {disfmarker} @ @ in the average , right ? When you add the negative to the positive value which is the true estimate ."}, {"turn": 450, "name": "Professor", "id": "C", "contribution": " Yeah . But nonetheless , uh , you know , these are {disfmarker} it 's another f kind of smoothing , right ? that you 're doing ."}, {"turn": 451, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 452, "name": "Professor", "id": "C", "contribution": " Right . So , you 've done your best shot at figuring out what the noise should be , and now i then you 've subtracted it off . And then after that , instead of {disfmarker} instead of , uh , uh , leaving it as is and adding things {disfmarker} adding up some neighbors , you artificially push it up ."}, {"turn": 453, "name": "PhD", "id": "B", "contribution": " Hmm ."}, {"turn": 454, "name": "Professor", "id": "C", "contribution": " Which is , you know , it 's {disfmarker} there 's no particular reason that that 's the right thing to do either , right ?"}, {"turn": 455, "name": "PhD", "id": "B", "contribution": " Yeah , yeah ."}, {"turn": 456, "name": "Professor", "id": "C", "contribution": " So , um , uh , i in fact , what you 'd be doing is saying , \" well , we 're d we 're {disfmarker} we 're going to definitely diminish the effect of this frequency in this little frequency bin in the {disfmarker} in the overall mel summation \" . It 's just a thought . I d I don't know if it would be {disfmarker}"}, {"turn": 457, "name": "PhD", "id": "A", "contribution": " Sort of the opposite of that would be if {disfmarker} if you find out you 're going to get a negative number , you don't do the subtraction for that bin ."}, {"turn": 458, "name": "PhD", "id": "B", "contribution": " Yeah . Uh - huh . That is true ."}, {"turn": 459, "name": "Professor", "id": "C", "contribution": " Nnn , yeah ,"}, {"turn": 460, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 461, "name": "Professor", "id": "C", "contribution": " although {disfmarker}"}, {"turn": 462, "name": "PhD", "id": "A", "contribution": " That would be almost the opposite , right ? Instead of leaving it negative , you don't do it . If your {disfmarker} if your subtraction 's going to result in a negative number , you {disfmarker} you don't do subtraction in that ."}, {"turn": 463, "name": "Professor", "id": "C", "contribution": " Yeah , but that means that in a situation where you thought that {disfmarker} that the bin was almost entirely noise , you left it ."}, {"turn": 464, "name": "PhD", "id": "A", "contribution": " Yeah . Yeah , I 'm just saying that 's like the opposite ."}, {"turn": 465, "name": "PhD", "id": "B", "contribution": " We just {disfmarker}"}, {"turn": 466, "name": "Professor", "id": "C", "contribution": " Uh ."}, {"turn": 467, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 468, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 469, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 470, "name": "Professor", "id": "C", "contribution": " Well , yeah that 's {disfmarker} that 's the opposite ,"}, {"turn": 471, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 472, "name": "Professor", "id": "C", "contribution": " yeah ."}, {"turn": 473, "name": "PhD", "id": "D", "contribution": " And , yeah , some people also {disfmarker} if it 's a negative value they , uh , re - compute it using inter interpolation from the edges and bins ."}, {"turn": 474, "name": "PhD", "id": "B", "contribution": " For frames , frequency bins ."}, {"turn": 475, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 476, "name": "PhD", "id": "D", "contribution": " Well , there are different things that you can do ."}, {"turn": 477, "name": "PhD", "id": "A", "contribution": " Oh ."}, {"turn": 478, "name": "Professor", "id": "C", "contribution": " People can also , uh , reflect it back up and essentially do a full wave rectification instead of a {disfmarker} instead of half wave ."}, {"turn": 479, "name": "PhD", "id": "A", "contribution": " Oh ."}, {"turn": 480, "name": "Professor", "id": "C", "contribution": " But it was just a thought that {disfmarker} that it might be something to try ."}, {"turn": 481, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Mm - hmm . Yep . Well , actually I tried , {vocalsound} something else based on this , um , is to {disfmarker} to put some smoothing , um , because it seems to {disfmarker} to help or it seems to help the Wiener filtering"}, {"turn": 482, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 483, "name": "PhD", "id": "D", "contribution": " and , mmm {disfmarker} So what I did is , uh , some kind of nonlinear smoothing . Actually I have a recursion that computes {disfmarker} Yeah , let me go back a little bit . Actually , when you do spectral subtraction you can , uh , find this {disfmarker} this equivalent in the s in the spectral domain . You can uh compute , y you can say that d your spectral subtraction is a filter , um , and the gain of this filter is the , um , {vocalsound} signal energy minus what you subtract , divided by the signal energy . And this is a gain that varies over time , and , you know , of course , uh , depending on the s on the noise spectrum and on the speech spectrum . And {disfmarker} what happen actually is that during low SNR values , the gain is close to zero but it varies a lot . Mmm , and this {disfmarker} this is the cause of musical noise and all these {disfmarker} the {disfmarker} {comment} the fact you {disfmarker} we go below zero one frame and then you can have an energy that 's above zero ."}, {"turn": 484, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 485, "name": "PhD", "id": "D", "contribution": " And {disfmarker} Mmm . So the smoothing is {disfmarker} I did a smoothing actually on this gain , uh , trajectory . But it 's {disfmarker} the smoothing is nonlinear in the sense that I tried to not smooth if the gain is high , because in this case we know that , uh , the estimate of the gain is correct because we {disfmarker} we are not close to {disfmarker} to {disfmarker} to zero , um , and to do more smoothing if the gain is low . Mmm . Um . Yeah . So , well , basically that 's this idea , and it seems to give pretty good results , uh , although I 've just {disfmarker} just tested on Italian and Finnish . And on Italian it seems {disfmarker} my result seems to be a little bit better than the Wiener filtering ,"}, {"turn": 486, "name": "PhD", "id": "B", "contribution": " Mm - hmm . Yeah , the one you showed yesterday ."}, {"turn": 487, "name": "PhD", "id": "D", "contribution": " right ?"}, {"turn": 488, "name": "PhD", "id": "B", "contribution": " Right ?"}, {"turn": 489, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 490, "name": "PhD", "id": "D", "contribution": " Uh , I don't know if you have these improvement the detailed improvements for Italian , Finnish , and Spanish there"}, {"turn": 491, "name": "PhD", "id": "B", "contribution": " Fff . No , I don't have , for each ,"}, {"turn": 492, "name": "PhD", "id": "D", "contribution": " or you have {disfmarker} just have your own ."}, {"turn": 493, "name": "PhD", "id": "B", "contribution": " I {disfmarker} I just {disfmarker} just have the final number here ."}, {"turn": 494, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 495, "name": "Professor", "id": "C", "contribution": " So these numbers he was giving before with the four point three , and the ten point one , and so forth , those were Italian , right ?"}, {"turn": 496, "name": "PhD", "id": "B", "contribution": " Yeah , yeah , yeah . So {disfmarker} so , no ,"}, {"turn": 497, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 498, "name": "PhD", "id": "D", "contribution": " Uh {disfmarker}"}, {"turn": 499, "name": "PhD", "id": "B", "contribution": " I actually didn't give you the number which is the final one ,"}, {"turn": 500, "name": "PhD", "id": "D", "contribution": " uh , no , we 've {disfmarker}"}, {"turn": 501, "name": "PhD", "id": "B", "contribution": " which is , after two stages of Wiener filtering . I mean , that was I just {disfmarker} well , like the overall improvement is like fifty - six point five . So ,"}, {"turn": 502, "name": "Professor", "id": "C", "contribution": " Right ."}, {"turn": 503, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 504, "name": "PhD", "id": "B", "contribution": " I mean , his number is still better than what I got in the two stages of Wiener filtering ."}, {"turn": 505, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 506, "name": "Professor", "id": "C", "contribution": " Right ."}, {"turn": 507, "name": "PhD", "id": "D", "contribution": " On Italian . But on Finnish it 's a little bit worse , apparently ."}, {"turn": 508, "name": "PhD", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 509, "name": "PhD", "id": "D", "contribution": " Um {disfmarker}"}, {"turn": 510, "name": "Professor", "id": "C", "contribution": " But do you have numbers in terms of word error rates on {disfmarker} on Italian ? So just so you have some sense of reference ?"}, {"turn": 511, "name": "PhD", "id": "D", "contribution": " Yeah . Uh , so , it 's , uh , three point , uh , eight ."}, {"turn": 512, "name": "Professor", "id": "C", "contribution": " Uh - huh ."}, {"turn": 513, "name": "PhD", "id": "D", "contribution": " Am I right ?"}, {"turn": 514, "name": "PhD", "id": "B", "contribution": " Oh , OK . Yeah , right , OK ."}, {"turn": 515, "name": "PhD", "id": "D", "contribution": " And then , uh , d uh , nine point , uh , one ."}, {"turn": 516, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 517, "name": "PhD", "id": "D", "contribution": " And finally , uh , sixteen point five ."}, {"turn": 518, "name": "Professor", "id": "C", "contribution": " And this is , um , spectral subtraction plus what ?"}, {"turn": 519, "name": "PhD", "id": "D", "contribution": " Plus {disfmarker} plus nonlinear smoothing . Well , it 's {disfmarker} the system {disfmarker} it 's exactly the sys the same system as Sunil tried ,"}, {"turn": 520, "name": "Professor", "id": "C", "contribution": " On - line normalization and LDA ?"}, {"turn": 521, "name": "PhD", "id": "D", "contribution": " but {disfmarker}"}, {"turn": 522, "name": "Professor", "id": "C", "contribution": " Yeah . Yeah ."}, {"turn": 523, "name": "PhD", "id": "D", "contribution": " Yeah . But instead of double stage Wiener filtering , it 's {disfmarker} it 's this smoothed spectral subtraction . Um , yeah ."}, {"turn": 524, "name": "PhD", "id": "A", "contribution": " What is it the , um , France Telecom system uses"}, {"turn": 525, "name": "Professor", "id": "C", "contribution": " Right ."}, {"turn": 526, "name": "PhD", "id": "A", "contribution": " for {disfmarker} Do they use spectral subtraction , or Wiener filtering , or {disfmarker} ?"}, {"turn": 527, "name": "PhD", "id": "B", "contribution": " They use spectral subtraction , right ."}, {"turn": 528, "name": "PhD", "id": "D", "contribution": " For what ?"}, {"turn": 529, "name": "PhD", "id": "B", "contribution": " French Telecom ."}, {"turn": 530, "name": "PhD", "id": "D", "contribution": " It {disfmarker} it 's Wiener filtering ,"}, {"turn": 531, "name": "PhD", "id": "B", "contribution": " Oh , it 's {disfmarker} it 's Wiener filtering ."}, {"turn": 532, "name": "PhD", "id": "D", "contribution": " am I right ?"}, {"turn": 533, "name": "PhD", "id": "A", "contribution": " Oh ."}, {"turn": 534, "name": "PhD", "id": "B", "contribution": " Sorry ."}, {"turn": 535, "name": "PhD", "id": "D", "contribution": " Well , it 's some kind of Wiener filtering {disfmarker}"}, {"turn": 536, "name": "PhD", "id": "B", "contribution": " Yeah , filtering . Yeah , it 's not exactly Wiener filtering but some variant of Wiener filtering ."}, {"turn": 537, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 538, "name": "PhD", "id": "A", "contribution": " I see ."}, {"turn": 539, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 540, "name": "Professor", "id": "C", "contribution": " Yeah , plus , uh , I guess they have some sort of cepstral normalization , as well ."}, {"turn": 541, "name": "PhD", "id": "B", "contribution": " s They have like {disfmarker} yeah , th the {disfmarker} just noise compensation technique is a variant of Wiener filtering ,"}, {"turn": 542, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 543, "name": "PhD", "id": "B", "contribution": " plus they do some {disfmarker} some smoothing techniques on the final filter . The {disfmarker} th they actually do the filtering in the time domain ."}, {"turn": 544, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 545, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 546, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 547, "name": "PhD", "id": "B", "contribution": " So they would take this HF squared back , taking inverse Fourier transform . And they convolve the time domain signal with that ."}, {"turn": 548, "name": "PhD", "id": "A", "contribution": " Oh , I see ."}, {"turn": 549, "name": "PhD", "id": "B", "contribution": " And they do some smoothing on that final filter , impulse response ."}, {"turn": 550, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 551, "name": "PhD", "id": "D", "contribution": " But they also have two {disfmarker} two different smoothing @ @ ."}, {"turn": 552, "name": "PhD", "id": "B", "contribution": " I mean , I 'm {disfmarker} I 'm @ @ ."}, {"turn": 553, "name": "PhD", "id": "D", "contribution": " One in the time domain and one in the frequency domain by just taking the first , um , coefficients of the impulse response ."}, {"turn": 554, "name": "PhD", "id": "B", "contribution": " But ."}, {"turn": 555, "name": "PhD", "id": "D", "contribution": " So , basically it 's similar . I mean , what you did , it 's similar"}, {"turn": 556, "name": "PhD", "id": "B", "contribution": " It 's similar in the smoothing and {disfmarker}"}, {"turn": 557, "name": "PhD", "id": "D", "contribution": " because you have also two {disfmarker} two kind of smoothing ."}, {"turn": 558, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 559, "name": "PhD", "id": "D", "contribution": " One in the time domain , and one in the frequency domain ,"}, {"turn": 560, "name": "PhD", "id": "B", "contribution": " Yeah . The frequency domain ."}, {"turn": 561, "name": "PhD", "id": "D", "contribution": " yeah ."}, {"turn": 562, "name": "PhD", "id": "A", "contribution": " Does the smoothing in the time domain help {disfmarker}"}, {"turn": 563, "name": "PhD", "id": "D", "contribution": " Um {disfmarker}"}, {"turn": 564, "name": "PhD", "id": "A", "contribution": " Well , do you get this musical noise stuff with Wiener filtering or is that only with , uh , spectral subtraction ?"}, {"turn": 565, "name": "PhD", "id": "B", "contribution": " No , you get it with Wiener filtering also ."}, {"turn": 566, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 567, "name": "PhD", "id": "A", "contribution": " Does the smoothing in the time domain help with that ? Or some other smoothing ?"}, {"turn": 568, "name": "PhD", "id": "B", "contribution": " Oh , no , you still end up with zeros in the s spectrum . Sometimes ."}, {"turn": 569, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 570, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 571, "name": "Professor", "id": "C", "contribution": " I mean , it 's not clear that these musical noises hurt us in recognition ."}, {"turn": 572, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 573, "name": "Professor", "id": "C", "contribution": " We don't know if they do ."}, {"turn": 574, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 575, "name": "Professor", "id": "C", "contribution": " I mean , they {disfmarker} they sound bad ."}, {"turn": 576, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 577, "name": "PhD", "id": "B", "contribution": " Yeah , I know ."}, {"turn": 578, "name": "Professor", "id": "C", "contribution": " But we 're not listening to it , usually ."}, {"turn": 579, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 580, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 581, "name": "PhD", "id": "D", "contribution": " Uh , actually the {disfmarker} the smoothing that I did {disfmarker} do here reduced the musical noise . Well , it {disfmarker}"}, {"turn": 582, "name": "PhD", "id": "B", "contribution": " Mm - hmm . Yeah , yeah ,"}, {"turn": 583, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 584, "name": "PhD", "id": "B", "contribution": " the {disfmarker}"}, {"turn": 585, "name": "PhD", "id": "D", "contribution": " Well , I cannot {disfmarker} you cannot hear beca well , actually what I d did not say is that this is not in the FFT bins . This is in the mel frequency bands . Um {disfmarker} So , it could be seen as a f a {disfmarker} a smoothing in the frequency domain because I used , in ad mel bands in addition and then the other phase of smoothing in the time domain . Mmm . But , when you look at the spectrogram , if you don't have an any smoothing , you clearly see , like {disfmarker} in silence portions , and at the beginning and end of speech , you see spots of high energy randomly distributed over the {disfmarker} the spectrogram ."}, {"turn": 586, "name": "PhD", "id": "A", "contribution": " Mm - hmm . Mm - hmm ."}, {"turn": 587, "name": "PhD", "id": "D", "contribution": " Um {disfmarker}"}, {"turn": 588, "name": "PhD", "id": "A", "contribution": " That 's the musical noise ?"}, {"turn": 589, "name": "PhD", "id": "D", "contribution": " Which is musical noise ,"}, {"turn": 590, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 591, "name": "PhD", "id": "D", "contribution": " yeah , if {disfmarker} if it {disfmarker} If you listen to it {disfmarker} uh , if you do this in the FFT bins , then you have spots of energy randomly distributing . And if you f if you re - synthesize these spot sounds as , like , sounds ,"}, {"turn": 592, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 593, "name": "PhD", "id": "D", "contribution": " uh {disfmarker}"}, {"turn": 594, "name": "Professor", "id": "C", "contribution": " Well , none of these systems , by the way , have {disfmarker} I mean , y you both are {disfmarker} are working with , um , our system that does not have the neural net ,"}, {"turn": 595, "name": "PhD", "id": "D", "contribution": " And {disfmarker}"}, {"turn": 596, "name": "PhD", "id": "B", "contribution": " Yep ."}, {"turn": 597, "name": "Professor", "id": "C", "contribution": " right ?"}, {"turn": 598, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 599, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 600, "name": "Professor", "id": "C", "contribution": " OK . So one would hope , presumably , that the neural net part of it would {disfmarker} would improve things further as {disfmarker} as they did before ."}, {"turn": 601, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah . Um {disfmarker} Yeah , although if {disfmarker} if we , um , look at the result from the proposals , {comment} one of the reason , uh , the n system with the neural net was , um , more than {disfmarker} well , around five percent better , is that it was much better on highly mismatched condition . I 'm thinking , for instance , on the TI - digits trained on clean speech and tested on noisy speech ."}, {"turn": 602, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 603, "name": "PhD", "id": "D", "contribution": " Uh , for this case , the system with the neural net was much better ."}, {"turn": 604, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 605, "name": "PhD", "id": "D", "contribution": " But not much on the {disfmarker} in the other cases ."}, {"turn": 606, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 607, "name": "PhD", "id": "D", "contribution": " And if we have no , uh , spectral subtraction or Wiener filtering , um , i the system is {disfmarker} Uh , we thought the neural {disfmarker} neural network is much better than before , even in these cases of high mismatch . So , maybe the neural net will help less but , um {disfmarker}"}, {"turn": 608, "name": "Professor", "id": "C", "contribution": " Maybe ."}, {"turn": 609, "name": "PhD", "id": "A", "contribution": " Could you train a neural net to do spectral subtraction ?"}, {"turn": 610, "name": "Professor", "id": "C", "contribution": " Yeah , it could do a nonlinear spectral subtraction"}, {"turn": 611, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 612, "name": "Professor", "id": "C", "contribution": " but I don't know if it {disfmarker} I mean , you have to figure out what your targets are ."}, {"turn": 613, "name": "PhD", "id": "A", "contribution": " Yeah , I was thinking if you had a clean version of the signal and {disfmarker} and a noisy version , and your targets were the M F - uh , you know , whatever , frequency bins {disfmarker}"}, {"turn": 614, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 615, "name": "Professor", "id": "C", "contribution": " Right ."}, {"turn": 616, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 617, "name": "Professor", "id": "C", "contribution": " Yeah , well , that 's not so much spectral subtraction then ,"}, {"turn": 618, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 619, "name": "Professor", "id": "C", "contribution": " but {disfmarker} but {disfmarker} but it 's {disfmarker} but at any rate , yeah , people , uh {disfmarker}"}, {"turn": 620, "name": "PhD", "id": "A", "contribution": " People do that ?"}, {"turn": 621, "name": "Professor", "id": "C", "contribution": " y yeah , in fact , we had visitors here who did that I think when you were here ba way back when ."}, {"turn": 622, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 623, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 624, "name": "Professor", "id": "C", "contribution": " Uh , people {disfmarker} d done lots of experimentation over the years with training neural nets . And it 's not a bad thing to do . It 's another approach ."}, {"turn": 625, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 626, "name": "Professor", "id": "C", "contribution": " M I mean , it 's {disfmarker} it , um {disfmarker}"}, {"turn": 627, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 628, "name": "Professor", "id": "C", "contribution": " The objection everyone always raises , which has some truth to it is that , um , it 's good for mapping from a particular noise to clean but then you get a different noise ."}, {"turn": 629, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 630, "name": "Professor", "id": "C", "contribution": " And the experiments we saw that visitors did here showed that it {disfmarker} there was at least some , um , {vocalsound} {comment} gentleness to the degradation when you switched to different noises . It did seem to help . So that {disfmarker} you 're right , that 's another {disfmarker} another way to go ."}, {"turn": 631, "name": "PhD", "id": "A", "contribution": " How did it compare on {disfmarker} I mean , for {disfmarker} for good cases where it {disfmarker} it {disfmarker} uh , stuff that it was trained on ? Did it do pretty well ?"}, {"turn": 632, "name": "Professor", "id": "C", "contribution": " Oh , yeah , it did very well ."}, {"turn": 633, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 634, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 635, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 636, "name": "Professor", "id": "C", "contribution": " Um ,"}, {"turn": 637, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 638, "name": "Professor", "id": "C", "contribution": " but to some extent that 's kind of what we 're doing . I mean , we 're not doing exactly that , we 're not trying to generate good examples but by trying to do the best classifier you possibly can , for these little phonetic categories ,"}, {"turn": 639, "name": "PhD", "id": "A", "contribution": " Mm - hmm . You could say it 's sort of built in ."}, {"turn": 640, "name": "Professor", "id": "C", "contribution": " It 's {disfmarker} Yeah , it 's kind of built into that ."}, {"turn": 641, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 642, "name": "Professor", "id": "C", "contribution": " And {disfmarker} and that 's why we have found that it {disfmarker} it does help ."}, {"turn": 643, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 644, "name": "Professor", "id": "C", "contribution": " Um {disfmarker} so , um , yeah , I mean , we 'll just have to try it . But I {disfmarker} I would {disfmarker} I would {disfmarker} I would imagine that it will help some . I mean , it {disfmarker} we 'll just have to see whether it helps more or less the same , but I would imagine it would help some ."}, {"turn": 645, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 646, "name": "Professor", "id": "C", "contribution": " So in any event , all of this {disfmarker} I was just confirming that all of this was with a simpler system ."}, {"turn": 647, "name": "PhD", "id": "D", "contribution": " Yeah ,"}, {"turn": 648, "name": "Professor", "id": "C", "contribution": " OK ?"}, {"turn": 649, "name": "PhD", "id": "D", "contribution": " yeah . Um , Yeah , so this is th the , um {disfmarker} Well , actually , this was kind of the first try with this spectral subtraction plus smoothing ,"}, {"turn": 650, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 651, "name": "PhD", "id": "D", "contribution": " and I was kind of excited by the result ."}, {"turn": 652, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 653, "name": "PhD", "id": "D", "contribution": " Um , then I started to optimize the different parameters . And , uh , the first thing I tried to optimize is the , um , time constant of the smoothing . And it seems that the one that I chose for the first experiment was the optimal one , so {vocalsound} uh ,"}, {"turn": 654, "name": "Professor", "id": "C", "contribution": " It 's amazing how often that happens ."}, {"turn": 655, "name": "PhD", "id": "D", "contribution": " Um , so this is the first thing . Um {disfmarker} Yeah , another thing that I {disfmarker} it 's important to mention is , um , that this has a this has some additional latency . Um . Because when I do the smoothing , uh , it 's a recursion that estimated the means , so {disfmarker} of the g of the gain curve . And this is a filter that has some latency . And I noticed that it 's better if we take into account this latency . So , instead o of using the current estimated mean to , uh , subtract the current frame , it 's better to use an estimate that 's some somewhere in the future . Um {disfmarker}"}, {"turn": 656, "name": "PhD", "id": "A", "contribution": " And that 's what causes the latency ? OK ."}, {"turn": 657, "name": "PhD", "id": "B", "contribution": " You mean , the m the mean is computed o based on some frames in the future also ?"}, {"turn": 658, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 659, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 660, "name": "PhD", "id": "B", "contribution": " Or {disfmarker} or no ?"}, {"turn": 661, "name": "PhD", "id": "D", "contribution": " It 's the recursion , so it 's {disfmarker} it 's the center recursion , right ?"}, {"turn": 662, "name": "PhD", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 663, "name": "PhD", "id": "D", "contribution": " Um {disfmarker} and the latency of this recursion is around fifty milliseconds ."}, {"turn": 664, "name": "Professor", "id": "C", "contribution": " One five ?"}, {"turn": 665, "name": "PhD", "id": "D", "contribution": ""}, {"turn": 666, "name": "Professor", "id": "C", "contribution": " One five ? Five zero ?"}, {"turn": 667, "name": "PhD", "id": "D", "contribution": " Five zero ,"}, {"turn": 668, "name": "Professor", "id": "C", "contribution": " Five zero ."}, {"turn": 669, "name": "PhD", "id": "D", "contribution": " yeah ."}, {"turn": 670, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 671, "name": "PhD", "id": "D", "contribution": " Um ,"}, {"turn": 672, "name": "PhD", "id": "B", "contribution": " I 'm sorry ,"}, {"turn": 673, "name": "PhD", "id": "D", "contribution": " mmm ."}, {"turn": 674, "name": "PhD", "id": "B", "contribution": " why {disfmarker} why is that delay coming ? Like , you estimate the mean ?"}, {"turn": 675, "name": "PhD", "id": "D", "contribution": " Yeah , the mean estimation has some delay , right ?"}, {"turn": 676, "name": "PhD", "id": "B", "contribution": " Oh , yeah ."}, {"turn": 677, "name": "PhD", "id": "D", "contribution": " I mean , the {disfmarker} the filter that {disfmarker} that estimates the mean has a time constant ."}, {"turn": 678, "name": "PhD", "id": "B", "contribution": " It isn't {disfmarker} OK , so it 's like it looks into the future also . OK ."}, {"turn": 679, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 680, "name": "Professor", "id": "C", "contribution": " What if you just look into the past ?"}, {"turn": 681, "name": "PhD", "id": "D", "contribution": " It 's , uh , not as good . It 's not bad ."}, {"turn": 682, "name": "Professor", "id": "C", "contribution": " How m by how much ?"}, {"turn": 683, "name": "PhD", "id": "D", "contribution": " Um , it helps a lot over the ba the baseline but , mmm {disfmarker}"}, {"turn": 684, "name": "Professor", "id": "C", "contribution": " By how much ?"}, {"turn": 685, "name": "PhD", "id": "D", "contribution": " it {disfmarker} It 's around three percent , um , relative ."}, {"turn": 686, "name": "Professor", "id": "C", "contribution": " Worse ."}, {"turn": 687, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah . Um ,"}, {"turn": 688, "name": "Professor", "id": "C", "contribution": " Hmm ."}, {"turn": 689, "name": "PhD", "id": "D", "contribution": " mmm {disfmarker} So , uh {disfmarker}"}, {"turn": 690, "name": "Professor", "id": "C", "contribution": " It 's depending on how all this stuff comes out we may or may not be able to add any latency ."}, {"turn": 691, "name": "PhD", "id": "D", "contribution": " Yeah , but {disfmarker} Yeah . So , yeah , it depends . Uh , y actually , it 's {disfmarker} it 's l it 's three percent . Right . Mmm . Yeah , b but I don't think we have to worry too much on that right now while {disfmarker} you kno . Mm - hmm ."}, {"turn": 692, "name": "Professor", "id": "C", "contribution": " Um , s Yeah , I mean , I think the only thing is that {disfmarker}"}, {"turn": 693, "name": "PhD", "id": "D", "contribution": " So {disfmarker}"}, {"turn": 694, "name": "Professor", "id": "C", "contribution": " I would worry about it a little ."}, {"turn": 695, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 696, "name": "Professor", "id": "C", "contribution": " Because if we completely ignore latency , and then we discover that we really have to do something about it , we 're going to be {disfmarker} find ourselves in a bind ."}, {"turn": 697, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 698, "name": "Professor", "id": "C", "contribution": " So , um , you know , maybe you could make it twenty - five . You know what I mean ?"}, {"turn": 699, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 700, "name": "Professor", "id": "C", "contribution": " Yeah , just , you know , just be {disfmarker} be a little conservative"}, {"turn": 701, "name": "PhD", "id": "D", "contribution": " Oh yes ."}, {"turn": 702, "name": "Professor", "id": "C", "contribution": " because we may end up with this crunch where all of a sudden we have to cut the latency in half or something ."}, {"turn": 703, "name": "PhD", "id": "D", "contribution": " s Mm - hmm . Yeah ."}, {"turn": 704, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 705, "name": "PhD", "id": "D", "contribution": " Um . So , yeah , there are other things in the , um , algorithm that I didn't , uh , @ @ a lot yet ,"}, {"turn": 706, "name": "PhD", "id": "A", "contribution": " Oh !"}, {"turn": 707, "name": "PhD", "id": "D", "contribution": " which {disfmarker}"}, {"turn": 708, "name": "PhD", "id": "A", "contribution": " Sorry . A quick question just about the latency thing . If {disfmarker} if there 's another part of the system that causes a latency of a hundred milliseconds , is this an additive thing ? Or c or is yours hidden in that ?"}, {"turn": 709, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 710, "name": "PhD", "id": "A", "contribution": " Uh {disfmarker}"}, {"turn": 711, "name": "PhD", "id": "D", "contribution": " No , it 's {disfmarker} it 's added ."}, {"turn": 712, "name": "PhD", "id": "A", "contribution": " It 's additive . OK ."}, {"turn": 713, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 714, "name": "PhD", "id": "B", "contribution": " We can {disfmarker} OK . We can do something in parallel also , in some like {disfmarker} some cases like , if you wanted to do voice activity detection ."}, {"turn": 715, "name": "PhD", "id": "A", "contribution": " Uh - huh ."}, {"turn": 716, "name": "PhD", "id": "B", "contribution": " And we can do that in parallel with some other filtering you can do ."}, {"turn": 717, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 718, "name": "PhD", "id": "B", "contribution": " So you can make a decision on that voice activity detection and then you decide whether you want to filter or not ."}, {"turn": 719, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 720, "name": "PhD", "id": "B", "contribution": " But by then you already have the sufficient samples to do the filtering ."}, {"turn": 721, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 722, "name": "PhD", "id": "B", "contribution": " So {disfmarker} So , sometimes you can do it anyway ."}, {"turn": 723, "name": "PhD", "id": "A", "contribution": " I mean , couldn't , uh {disfmarker} I {disfmarker} Couldn't you just also {disfmarker} I mean , i if you know that the l the largest latency in the system is two hundred milliseconds , don't you {disfmarker} couldn't you just buffer up that number of frames and then everything uses that buffer ?"}, {"turn": 724, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 725, "name": "PhD", "id": "A", "contribution": " And that way it 's not additive ?"}, {"turn": 726, "name": "Professor", "id": "C", "contribution": " Well , in fact , everything is sent over in buffers cuz of {disfmarker} isn't it the TCP buffer some {disfmarker} ?"}, {"turn": 727, "name": "PhD", "id": "B", "contribution": " You mean , the {disfmarker} the data , the super frame or something ?"}, {"turn": 728, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 729, "name": "Professor", "id": "C", "contribution": " Yeah , yeah ."}, {"turn": 730, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 731, "name": "PhD", "id": "B", "contribution": " Yeah , but that has a variable latency because the last frame doesn't have any latency"}, {"turn": 732, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 733, "name": "PhD", "id": "B", "contribution": " and first frame has a twenty framed latency . So you can't r rely on that latency all the time ."}, {"turn": 734, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 735, "name": "PhD", "id": "B", "contribution": " Because {disfmarker} I mean the transmission over {disfmarker} over the air interface is like a buffer ."}, {"turn": 736, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 737, "name": "PhD", "id": "B", "contribution": " Twenty frame {disfmarker}"}, {"turn": 738, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 739, "name": "PhD", "id": "B", "contribution": " twenty four frames ."}, {"turn": 740, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 741, "name": "PhD", "id": "B", "contribution": " So {disfmarker} But the only thing is that the first frame in that twenty - four frame buffer has a twenty - four frame latency . And the last frame doesn't have any latency ."}, {"turn": 742, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 743, "name": "PhD", "id": "B", "contribution": " Because it just goes as {disfmarker}"}, {"turn": 744, "name": "PhD", "id": "A", "contribution": " Yeah , I wasn't thinking of that one in particular"}, {"turn": 745, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 746, "name": "PhD", "id": "A", "contribution": " but more of , you know , if {disfmarker} if there is some part of your system that has to buffer twenty frames , uh , can't the other parts of the system draw out of that buffer and therefore not add to the latency ?"}, {"turn": 747, "name": "Professor", "id": "C", "contribution": " Yeah . Yeah . And {disfmarker} and that 's sort of one of the {disfmarker} all of that sort of stuff is things that they 're debating in their standards committee ."}, {"turn": 748, "name": "PhD", "id": "A", "contribution": " Oh ! Hmm ."}, {"turn": 749, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Yeah . So , um , there is uh , {comment} these parameters that I still have to {disfmarker} to look at . Like , I played a little bit with this overestimation factor , uh , but I still have to {disfmarker} to look more at this , um , at the level of noise I add after . Uh , I know that adding noise helped , um , the system just using spectral subtraction without smoothing , but I don't know right now if it 's still important or not , and if the level I choose before is still the right one . Same thing for the shape of the {disfmarker} the noise . Maybe it would be better to add just white noise instead of speech shaped noise ."}, {"turn": 750, "name": "Professor", "id": "C", "contribution": " That 'd be more like the JRASTA thing in a sense . Yeah ."}, {"turn": 751, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Um , yep . Uh , and another thing is to {disfmarker} Yeah , for this I just use as noise estimate the mean , uh , spectrum of the first twenty frames of each utterance . I don't remember for this experiment what did you use for these two stage {disfmarker}"}, {"turn": 752, "name": "PhD", "id": "B", "contribution": " I used ten {disfmarker} just ten frames . Yeah , because {disfmarker}"}, {"turn": 753, "name": "PhD", "id": "D", "contribution": " The ten frames ?"}, {"turn": 754, "name": "PhD", "id": "B", "contribution": " I mean , the reason was like in TI - digits I don't have a lot . I had twenty frames most of the time ."}, {"turn": 755, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Um . But , so what 's this result you told me about , the fact that if you use more than ten frames you can {disfmarker} improve by t"}, {"turn": 756, "name": "PhD", "id": "B", "contribution": " Well , that 's {disfmarker} that 's using the channel zero . If I use a channel zero VAD to estimate the noise ."}, {"turn": 757, "name": "PhD", "id": "D", "contribution": " Oh , OK ."}, {"turn": 758, "name": "PhD", "id": "B", "contribution": " Which {disfmarker}"}, {"turn": 759, "name": "PhD", "id": "D", "contribution": " But this is ten frames plus {disfmarker} plus"}, {"turn": 760, "name": "PhD", "id": "B", "contribution": " Channel zero dropping ."}, {"turn": 761, "name": "PhD", "id": "D", "contribution": " channel {disfmarker}"}, {"turn": 762, "name": "PhD", "id": "B", "contribution": " Hmm ."}, {"turn": 763, "name": "PhD", "id": "D", "contribution": " Uh , no , these results with two stage Wiener filtering is ten frames"}, {"turn": 764, "name": "PhD", "id": "B", "contribution": " t Oh , this {disfmarker}"}, {"turn": 765, "name": "PhD", "id": "D", "contribution": " but possibly more . I mean , if channel one VAD gives you {disfmarker}"}, {"turn": 766, "name": "PhD", "id": "B", "contribution": " f Yeah . Mm - hmm . Yeah ."}, {"turn": 767, "name": "PhD", "id": "D", "contribution": " Yeah . OK . Yeah , but in this experiment I did {disfmarker} I didn't use any VAD . I just used the twenty first frame to estimate the noise . And {disfmarker} So I expected it to be a little bit better , {vocalsound} if , uh , I use more {disfmarker} more frames . Um . OK , that 's it for spectral subtraction . The second thing I was working on is to , um , try to look at noise estimation , {comment} mmm , and using some technique that doesn't need voice activity detection . Um , and for this I u simply used some code that , uh , {vocalsound} I had from {disfmarker} from Belgium , which is technique that , um , takes a bunch of frame , um , and for each frequency bands of this frame , takes a look at the minima of the energy . And then average these minima and take this as an {disfmarker} an energy estimate of the noise for this particular frequency band . And there is something more to this actually . What is done is that , {vocalsound} uh , these minima are computed , um , based on , um , high resolution spectra . So , I compute an FFT based on the long , uh , signal frame which is sixty - four millisecond {disfmarker}"}, {"turn": 768, "name": "PhD", "id": "A", "contribution": " So you have one minimum for each frequency ?"}, {"turn": 769, "name": "PhD", "id": "D", "contribution": " What {disfmarker} what I {disfmarker} what I d uh , I do actually , is to take a bunch of {disfmarker} to take a tile on the spectrogram and this tile is five hundred milliseconds long and two hundred hertz wide ."}, {"turn": 770, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 771, "name": "PhD", "id": "D", "contribution": " And this tile {disfmarker} Uh , in this tile appears , like , the harmonics if you have a voiced sound , because it 's {disfmarker} it 's the FTT bins . And when you take the m the minima of {disfmarker} of these {disfmarker} this tile , when you don't have speech , these minima will give you some noise level estimate , If you have voiced speech , these minima will still give you some noise estimate because the minima are between the harmonics . And {disfmarker} If you have other {disfmarker} other kind of speech sounds then it 's not the case , but if the time frame is long enough , uh , like s five hundred milliseconds seems to be long enough , {comment} you still have portions which , uh , are very close {disfmarker} whi which minima are very close to the noise energy ."}, {"turn": 772, "name": "Professor", "id": "C", "contribution": " I 'm confused . You said five hundred milliseconds"}, {"turn": 773, "name": "PhD", "id": "D", "contribution": " Mmm ?"}, {"turn": 774, "name": "Professor", "id": "C", "contribution": " but you said sixty - four milliseconds . Which is which ? What ?"}, {"turn": 775, "name": "PhD", "id": "D", "contribution": " Sixty - four milliseconds is to compute the FFT , uh , bins ."}, {"turn": 776, "name": "Professor", "id": "C", "contribution": " Yeah ,"}, {"turn": 777, "name": "PhD", "id": "D", "contribution": " The {disfmarker} the FFT ."}, {"turn": 778, "name": "Professor", "id": "C", "contribution": " yeah ."}, {"turn": 779, "name": "PhD", "id": "D", "contribution": " Um , actually it 's better to use sixty - four milliseconds because , um , if you use thirty milliseconds , then , uh , because of the {disfmarker} this short windowing and at low pitch , uh , sounds , {vocalsound} the harmonics are not , wha uh , correctly separated ."}, {"turn": 780, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 781, "name": "PhD", "id": "D", "contribution": " So if you take these minima , it {disfmarker} b {vocalsound} they will overestimate the noise a lot ."}, {"turn": 782, "name": "Professor", "id": "C", "contribution": " So you take sixty - four millisecond F F Ts and then you average them {comment} over five hundred ? Or {disfmarker} ? Uh , what do you do over five hundred ?"}, {"turn": 783, "name": "PhD", "id": "D", "contribution": " So I take {disfmarker} to {disfmarker} I take a bunch of these sixty - four millisecond frame to cover five hundred milliseconds ,"}, {"turn": 784, "name": "Professor", "id": "C", "contribution": " Ah . OK ."}, {"turn": 785, "name": "PhD", "id": "D", "contribution": " and then I look for the minima ,"}, {"turn": 786, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 787, "name": "Professor", "id": "C", "contribution": " I see ."}, {"turn": 788, "name": "PhD", "id": "D", "contribution": " on the {disfmarker} on {disfmarker} on the bunch of uh fifty frames , right ?"}, {"turn": 789, "name": "Professor", "id": "C", "contribution": " I see ."}, {"turn": 790, "name": "PhD", "id": "D", "contribution": " Mmm . So the interest of this is that , as y with this technique you can estimate u some reasonable noise spectra with only five hundred milliseconds of {disfmarker} of signal , so if the {disfmarker} the n the noise varies a lot , uh , you can track {disfmarker} better track the noise ,"}, {"turn": 791, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 792, "name": "PhD", "id": "D", "contribution": " which is not the case if you rely on the voice activity detector . So even if there are no no speech pauses , you can track the noise level . The only requirement is that you must have , in these five hundred milliseconds segment , {comment} you must have voiced sound at least . Cuz this {disfmarker} these will help you to {disfmarker} to track the {disfmarker} the noise level . Um . So what I did is just to simply replace the VAD - based , uh , noise estimate by this estimate , first on SpeechDat - Car {disfmarker} Well , only on SpeechDat - Car actually . And it 's , uh , slightly worse , like one percent relative compared to the VAD - based {pause} estimates . Um , I think the reason why it 's not better , is that the SpeechDat - Car noises are all stationary . Um . So , u y y there really is no need to have something that 's adaptive"}, {"turn": 793, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 794, "name": "PhD", "id": "D", "contribution": " and {disfmarker} Uh , well , they are mainly stationary . Um . But , I expect s maybe some improvement on TI - digits because , nnn , in this case the noises are all sometimes very variable . Uh , so I have to test it . Mmm ."}, {"turn": 795, "name": "Professor", "id": "C", "contribution": " But are you comparing with something {disfmarker} e I 'm {disfmarker} I 'm {disfmarker} p s a little confused again , i it {disfmarker} Uh , when you compare it with the V A D - based ,"}, {"turn": 796, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 797, "name": "Professor", "id": "C", "contribution": " VAD - Is this {disfmarker} is this the {disfmarker} ?"}, {"turn": 798, "name": "PhD", "id": "D", "contribution": " It 's {disfmarker} It 's the France - Telecom - based spectra , s uh , Wiener filtering and VAD . So it 's their system but just I replace their noise estimate by this one ."}, {"turn": 799, "name": "Professor", "id": "C", "contribution": " Oh , you 're not doing this with our system ?"}, {"turn": 800, "name": "PhD", "id": "D", "contribution": " In i I 'm not {disfmarker} No , no . Yeah , it 's our system but with just the Wiener filtering from their system . Right ? Mmm ."}, {"turn": 801, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 802, "name": "PhD", "id": "D", "contribution": " Yeah . Actually , th the best system that we still have is , uh , our system but with their noise compensation scheme , right ?"}, {"turn": 803, "name": "Professor", "id": "C", "contribution": " Right . But {disfmarker}"}, {"turn": 804, "name": "PhD", "id": "D", "contribution": " So I 'm trying to improve on this , and {disfmarker} by {disfmarker} by replacing their noise estimate by , uh , something that might be better ."}, {"turn": 805, "name": "Professor", "id": "C", "contribution": " OK . But the spectral subtraction scheme that you reported on also re requires a {disfmarker} a noise estimate ."}, {"turn": 806, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah ."}, {"turn": 807, "name": "Professor", "id": "C", "contribution": " Couldn't you try this for that ?"}, {"turn": 808, "name": "PhD", "id": "D", "contribution": " But I di"}, {"turn": 809, "name": "Professor", "id": "C", "contribution": " Do you think it might help ?"}, {"turn": 810, "name": "PhD", "id": "D", "contribution": " Not yet , because I did this in parallel ,"}, {"turn": 811, "name": "Professor", "id": "C", "contribution": " I see ,"}, {"turn": 812, "name": "PhD", "id": "D", "contribution": " and I was working on one and the other ."}, {"turn": 813, "name": "Professor", "id": "C", "contribution": " I see . Yeah ."}, {"turn": 814, "name": "PhD", "id": "D", "contribution": " Um ,"}, {"turn": 815, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 816, "name": "PhD", "id": "D", "contribution": " Yeah , for {disfmarker} for sure I will . I can try also , mmm , the spectral subtraction ."}, {"turn": 817, "name": "PhD", "id": "B", "contribution": " So I 'm also using that n new noise estimate technique on this Wiener filtering what I 'm trying ."}, {"turn": 818, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 819, "name": "PhD", "id": "B", "contribution": " So I {disfmarker} I have , like , some experiments running , I don't have the results ."}, {"turn": 820, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 821, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 822, "name": "PhD", "id": "B", "contribution": " So ."}, {"turn": 823, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 824, "name": "PhD", "id": "B", "contribution": " I don't estimate the f noise on the ten frames but use his estimate ."}, {"turn": 825, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 826, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Um . Yeah . I , um , also implemented a sp um {disfmarker} spectral whitening idea which is in the , um , Ericsson proposal . Uh , the idea is just to {vocalsound} um , flatten the log , uh , spectrum , um , and to flatten it more if the {disfmarker} the probability of silence is higher . So in this way , you can also reduce {disfmarker} somewhat reduce the musical noise and you reduce the variability if you have different noise shapes , because the {disfmarker} the spectrum becomes more flat in the silence portions . Um . Yeah . With this , no improvement , uh , but there are a lot of parameters that we can play with and , um {disfmarker} Actually , this {disfmarker} this could be seen as a soft version of the frame dropping because , um , you could just put the threshold and say that \" below the threshold , I will flatten {disfmarker} comp completely flatten the {disfmarker} the spectrum \" . And above this threshold , uh , keep the same spectrum . So it would be like frame dropping , because during the silence portions which are below the threshold of voice activity probability , {comment} uh , w you would have some kind of dummy frame which is a perfectly flat spectrum . And this , uh , whitening is something that 's more soft because , um , you whiten {disfmarker} you just , uh , have a function {disfmarker} the whitening is a function of the speech probability , so it 's not a hard decision ."}, {"turn": 827, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 828, "name": "PhD", "id": "D", "contribution": " Um , so I think maybe it can be used together with frame dropping and when we are not sure about if it 's speech or silence , well , maybe it has something do with this ."}, {"turn": 829, "name": "Professor", "id": "C", "contribution": " It 's interesting . I mean , um , you know , in {disfmarker} in JRASTA we were essentially adding in , uh , white {disfmarker} uh , white noise dependent on our estimate of the noise ."}, {"turn": 830, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 831, "name": "Professor", "id": "C", "contribution": " On the overall estimate of the noise . Uh , I think it never occurred to us to use a probability in there ."}, {"turn": 832, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 833, "name": "Professor", "id": "C", "contribution": " You could imagine one that {disfmarker} that {disfmarker} that made use of where {disfmarker} where the amount that you added in was , uh , a function of the probability of it being s speech or noise ."}, {"turn": 834, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Mm - hmm . Yeah , w Yeah , right now it 's a constant that just depending on the {disfmarker} the noise spectrum ."}, {"turn": 835, "name": "PhD", "id": "B", "contribution": " There 's {disfmarker}"}, {"turn": 836, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 837, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Mm - hmm ."}, {"turn": 838, "name": "Professor", "id": "C", "contribution": " Cuz that {disfmarker} that brings in sort of powers of classifiers that we don't really have in , uh , this other estimate . So it could be {disfmarker} it could be interesting ."}, {"turn": 839, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Mm - hmm ."}, {"turn": 840, "name": "Professor", "id": "C", "contribution": " What {disfmarker} what {disfmarker} what point does the , uh , system stop recording ? How much {disfmarker}"}, {"turn": 841, "name": "PhD", "id": "A", "contribution": " It 'll keep going till {disfmarker} I guess when they run out of disk space ,"}, {"turn": 842, "name": "Professor", "id": "C", "contribution": " It went a little long ? I mean , disk {disfmarker}"}, {"turn": 843, "name": "PhD", "id": "A", "contribution": " but {disfmarker} I think we 're OK ."}, {"turn": 844, "name": "PhD", "id": "D", "contribution": " So ."}, {"turn": 845, "name": "Professor", "id": "C", "contribution": " OK ."}, {"turn": 846, "name": "PhD", "id": "D", "contribution": " Yeah . Uh {disfmarker} Yeah , so there are {disfmarker} with this technique there are some {disfmarker} I just did something exactly the same as {disfmarker} as the Ericsson proposal but , um , {vocalsound} the probability of speech is not computed the same way . And I think , i for {disfmarker} yeah , for a lot of things , actually a g a good speech probability is important . Like for frame dropping you improve , like {disfmarker} you can improve from ten percent as Sunil showed , if you use the channel zero speech probabilities ."}, {"turn": 847, "name": "Professor", "id": "C", "contribution": " Mm - hmm . Mm - hmm ."}, {"turn": 848, "name": "PhD", "id": "D", "contribution": " For this it might help , um {disfmarker}"}, {"turn": 849, "name": "Professor", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 850, "name": "PhD", "id": "D", "contribution": " S so , yeah . Uh , so yeah , the next thing I started to do is to , {vocalsound} uh , try to develop a better voice activity detector . And , um {disfmarker} I d um {disfmarker} yeah , for this I think we can maybe try to train the neural network for voice activity detection on all the data that we have , including all the SpeechDat - Car data . Um {disfmarker} And so I 'm starting to obtain alignments on these databases . Um , and the way I mi I do that is that I just use the HTK system but I train it only on the close - talking microphone . And then I aligned {disfmarker} I obtained the Viterbi alignment of the training utterances . Um {disfmarker} It seems to be , uh i Actually what I observed is that for Italian it doesn't seem {disfmarker} Th - there seems to be a problem ."}, {"turn": 851, "name": "PhD", "id": "B", "contribution": " No . So , it doesn't seems to help by their use of channel zero or channel one ."}, {"turn": 852, "name": "PhD", "id": "D", "contribution": " Well . Because {disfmarker} What ?"}, {"turn": 853, "name": "PhD", "id": "B", "contribution": " Uh , you mean their d the frame dropping , right ? Yeah , it doesn't {disfmarker}"}, {"turn": 854, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah . So , u but actually the VAD was trained on Italian also ,"}, {"turn": 855, "name": "PhD", "id": "B", "contribution": " Italian ."}, {"turn": 856, "name": "PhD", "id": "D", "contribution": " so {disfmarker} Um , the c the current VAD that we have was trained on , uh , t SPINE , right ?"}, {"turn": 857, "name": "PhD", "id": "B", "contribution": " TI - digits ."}, {"turn": 858, "name": "PhD", "id": "D", "contribution": " Italian , and TI - digits with noise and {disfmarker}"}, {"turn": 859, "name": "PhD", "id": "B", "contribution": ""}, {"turn": 860, "name": "PhD", "id": "D", "contribution": " Uh , yeah . And it seems to work on Italian but not on the Finnish and Spanish data . So , maybe one reason is that s s Finnish and Spanish noise are different . And actually we observed {disfmarker} we listened to some of the utterances and sometimes for Finnish there is music in the recordings and strange things , right ?"}, {"turn": 861, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 862, "name": "PhD", "id": "D", "contribution": " Um {disfmarker} Yeah , so the idea was to train all the databases and obtain an alignment to train on these databases , and , um , also to , um , try different kind of features , {vocalsound} uh , as input to the VAD network . And we came up with a bunch of features that we want to try like , um , the spectral slope , the , um , the degree o degree of voicing with the features that , uh , we started to develop with Carmen , um , e with , uh , the correlation between bands and different kind of features ,"}, {"turn": 863, "name": "PhD", "id": "B", "contribution": " Yeah . Mm - hmm ."}, {"turn": 864, "name": "PhD", "id": "D", "contribution": " and {disfmarker} Yeah ."}, {"turn": 865, "name": "PhD", "id": "B", "contribution": " The energy also ."}, {"turn": 866, "name": "PhD", "id": "D", "contribution": " The energy ."}, {"turn": 867, "name": "PhD", "id": "B", "contribution": " Yeah ."}, {"turn": 868, "name": "Professor", "id": "C", "contribution": " Yeah , right ."}, {"turn": 869, "name": "PhD", "id": "D", "contribution": " Yeah . Of course . Yeah ."}, {"turn": 870, "name": "Professor", "id": "C", "contribution": " OK . Well , Hans - Guenter will be here next week so I think he 'll be interested in all {disfmarker} all of these things . And , so ."}, {"turn": 871, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 872, "name": "Professor", "id": "C", "contribution": " Mmm ."}, {"turn": 873, "name": "PhD", "id": "A", "contribution": " OK , shall we , uh , do digits ?"}, {"turn": 874, "name": "Professor", "id": "C", "contribution": " Yeah ."}, {"turn": 875, "name": "PhD", "id": "A", "contribution": " Want to go ahead , Morgan ?"}, {"turn": 876, "name": "Professor", "id": "C", "contribution": " Sure ."}, {"turn": 877, "name": "PhD", "id": "A", "contribution": " OK ."}]}