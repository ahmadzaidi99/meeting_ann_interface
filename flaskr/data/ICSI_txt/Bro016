Turn 0, E (Professor): Let 's see . Test ? Test ? Yeah . OK .
Turn 1, A (Grad): Hello ?
Turn 2, B (PhD): Channel one .
Turn 3, A (Grad): Hello ?
Turn 4, C (PhD): Test .
Turn 5, E (Professor): I was saying Hynek 'll be here next week , uh , Wednesday through Friday {disfmarker} uh , through Saturday , and , um , I won't be here Thursday and Friday . But my suggestion is that , uh , at least for this meeting , people should go ahead , uh , cuz Hynek will be here , and , you know , we don't have any Czech accent yet , uh , {vocalsound} as far as I know , so {disfmarker} There we go .
Turn 6, F (PhD): OK .
Turn 7, E (Professor): Um . So other than reading digits , what 's our agenda ?
Turn 8, F (PhD): I don't really have , uh , anything new . Been working on {pause} Meeting Recorder stuff . So .
Turn 9, E (Professor): OK . Um . Do you think that would be the case for next week also ? Or is {disfmarker} is , uh {disfmarker} ? What 's your projection on {disfmarker} ?
Turn 10, F (PhD): Um .
Turn 11, E (Professor): Cuz the one thing {disfmarker} the one thing that seems to me we really should try , if you hadn't tried it before , because it hadn't occurred to me {disfmarker} it was sort of an obvious thing {disfmarker} is , um , adjusting the , uh , sca the scaling and , uh , insertion penalty sorta stuff .
Turn 12, F (PhD): I did play with that , actually , a little bit . Um . What happens is , uh , {vocalsound} when you get to the noisy stuff , you start getting lots of insertions .
Turn 13, E (Professor): Right .
Turn 14, F (PhD): And , um , so I 've tried playing around a little bit with , um , the insertion penalties and things like that .
Turn 15, E (Professor): Yeah .
Turn 16, F (PhD): Um . I mean , it {disfmarker} it didn't make a whole lot of difference . Like for the well - matched case , it seemed like it was pretty good . Um . {vocalsound} I could do more playing with that , though . And , uh {disfmarker}
Turn 17, E (Professor): But you were looking at mel cepstrum .
Turn 18, F (PhD): and see . Yes .
Turn 19, E (Professor): Right .
Turn 20, F (PhD): Oh , you 're talking about for th {vocalsound} for our features .
Turn 21, E (Professor): Right . So , I mean , i it it 's not the direction that you were working with that we were saying what 's the {disfmarker} uh , what 's the best you can do with {disfmarker} with mel cepstrum . But , they raised a very valid point ,
Turn 22, F (PhD): Mmm .
Turn 23, E (Professor): which , I guess {disfmarker} So , to first order {disfmarker} I mean , you have other things you were gonna do , but to first order , I would say that the conclusion is that if you , um , do , uh , some monkeying around with , uh , the exact HTK training and @ @ {comment} with , uh , you know , how many states and so forth , that it {disfmarker} it doesn't particularly improve the performance . In other words , that even though it sounds pretty dumb , just applying the same number of states to everything , more or less , no matter what language , isn't so bad . Right ? And I guess you hadn't gotten to all the experiments you wanted to do with number of Gaussians ,
Turn 24, F (PhD): Right .
Turn 25, E (Professor): but , um , let 's just {disfmarker} If we had to {disfmarker} if we had to draw a conclusion on the information we have so far , we 'd say something like that . Right ?
Turn 26, F (PhD): Mm - hmm .
Turn 27, E (Professor): Uh , so the next question to ask , which is I think the one that {disfmarker} that {disfmarker} that Andreas was dre addressing himself to in the lunch meeting , is , um , we 're not supposed to adjust the back - end , but anybody using the system would .
Turn 28, F (PhD): Yeah .
Turn 29, E (Professor): So , if you were just adjusting the back - end , how much better would you do , uh , in noise ? Uh , because the language scaling and insertion penalties and so forth are probably set to be about right for mel cepstrum .
Turn 30, F (PhD): Mm - hmm .
Turn 31, E (Professor): But , um , they 're probably not at all set right for these things , particularly these things that look over , uh , larger time windows , in one way or another with {disfmarker} with LDA and KLT and neural nets and {vocalsound} all these things . In the fa past we 've always found that we had to increase the insertion penalty to {disfmarker} to correspond to such things . So , I think that 's , uh , @ @ {comment} that 's kind of a first - order thing that {disfmarker} that we should try .
Turn 32, F (PhD): So for th so the experiment is to , um , run our front - end like normal , with the default , uh , insertion penalties and so forth , and then tweak that a little bit and see how much of a difference it makes
Turn 33, E (Professor): So by " our front - end " I mean take , you know , the Aurora - two s take some version that Stephane has that is , you know , our current best version of something .
Turn 34, F (PhD): if we were {disfmarker} Mm - hmm .
Turn 35, E (Professor): Um . I mean , y don't wanna do this over a hundred different things that they 've tried but , you know , for some version that you say is a good one . You know ? Um . How {disfmarker} how much , uh , does it improve if you actually adjust that ?
Turn 36, F (PhD): OK .
Turn 37, E (Professor): But it is interesting . You say you {disfmarker} you have for the noisy {disfmarker} How about for the {disfmarker} for the mismatched or {disfmarker} or {disfmarker} or {disfmarker} or the {disfmarker} or the medium mismatched conditions ? Have you {disfmarker} ? When you adjusted those numbers for mel cepstrum , did it {disfmarker} ?
Turn 38, F (PhD): Uh , I {disfmarker} I don't remember off the top of my head . Um . Yeah . I didn't even write them down . I {disfmarker} I {disfmarker} I don't remember . I would need to {disfmarker} Well , I did write down , um {disfmarker} So , when I was doing {disfmarker} I just wrote down some numbers for the well - matched case .
Turn 39, E (Professor): Yeah .
Turn 40, F (PhD): Um . Looking at the {disfmarker} I wrote down what the deletions , substitutions , and insertions were , uh , for different numbers of states per phone .
Turn 41, E (Professor): Yeah .
Turn 42, F (PhD): Um , but , uh , that {disfmarker} that 's all I wrote down .
Turn 43, E (Professor): OK .
Turn 44, F (PhD): So . I {disfmarker} I would {disfmarker} Yeah . I would need to do that .
Turn 45, E (Professor): OK . So {disfmarker}
Turn 46, F (PhD): I can do that for next week .
Turn 47, E (Professor): Yeah . And , um {disfmarker} Yeah . Also , eh , eh , sometimes if you run behind on some of these things , maybe we can get someone else to do it and you can supervise or something . But {disfmarker} but I think it would be {disfmarker} it 'd be good to know that .
Turn 48, F (PhD): OK . I just need to get , um , {vocalsound} front - end , uh , stuff from you
Turn 49, B (PhD): Hmm .
Turn 50, F (PhD): or you point me to some files {pause} that you 've already calculated .
Turn 51, B (PhD): Yeah . Alright .
Turn 52, E (Professor): OK . Uh .
Turn 53, F (PhD): I probably will have time to do that and time to play a little bit with the silence model .
Turn 54, E (Professor): Mm - hmm .
Turn 55, F (PhD): So maybe I can have that for next week when Hynek 's here .
Turn 56, E (Professor): Yeah .
Turn 57, B (PhD): Mm - hmm .
Turn 58, E (Professor): Yeah . Cuz , I mean , the {disfmarker} the other {disfmarker} That , in fact , might have been part of what , uh , the difference was {disfmarker} at least part of it that {disfmarker} that we were seeing . Remember we were seeing the SRI system was so much better than the tandem system .
Turn 59, F (PhD): Hmm .
Turn 60, E (Professor): Part of it might just be that the SRI system , they {disfmarker} they {disfmarker} they always adjust these things to be sort of optimized ,
Turn 61, F (PhD): Is there {disfmarker} ?
Turn 62, E (Professor): and {disfmarker}
Turn 63, F (PhD): I wonder if there 's anything that we could do {vocalsound} to the front - end that would affect the insertion {disfmarker}
Turn 64, E (Professor): Yes . I think you can .
Turn 65, F (PhD): What could you do ?
Turn 66, E (Professor): Well , um {disfmarker} uh , part of what 's going on , um , is the , uh , the range of values . So , if you have something that has a much smaller range or a much larger range , and taking the appropriate root .
Turn 67, F (PhD): Oh . Mm - hmm .
Turn 68, E (Professor): You know ? If something is kind of like the equivalent of a bunch of probabilities multiplied together , you can take a root of some sort . If it 's like seven probabilities together , you can take the seventh root of it or something , or if it 's in the log domain , divide it by seven .
Turn 69, F (PhD): Mm - hmm .
Turn 70, E (Professor): But {disfmarker} but , um , that has a similar effect because it changes the scale of the numbers {disfmarker} of the differences between different candidates from the acoustic model
Turn 71, F (PhD): Oh , right .
Turn 72, E (Professor): as opposed to what 's coming from the language model .
Turn 73, F (PhD): So that w Right . So , in effect , that 's changing the value of your insertion penalty .
Turn 74, E (Professor): Yeah . I mean , it 's more directly like the {disfmarker} the language scaling or the , uh {disfmarker} the model scaling or acoustic scaling ,
Turn 75, F (PhD): That 's interesting .
Turn 76, E (Professor): but you know that those things have kind of a similar effect to the insertion penalty
Turn 77, F (PhD): Mm - hmm .
Turn 78, E (Professor): anyway . They 're a slightly different way of {disfmarker} of handling it .
Turn 79, F (PhD): Right .
Turn 80, E (Professor): So , um {disfmarker}
Turn 81, F (PhD): So if we know what the insertion penalty is , then we can get an idea about what range our number should be in ,
Turn 82, E (Professor): I think so .
Turn 83, F (PhD): so that they {pause} match with that .
Turn 84, E (Professor): Yeah . Yeah . So that 's why I think that 's another reason other than curiosity as to why i it would in fact be kinda neat to find out if we 're way off . I mean , the other thing is , are aren't we seeing {disfmarker} ? Y y
Turn 85, F (PhD): Mm - hmm .
Turn 86, E (Professor): I 'm sure you 've already looked at this bu in these noisy cases , are {disfmarker} ? We are seeing lots of insertions . Right ? The insertion number is quite high ?
Turn 87, B (PhD): Yeah .
Turn 88, E (Professor): I know the VAD takes pre care of part of that ,
Turn 89, F (PhD): Yeah .
Turn 90, B (PhD): Yeah .
Turn 91, E (Professor): but {disfmarker}
Turn 92, F (PhD): I 've seen that with the mel cepstrum . I don't {disfmarker} I don't know about {pause} the Aurora front - end , but {disfmarker}
Turn 93, B (PhD): I think it 's much more balanced with , uh {disfmarker} when the front - end is more robust . Yeah . I could look at it {disfmarker} at this . Yeah . Mm - hmm .
Turn 94, E (Professor): Yeah . Wha - what 's a typical number ?
Turn 95, B (PhD): I don't {disfmarker} I don't know .
Turn 96, E (Professor): Do we {disfmarker} ? Oh , you {disfmarker} oh , you don't know .
Turn 97, B (PhD): I don't have this in {disfmarker}
Turn 98, E (Professor): OK . I 'm sure it 's more balanced ,
Turn 99, B (PhD): Mm - hmm .
Turn 100, E (Professor): but it {disfmarker} it {disfmarker} it wouldn't surprise me if there 's still {disfmarker}
Turn 101, B (PhD): Mm - hmm .
Turn 102, E (Professor): I mean , in {disfmarker} in the {disfmarker} the {disfmarker} the old systems we used to do , I {disfmarker} I {disfmarker} uh , I remember numbers kind of like insertions being half the number of deletions , as being {disfmarker} and both numbers being {disfmarker} tend to be on the small side comparing to {disfmarker} to , uh , substitutions .
Turn 103, B (PhD): Mm - hmm .
Turn 104, F (PhD): Well , this {disfmarker} the whole problem with insertions was what I think , um , we talked about when the guy from OGI came down {pause} that one time and {disfmarker} and that was when people were saying , well we should have a , uh , uh , voice activity detector {disfmarker}
Turn 105, E (Professor): Right .
Turn 106, F (PhD): that , because all that stuff {comment} that we 're getting thr the silence that 's getting through is causing insertions . So .
Turn 107, B (PhD): Mmm .
Turn 108, E (Professor): Right .
Turn 109, F (PhD): I 'll bet you there 's still a lot {vocalsound} of insertions .
Turn 110, B (PhD): Mm - hmm .
Turn 111, E (Professor): Yeah . And it may be less of a critical thing . I mean , the fact that some get by may be less of a critical thing if you , uh , get things in the right range .
Turn 112, F (PhD): Mm - hmm .
Turn 113, E (Professor): So , I mean , the insertions is {disfmarker} is a symptom . It 's a symptom that there 's something , uh , wrong with the range .
Turn 114, F (PhD): Right .
Turn 115, E (Professor): But there 's {disfmarker} uh , your {disfmarker} your {disfmarker} your substitutions tend to go up as well . So , uh , I {disfmarker} I {disfmarker} I think that ,
Turn 116, F (PhD): Mm - hmm .
Turn 117, E (Professor): uh , the most obvious thing is just the insertions , @ @ . But {disfmarker} Uh {disfmarker} um . If you 're operating in the wrong range {disfmarker} I mean , that 's why just in general , if you {vocalsound} change what these {disfmarker} these penalties and scaling factors are , you reach some point that 's a {disfmarker} that 's a minimum . So . Um . Um . We do have to do well over a range of different conditions , some of which are noisier than others . Um . But , um , I think we may get a better handle on that if we {disfmarker} if we see {disfmarker} Um , I mean we ca it 's if we actually could pick a {disfmarker} a {disfmarker} a more stable value for the range of these features , it , um , uh , could {disfmarker} Uh {disfmarker} Even though it 's {disfmarker} it 's {disfmarker} it 's true that in a real situation you can in fact adjust the {disfmarker} these {disfmarker} these scaling factors in the back - end , and it 's ar artificial here that we 're not adjusting those , you certainly don't wanna be adjusting those all the time . And if you have a nice front - end that 's in roughly the right range {disfmarker}
Turn 118, F (PhD): Hmm .
Turn 119, E (Professor): I remember after we got our stuff more or less together in the previous systems we built , that we tended to set those scaling factors at kind of a standard level , and we would rarely adjust them again , even though you could get a {disfmarker}
Turn 120, F (PhD): Mm - hmm .
Turn 121, E (Professor): for an evaluation you can get an extra point or something if you tweaked it a little bit . But , once we knew what rou roughly the right operating range was , it was pretty stable , and {disfmarker} Uh , we might just not even be in the right operating range .
Turn 122, F (PhD): So , would the {disfmarker} ? Uh , would a good idea be to try to map it into the same range that you get in the well - matched case ? So , if we computed what the range was in well - matched , and then when we get our noisy conditions out we try to make it have the same range as {disfmarker} ?
Turn 123, E (Professor): No . You don't wanna change it for different conditions . No . No . I {disfmarker} I {disfmarker} I {disfmarker} What {disfmarker} what I 'm saying {disfmarker}
Turn 124, F (PhD): Oh , I wasn't suggesting change it for different conditions . I was just saying that when we pick a range , we {disfmarker} we wanna pick a range that we map our numbers into {disfmarker}
Turn 125, E (Professor): Yeah .
Turn 126, F (PhD): we should probably pick it based on the range that we get in the well - matched case . Otherwise , I mean , what range are we gonna choose to {disfmarker} to map everything into ?
Turn 127, E (Professor): Well . It depends how much we wanna do gamesmanship and how much we wanna do {disfmarker} I mean , i if he it {disfmarker} to me , actually , even if you wanna be {disfmarker} play on the gamesmanship side , it can be kinda tricky . So , I mean , what you would do is set the {disfmarker} set the scaling factors , uh , so that you got the best number for this point four five times the {disfmarker} {vocalsound} you know , and so on .
Turn 128, F (PhD): Mm - hmm .
Turn 129, E (Professor): But they might change that {disfmarker} those weightings .
Turn 130, F (PhD): Yeah .
Turn 131, E (Professor): Um . So {disfmarker} Uh {disfmarker} I just sorta think we need to explore the space . Just take a look at it a little bit .
Turn 132, F (PhD): Mm - hmm .
Turn 133, E (Professor): And we {disfmarker} we {disfmarker} we may just find that {disfmarker} that we 're way off .
Turn 134, F (PhD): OK . Mm - hmm .
Turn 135, E (Professor): Maybe we 're not . You know ? As for these other things , it may turn out that , uh , {vocalsound} it 's kind of reasonable . But then {disfmarker} I mean , Andreas gave a very reasonable response , and he 's probably not gonna be the only one who 's gonna say this in the future {disfmarker} of , you know , people {disfmarker} people within this tight - knit community who are doing this evaluation {vocalsound} are accepting , uh , more or less , that these are the rules . But , people outside of it who look in at the broader picture are certainly gonna say " Well , wait a minute . You 're doing all this standing on your head , uh , on the front - end ,
Turn 136, F (PhD): Yeah .
Turn 137, E (Professor): when all you could do is just adjust this in the back - end with one s one knob . "
Turn 138, F (PhD): Mm - hmm .
Turn 139, E (Professor): And so we have to at least , I think , determine that that 's not true , which would be OK , or determine that it is true , in which case we want to adjust that and then continue with {disfmarker} with what we 're doing . And as you say {disfmarker} as you point out {disfmarker} finding ways to then compensate for that in the front - end {vocalsound} also then becomes a priority for this particular test ,
Turn 140, F (PhD): Right .
Turn 141, E (Professor): and saying you don't have to do that .
Turn 142, F (PhD): Mm - hmm .
Turn 143, E (Professor): So . OK . So , uh {disfmarker} What 's new with you ?
Turn 144, B (PhD): Uh . So there 's nothing {pause} new . Um .
Turn 145, E (Professor): Uh , what 's old with you that 's developed ?
Turn 146, B (PhD): I 'm sorry ?
Turn 147, E (Professor): You {disfmarker} OK . What 's old with you that has developed over the last week or two ?
Turn 148, B (PhD): Mmm . Well , so we 've been mainly working on the report and {disfmarker} and {disfmarker} Yeah .
Turn 149, F (PhD): Mainly working on what ?
Turn 150, B (PhD): On the report {pause} of the work that was already done .
Turn 151, F (PhD): Oh .
Turn 152, B (PhD): Um . Mm - hmm . That 's all .
Turn 153, F (PhD): How about that {disfmarker} ? Any - anything new on the thing that , uh , you were working on with the , uh {disfmarker} ?
Turn 154, C (PhD): I don't have results yet .
Turn 155, F (PhD): No results ? Yeah .
Turn 156, E (Professor): What was that ?
Turn 157, F (PhD): The {disfmarker} the , uh ,
Turn 158, A (Grad): Voicing thing .
Turn 159, F (PhD): voicing detector .
Turn 160, E (Professor): I mean , what what 's {disfmarker} what 's going on now ? What are you {pause} doing ?
Turn 161, C (PhD): Uh , to try to found , nnn , robust feature for detect between voice and unvoice . And we {disfmarker} w we try to use {vocalsound} the variance {vocalsound} of the es difference between the FFT spectrum and mel filter bank spectrum .
Turn 162, E (Professor): Yeah .
Turn 163, C (PhD): Uh , also the {disfmarker} another parameter is {disfmarker} relates with the auto - correlation function .
Turn 164, E (Professor): Uh - huh .
Turn 165, C (PhD): R - ze energy and the variance a also of the auto - correlation function .
Turn 166, E (Professor): Uh - huh . So , that 's {disfmarker} Yeah . That 's what you were describing , I guess , a week or two ago .
Turn 167, C (PhD): Yeah . But we don't have res we don't have result of the AURO for Aurora yet .
Turn 168, E (Professor): So .
Turn 169, C (PhD): We need to train the neural network
Turn 170, E (Professor): Mm - hmm .
Turn 171, C (PhD): and {disfmarker}
Turn 172, E (Professor): So you 're training neural networks now ?
Turn 173, C (PhD): No , not yet .
Turn 174, E (Professor): So , what {disfmarker} wha {vocalsound} wh wha what what 's going on ?
Turn 175, C (PhD): Well , we work in the report , too , because we have a lot of result ,
Turn 176, E (Professor): Uh - huh .
Turn 177, C (PhD): they are very dispersed , and was necessary to {disfmarker} to look in all the directory to {disfmarker} to {disfmarker} to give some more structure .
Turn 178, B (PhD): Yea
Turn 179, E (Professor): So . B So {disfmarker} Yeah . I if I can summarize , basically what 's going on is that you 're going over a lot of material that you have generated in furious fashion , f generating many results and doing many experiments and trying to pull it together into some coherent form to be able to see wha see what happens .
Turn 180, C (PhD): Hm - hmm .
Turn 181, B (PhD): Uh , y yeah . Basically we we 've stopped , uh , experimenting ,
Turn 182, E (Professor): Yes ?
Turn 183, B (PhD): I mean . We 're just writing some kind of technical report . And {disfmarker}
Turn 184, F (PhD): Is this a report that 's for Aurora ? Or is it just like a tech report for ICSI ,
Turn 185, C (PhD): No .
Turn 186, B (PhD): Yeah .
Turn 187, C (PhD): For ICSI .
Turn 188, F (PhD): or {disfmarker} ? Ah . I see .
Turn 189, B (PhD): Yeah .
Turn 190, C (PhD): Just summary of the experiment and the conclusion and something like that .
Turn 191, E (Professor): Yeah .
Turn 192, B (PhD): Mm - hmm .
Turn 193, E (Professor): OK . So , my suggestion , though , is that you {disfmarker} you not necessarily finish that . But that you put it all together so that it 's {disfmarker} you 've got {disfmarker} you 've got a clearer structure to it . You know what things are , you have things documented , you 've looked things up that you needed to look up .
Turn 194, B (PhD): Mm - hmm .
Turn 195, E (Professor): So that , you know {disfmarker} so that such a thing can be written . And , um {disfmarker} When {disfmarker} when {disfmarker} when do you leave again ?
Turn 196, C (PhD): Uh , in July . First of July .
Turn 197, E (Professor): First of July ? OK . And that you figure on actually finishing it in {disfmarker} in June . Because , you know , you 're gonna have another bunch of results to fit in there anyway .
Turn 198, B (PhD): Mm - hmm .
Turn 199, C (PhD): Mm - hmm .
Turn 200, E (Professor): And right now it 's kind of important that we actually go forward with experiments .
Turn 201, C (PhD): It 's not .
Turn 202, E (Professor): So {disfmarker} so , I {disfmarker} I think it 's good to pause , and to gather everything together and make sure it 's in good shape , so that other people can get access to it and so that it can go into a report in June . But I think {vocalsound} to {disfmarker} to really work on {disfmarker} on fine - tuning the report n at this point is {disfmarker} is probably bad timing , I {disfmarker} I {pause} think .
Turn 203, B (PhD): Mm - hmm . Yeah . Well , we didn't {disfmarker} we just planned to work on it one week on this report , not {disfmarker} no more , anyway . Um .
Turn 204, E (Professor): But you ma you may really wanna add other things later anyway
Turn 205, B (PhD): Yeah . Mm - hmm .
Turn 206, E (Professor): because you {disfmarker}
Turn 207, B (PhD): Mmm .
Turn 208, E (Professor): There 's more to go ?
Turn 209, B (PhD): Yeah . Well , so I don't know . There are small things that we started to {disfmarker} to do . But {disfmarker}
Turn 210, F (PhD): Are you discovering anything , uh , that makes you scratch your head as you write this report , like why did we do that , or why didn't we do this ,
Turn 211, B (PhD): Uh .
Turn 212, F (PhD): or {disfmarker} ?
Turn 213, B (PhD): Yeah . Yeah . And {disfmarker} Actually , there were some tables that were also with partial results . We just noticed that , wh while gathering the result that for some conditions we didn't have everything .
Turn 214, F (PhD): Mmm .
Turn 215, B (PhD): But anyway . Um . Yeah , yeah . We have , yeah , extracted actually the noises from {pause} the SpeechDat - Car . And so , we can train neural network with speech and these noises . Um . It 's difficult to say what it will give , because when we look at the Aurora {disfmarker} the TI - digits experiments , um , they have these three conditions that have different noises , and apparently this system perform as well on the seen noises {disfmarker} on the unseen noises and on the seen noises . But , I think this is something we have to try anyway . So {disfmarker} adding the noises from {disfmarker} from the SpeechDat - Car . Um .
Turn 216, E (Professor): That 's {disfmarker} that 's , uh {disfmarker} that 's permitted ?
Turn 217, B (PhD): Uh . Well , OGI does {disfmarker} did that . Um . At some point they did that for {disfmarker} for the voice activity detector .
Turn 218, C (PhD): Uh , for a v VAD .
Turn 219, B (PhD): Right ? Um .
Turn 220, F (PhD): Could you say it again ? What {disfmarker} what exactly did they do ?
Turn 221, B (PhD): They used some parts of the , um , Italian database to train the voice activity detector , I think . It {disfmarker}
Turn 222, E (Professor): Yeah . I guess the thing is {disfmarker} Yeah . I guess that 's a matter of interpretation . The rules as I understand it , is that in principle the Italian and the Spanish and the English {disfmarker} no , Italian and the Finnish and the English ? {disfmarker} were development data
Turn 223, B (PhD): Yeah . And Spanish , yeah .
Turn 224, E (Professor): on which you could adjust things . And the {disfmarker} and the German and Danish were the evaluation data .
Turn 225, B (PhD): Mm - hmm .
Turn 226, E (Professor): And then when they finally actually evaluated things they used everything .
Turn 227, B (PhD): Yeah . That 's right . Uh {disfmarker}
Turn 228, E (Professor): So {disfmarker} Uh , and it is true that the performance , uh , on the German was {disfmarker} I mean , even though the improvement wasn't so good , the pre the raw performance was really pretty good .
Turn 229, B (PhD): Mm - hmm .
Turn 230, E (Professor): So {disfmarker} And , uh , it {disfmarker} it doesn't appear that there 's strong evidence that even though things were somewhat tuned on those three or four languages , that {disfmarker} that going to a different language really hurt you . And the noises were not exactly the same . Right ? Because it was taken from a different , uh {disfmarker} I mean they were different drives .
Turn 231, B (PhD): Different cars . Yeah .
Turn 232, E (Professor): I mean , it was {disfmarker} it was actual different cars and so on .
Turn 233, B (PhD): Yeah .
Turn 234, E (Professor): So . Um , it 's somewhat tuned . It 's tuned more than , you know , a {disfmarker} a {disfmarker} a {disfmarker} a {disfmarker}
Turn 235, B (PhD): Mm - hmm .
Turn 236, E (Professor): You 'd really like to have something that needed no particular noise at all , maybe just some white noise or something like that a at most .
Turn 237, B (PhD): Mm - hmm .
Turn 238, E (Professor): But that 's not really what this contest is . So . Um , I guess it 's OK .
Turn 239, B (PhD): Mm - hmm .
Turn 240, E (Professor): That 's something I 'd like to understand before we actually use something from it ,
Turn 241, F (PhD): I think it 's {disfmarker}
Turn 242, E (Professor): because it would {disfmarker}
Turn 243, F (PhD): it 's probably something that , mmm , the {disfmarker} you know , the , uh , experiment designers didn't really think about , because I think most people aren't doing trained systems , or , you know , uh , systems that are like ours , where you actually use the data to build models . I mean , they just {pause} doing signal - processing .
Turn 244, B (PhD): Yeah .
Turn 245, E (Professor): Well , it 's true ,
Turn 246, F (PhD): So .
Turn 247, E (Professor): except that , uh , that 's what we used in Aurora one , and then they designed the things for Aurora - two knowing that we were doing that .
Turn 248, F (PhD): Yeah . That 's true .
Turn 249, E (Professor): Um .
Turn 250, F (PhD): And they didn't forbid us {disfmarker} right ? {disfmarker} to build models on the data ?
Turn 251, E (Professor): No . But , I think {disfmarker} I think that it {disfmarker} it {disfmarker} it probably would be the case that if , say , we trained on Italian , uh , data and then , uh , we tested on Danish data and it did terribly , uh , that {disfmarker} that it would look bad . And I think someone would notice and would say " Well , look . This is not generalizing . " I would hope tha I would hope they would .
Turn 252, F (PhD): Mm - hmm .
Turn 253, E (Professor): Um . But , uh , it 's true . You know , maybe there 's parameters that other people have used {disfmarker} you know , th that they have tuned in some way for other things . So it 's {disfmarker} it 's , uh {disfmarker} We should {disfmarker} we should {disfmarker} Maybe {disfmarker} that 's maybe a topic {disfmarker} Especially if you talk with him when I 'm not here , that 's a topic you should discuss with Hynek
Turn 254, B (PhD): Mm - hmm .
Turn 255, E (Professor): to , you know , double check it 's OK .
Turn 256, F (PhD): Do we know anything about {pause} the speakers for each of the , uh , training utterances ?
Turn 257, B (PhD): What do you mean ? We {disfmarker} we {disfmarker}
Turn 258, F (PhD): Do you have speaker information ?
Turn 259, E (Professor): Social security number
Turn 260, F (PhD): That would be good .
Turn 261, B (PhD): Like , we have {pause} male , female ,
Turn 262, C (PhD): Hmm .
Turn 263, F (PhD): Bank PIN .
Turn 264, B (PhD): at least .
Turn 265, F (PhD): Just male f female ?
Turn 266, B (PhD): Mmm .
Turn 267, E (Professor): What kind of information do you mean ?
Turn 268, F (PhD): Well , I was thinking about things like , you know , gender , uh {disfmarker} you know , gender - specific nets and , uh , vocal tract length normalization .
Turn 269, B (PhD): Mm - hmm .
Turn 270, F (PhD): Things like that . I d I don't {disfmarker} I didn't know what information we have about the speakers that we could try to take advantage of .
Turn 271, B (PhD): Mm - hmm .
Turn 272, E (Professor): Hmm . Uh . Right . I mean , again , i if you had the whole system you were optimizing , that would be easy to see . But if you 're {vocalsound} supposedly just using a fixed back - end and you 're just coming up with a feature vector , w w I 'm not sure {disfmarker} I mean , having the two nets {disfmarker} Suppose you detected that it was male , it was female {disfmarker} you come up with different {disfmarker}
Turn 273, F (PhD): Well , you could put them both in as separate streams or something . Uh .
Turn 274, B (PhD): Mm - hmm .
Turn 275, E (Professor): Maybe .
Turn 276, F (PhD): I don't know . I was just wondering if there was other information we could exploit .
Turn 277, B (PhD): Mm - hmm .
Turn 278, E (Professor): Hmm . Yeah , it 's an interesting thought . Maybe having something along the {disfmarker} I mean , you can't really do vocal tract normalization . But something that had some of that effect
Turn 279, F (PhD): Yeah .
Turn 280, E (Professor): being applied to the data in some way .
Turn 281, F (PhD): Mm - hmm .
Turn 282, E (Professor): Um .
Turn 283, B (PhD): Do you have something simple in mind for {disfmarker} I mean , vocal tract length normalization ?
Turn 284, F (PhD): Uh no . I hadn't {disfmarker} I hadn't thought {disfmarker} it was {disfmarker} thought too much about it , really . It just {disfmarker} something that popped into my head just now . And so I {disfmarker} I {disfmarker} I mean , you could maybe use the ideas {disfmarker} a similar {pause} idea to what they do in vocal tract length normalization . You know , you have some sort of a , uh , general speech model , you know , maybe just a mixture of Gaussians that you evaluate every utterance against , and then you see where each , you know , utterance {disfmarker} like , the likelihood of each utterance . You divide the {disfmarker} the range of the likelihoods up into discrete bins and then each bin 's got some knob {disfmarker} uh , setting .
Turn 285, E (Professor): Yeah . But just listen to yourself . I mean , that uh really doesn't sound like a real - time thing with less than two hundred milliseconds , uh , latency that {disfmarker} and where you 're not adjusting the statistical engine at all .
Turn 286, F (PhD): Yeah . Yeah .
Turn 287, B (PhD): Mm - hmm .
Turn 288, F (PhD): Yeah . That 's true .
Turn 289, E (Professor): You know , that just {disfmarker}
Turn 290, F (PhD): Right .
Turn 291, B (PhD): Hmm .
Turn 292, E (Professor): I mean {disfmarker} Yeah .
Turn 293, F (PhD): Could be expensive .
Turn 294, E (Professor): No . Well not just expensive . I {disfmarker} I {disfmarker} I don't see how you could possibly do it . You can't look at the whole utterance and do anything . You know , you can only {disfmarker} Right ?
Turn 295, F (PhD): Oh ,
Turn 296, E (Professor): Each frame comes in and it 's gotta go out the other end .
Turn 297, F (PhD): right .
Turn 298, E (Professor): So , uh {disfmarker}
Turn 299, F (PhD): Right . So whatever it was , it would have to be uh sort of on a per frame basis .
Turn 300, E (Professor): Yeah .
Turn 301, B (PhD): Mm - hmm .
Turn 302, E (Professor): Yeah . I mean , you can do , um {disfmarker} Fairly quickly you can do male female {disfmarker} f male female stuff .
Turn 303, F (PhD): Yeah . Yeah .
Turn 304, E (Professor): But as far as , I mean {disfmarker} Like I thought BBN did a thing with , uh , uh , vocal tract normalization a ways back . Maybe other people did too . With {disfmarker} with , uh , uh , l trying to identify third formant {disfmarker} average third formant {disfmarker} {vocalsound} using that as an indicator of {disfmarker}
Turn 305, F (PhD): I don't know .
Turn 306, E (Professor): So . You know , third formant {disfmarker} I if you imagine that to first order what happens with , uh , changing vocal tract is that , uh , the formants get moved out by some proportion {disfmarker}
Turn 307, F (PhD): Mm - hmm .
Turn 308, E (Professor): So , if you had a first formant that was one hundred hertz before , if the fifty {disfmarker} if the vocal tract is fifty percent shorter , then it would be out at seven fifty hertz , and so on . So , that 's a move of two hundred fifty hertz . Whereas the third formant which might have started off at twenty - five hundred hertz , you know , might be out to thirty - seven fifty , you know so it 's at {disfmarker} So , although , you frequently get less distinct higher formants , it 's still {disfmarker} third formant 's kind of a reasonable compromise , and {disfmarker}
Turn 309, F (PhD): Mm - hmm .
Turn 310, E (Professor): So , I think , eh , if I recall correctly , they did something like that . And {disfmarker} and {disfmarker}
Turn 311, F (PhD): Hmm .
Turn 312, E (Professor): But {disfmarker} Um , that doesn't work for just having one frame or something .
Turn 313, F (PhD): Yeah .
Turn 314, B (PhD): Mm - hmm .
Turn 315, E (Professor): You know ? That 's more like looking at third formant over {disfmarker} over a turn or something like that ,
Turn 316, B (PhD): Mm - hmm .
Turn 317, E (Professor): and {disfmarker}
Turn 318, F (PhD): Right .
Turn 319, E (Professor): Um . So . But on the other hand , male female is a {disfmarker} is a {disfmarker} is a much simpler categorization than figuring out a {disfmarker} a factor to , uh , squish or expand the {disfmarker} the spectrum .
Turn 320, F (PhD): Mm - hmm .
Turn 321, E (Professor): So , um . Y you could imagine that {disfmarker} I mean , just like we 're saying voiced - unvoiced is good to know {disfmarker} uh , male female is good to know also . Um .
Turn 322, F (PhD): Mm - hmm .
Turn 323, E (Professor): But , you 'd have to figure out a way to {disfmarker} to {disfmarker} to , uh , incorporate it on the fly . Uh , I mean , I guess , as you say , one thing you could do is simply , uh , have the {disfmarker} the male and female output vectors {disfmarker} you know , tr nets trained only on males and n trained only on females or {disfmarker} or , uh , you know . But {disfmarker} Um . I don't know if that would really help , because you already have males and females and it 's mm - hmm putting into one net . So is it {disfmarker} ?
Turn 324, F (PhD): Is it balanced , um , in terms of gender {disfmarker} the data ?
Turn 325, B (PhD): Mmm .
Turn 326, E (Professor): Do you know ?
Turn 327, B (PhD): Almost , yeah .
Turn 328, F (PhD): Hmm .
Turn 329, B (PhD): Mm - hmm .
Turn 330, E (Professor): Hmm . OK . Y you 're {disfmarker} you were saying before {disfmarker} ?
Turn 331, B (PhD): Uh . Yeah . So , this noise , um {disfmarker} Yeah . The MSG {disfmarker} Um . Mmm . There is something {disfmarker} perhaps , I could spend some days to look at this thing , cuz it seems that when we train networks on {disfmarker} let 's say , on TIMIT with MSG features , they {disfmarker} they look as good as networks trained on PLP . But , um , when they are used on {disfmarker} on the SpeechDat - Car data , it 's not the case {disfmarker} oh , well . The MSG features are much worse , and so maybe they 're , um , less {disfmarker} more sensitive to different recording conditions , or {disfmarker} Shou
Turn 332, E (Professor): Shouldn't be . They should be less so .
Turn 333, B (PhD): Yeah . But {disfmarker}
Turn 334, E (Professor): R right ?
Turn 335, B (PhD): Mmm .
Turn 336, E (Professor): Wh - ? But let me ask you this . What {disfmarker} what 's the , um {disfmarker} ? Do you kno recall if the insertions were {disfmarker} were higher with MSG ?
Turn 337, B (PhD): I don't know . I cannot tell . But {disfmarker} It 's {disfmarker} it {disfmarker} the {disfmarker} the error rate is higher . So , I don
Turn 338, E (Professor): Yeah . But you should always look at insertions , deletions , and substitutions .
Turn 339, B (PhD): Yeah . Mm - hmm .
Turn 340, E (Professor): So {disfmarker}
Turn 341, B (PhD): Mm - hmm .
Turn 342, E (Professor): so , uh {disfmarker} MSG is very , very dif Eh , PLP is very much like mel cepstrum . MSG is very different from both of them .
Turn 343, B (PhD): Mm - hmm .
Turn 344, E (Professor): So , if it 's very different , then this is the sort of thing {disfmarker} I mean I 'm really glad Andreas brought this point up . I {pause} sort of had forgotten to discuss it . Um . You always have to look at how this {disfmarker} uh , these adjustments , uh , affect things . And even though we 're not allowed to do that , again we maybe could reflect that back to our use of the features .
Turn 345, B (PhD): Mm - hmm .
Turn 346, E (Professor): So if it {disfmarker} if in fact , uh {disfmarker} The problem might be that the range of the MSG features is quite different than the range of the PLP or mel cepstrum .
Turn 347, B (PhD): Mm - hmm . Mm - hmm .
Turn 348, E (Professor): And you might wanna change that .
Turn 349, B (PhD): But {disfmarker} Yeah . But , it 's d it 's after {disfmarker} Well , it 's tandem features , so {disfmarker} Mmm .
Turn 350, E (Professor): Yeah .
Turn 351, B (PhD): Yeah . We {disfmarker} we have estimation of post posteriors with PLP and with MSG as input ,
Turn 352, E (Professor): Yeah .
Turn 353, B (PhD): so I don Well . I don't know .
Turn 354, E (Professor): That means they 're between zero and one .
Turn 355, B (PhD): Mm - hmm .
Turn 356, E (Professor): But i it {disfmarker} it {disfmarker} it {disfmarker} it doesn't necessarily {disfmarker} You know , they could be , um {disfmarker} Do - doesn't tell you what the variance of the things is .
Turn 357, B (PhD): Mmm . Mm - hmm .
Turn 358, E (Professor): Right ? Cuz if you 're taking the log of these things , it could be , uh {disfmarker} Knowing what the sum of the probabilities are , doesn't tell you what the sum of the logs are .
Turn 359, B (PhD): Mm - hmm . Yeah .
Turn 360, E (Professor): So .
Turn 361, B (PhD): Yeah . So we should look at the likelihood , or {disfmarker} or what ? Or {disfmarker} well , at the log , perhaps , and {disfmarker}
Turn 362, E (Professor): Yeah . Yeah .
Turn 363, B (PhD): Mm - hmm .
Turn 364, E (Professor): Or what {disfmarker} you know , what you 're uh {disfmarker} the thing you 're actually looking at .
Turn 365, B (PhD): Mm - hmm .
Turn 366, E (Professor): So your {disfmarker} your {disfmarker} the values that are {disfmarker} are actually being fed into HTK .
Turn 367, B (PhD): Mm - hmm . But {disfmarker}
Turn 368, E (Professor): What do they look like ?
Turn 369, F (PhD): No And so th the , uh {disfmarker} for the tandem system , the values that come out of the net don't go through the sigmoid . Right ? They 're sort of the pre - nonlinearity values ?
Turn 370, B (PhD): Yes .
Turn 371, E (Professor): Right . So they 're {pause} kinda like log probabilities is what I was saying .
Turn 372, F (PhD): And those {disfmarker} OK . And tho that 's what goes {pause} into {pause} HTK ?
Turn 373, E (Professor): Uh , almost . But then you actually do a KLT on them .
Turn 374, F (PhD): OK .
Turn 375, E (Professor): Um . They aren't normalized after that , are they ?
Turn 376, B (PhD): Mmm . No , they are not {disfmarker} no .
Turn 377, E (Professor): No . OK . So , um . Right . So the question is {disfmarker} Yeah . Whatever they are at that point , um , are they something for which taking a square root or cube root or fourth root or something like that is {disfmarker} is gonna be a good or a bad thing ? So .
Turn 378, B (PhD): Mm - hmm .
Turn 379, E (Professor): Uh , and that 's something that nothing {disfmarker} nothing else after that is gonna {disfmarker} Uh , things are gonna scale it {disfmarker} Uh , you know , subtract things from it , scale it from it , but nothing will have that same effect . Um . So . Um . Anyway , eh {disfmarker}
Turn 380, F (PhD): Yeah . Cuz if {disfmarker} if the log probs that are coming out of the MSG are really big , the standard {pause} insertion penalty is gonna have very little effect
Turn 381, E (Professor): Well , the {disfmarker} Right .
Turn 382, F (PhD): compared to , you know , a smaller set of log probs .
Turn 383, E (Professor): Yeah . No . Again you don't really {pause} look at that . It 's something {disfmarker} that , and then it 's going through this transformation that 's probably pretty close to {disfmarker} It 's , eh , whatever the KLT is doing . But it 's probably pretty close to what a {disfmarker} a {disfmarker} a discrete cosine transformation is doing .
Turn 384, F (PhD): Yeah .
Turn 385, E (Professor): But still it 's {disfmarker} it 's not gonna probably radically change the scale of things . I would think . And , uh {disfmarker} Yeah . It may be entirely off and {disfmarker} and it may be {disfmarker} at the very least it may be quite different for MSG than it is for mel cepstrum or PLP . So that would be {disfmarker} So the first thing I 'd look at without adjusting anything would just be to go back to the experiment and look at the , uh , substitutions , insertions , and deletions . And if the {disfmarker} if the , uh {disfmarker} i if there 's a fairly large effect of the difference , say , uh , uh , the r ratio between insertions and deletions for the two cases then that would be , uh , an indicator that it might {disfmarker} might be in that direction .
Turn 386, B (PhD): Mm - hmm . Mm - hmm . Yeah . But ,
Turn 387, E (Professor): Anything else ?
Turn 388, B (PhD): my {disfmarker} my point was more that it {disfmarker} it works sometimes and {disfmarker} but sometimes it doesn't work .
Turn 389, E (Professor): Yeah .
Turn 390, B (PhD): So .
Turn 391, E (Professor): Well .
Turn 392, B (PhD): And it works on TI - digits and on SpeechDat - Car it doesn't work , and {disfmarker}
Turn 393, E (Professor): Yeah .
Turn 394, B (PhD): Mm - hmm . Yeah . Well .
Turn 395, E (Professor): But , you know , some problems are harder than others ,
Turn 396, B (PhD): Mm - hmm . Yeah .
Turn 397, E (Professor): and {disfmarker} And , uh , sometimes , you know , there 's enough evidence for something to work and then it 's harder , it breaks . You know ,
Turn 398, B (PhD): Mm - hmm .
Turn 399, E (Professor): so it 's {disfmarker} But it {disfmarker} but , um , i it {disfmarker} it could be that when you say it works maybe we could be doing much better , even in TI - digits . Right ?
Turn 400, B (PhD): Yeah . Yeah , sure .
Turn 401, E (Professor): So .
Turn 402, B (PhD): Uh .
Turn 403, E (Professor): Hmm ? Yeah .
Turn 404, B (PhD): Yeah . Well , there is also the spectral subtraction , which , um {disfmarker} I think maybe we should , uh , try to integrate it in {disfmarker} in our system .
Turn 405, E (Professor): Yeah .
Turn 406, B (PhD): Mmm . Mm - hmm .
Turn 407, E (Professor): Right .
Turn 408, B (PhD): But ,
Turn 409, E (Professor): O
Turn 410, B (PhD): I think that would involve to {disfmarker} {vocalsound} to mmm {vocalsound} use a big {disfmarker} a {disfmarker} al already a big bunch of the system of Ericsson . Because he has spectral subtraction , then it 's followed by , {vocalsound} um , other kind of processing that 's {disfmarker} are dependent on the {disfmarker} uh , if it 's speech or noi or silence .
Turn 411, E (Professor): Mm - hmm .
Turn 412, B (PhD): And there is this kind of spectral flattening after {disfmarker} if it 's silence , and {disfmarker} and s I {disfmarker} I think it 's important , um , {vocalsound} to reduce this musical noise and this {disfmarker} this increase of variance during silence portions . So . Well . This was in this would involve to take almost everything from {disfmarker} from the {disfmarker} this proposal and {disfmarker} and then just add some kind of on - line normalization in {disfmarker} in the neural network . Mmm .
Turn 413, E (Professor): OK . Well , this 'll be , I think , something for discussion with Hynek next week .
Turn 414, B (PhD): Yeah . Mm - hmm .
Turn 415, E (Professor): Yeah . OK . Right . So . How are , uh , uh {disfmarker} how are things going with what you 're doing ?
Turn 416, D (Grad): Oh . Well , um , I took a lot of time just getting my taxes out of the way {disfmarker} multi - national taxes . So , I 'm {disfmarker} I 'm starting to write code now for my work but I don't have any results yet . Um , i it would be good for me to talk to Hynek , I think , when he 's here .
Turn 417, E (Professor): Yeah .
Turn 418, D (Grad): Do you know what his schedule will be like ?
Turn 419, E (Professor): Uh , he 'll be around for three days .
Turn 420, D (Grad): OK . So , y
Turn 421, E (Professor): Uh , we 'll have a lot of time .
Turn 422, D (Grad): OK .
Turn 423, E (Professor): So , uh {disfmarker} Um . I 'll , uh {disfmarker} You know , he 's {disfmarker} he 'll {disfmarker} he 'll be talking with everybody in this room So .
Turn 424, F (PhD): But you said you won't {disfmarker} you won't be here next Thursday ?
Turn 425, E (Professor): Not Thursday and Friday . Yeah . Cuz I will be at faculty retreat .
Turn 426, F (PhD): Hmm .
Turn 427, E (Professor): So . I 'll try to {vocalsound} connect with him and people as {disfmarker} as I can on {disfmarker} on Wednesday . But {disfmarker} Um . Oh , how 'd taxes go ? Taxes go OK ?
Turn 428, D (Grad): Mmm . Yeah .
Turn 429, E (Professor): Yeah . Oh , good . Yeah . Yeah . That 's just {disfmarker} that 's {disfmarker} that 's one of the big advantages of not making much money is {vocalsound} the taxes are easier . Yeah .
Turn 430, F (PhD): Unless you 're getting money in two countries .
Turn 431, E (Professor): I think you are . Aren't you ?
Turn 432, F (PhD): They both want their cut .
Turn 433, B (PhD): Hmm .
Turn 434, D (Grad): Hmm . Yeah .
Turn 435, F (PhD): Right ?
Turn 436, E (Professor): Yeah . Yeah . Huh . Canada w Canada wants a cut ?
Turn 437, D (Grad): Mm - hmm .
Turn 438, E (Professor): Have to do {disfmarker} So you {disfmarker} you have to do two returns ?
Turn 439, D (Grad): Mmm . W uh , for two thousand I did . Yeah .
Turn 440, E (Professor): Oh , oh . Yeah . For tw That 's right , ju
Turn 441, F (PhD): But not for this next year ?
Turn 442, E (Professor): Two thousand . Yeah . Probably not this next year , I guess .
Turn 443, D (Grad): Ye
Turn 444, E (Professor): Yeah .
Turn 445, D (Grad): Um .
Turn 446, E (Professor): Yeah .
Turn 447, D (Grad): Uh , I 'll {disfmarker} I 'll still have a bit of Canadian income but it 'll be less complicated because I will not be a {disfmarker} considered a resident of Canada anymore , so I won't have to declare my American income on my Canadian return .
Turn 448, E (Professor): OK . Alright . Uh . Barry , do you wanna {pause} say something about your stuff here ?
Turn 449, A (Grad): Oh , um . Right . I {pause} just , um , continuing looking at , uh , ph uh , phonetic events , and , uh , this Tuesday gonna be , uh , meeting with John Ohala with Chuck to talk some more about these , uh , ph um , phonetic events . Um , came up with , uh , a plan of attack , uh , gonna execute , and um {disfmarker} Yeah . It 's {disfmarker} that 's pretty much it .
Turn 450, E (Professor): Oh , well . No Um , why don't you say something about what it is ?
Turn 451, A (Grad): Oh , you {disfmarker} oh , you want {disfmarker} you want details . Hmm . OK .
Turn 452, E (Professor): Well , we 're all gathered here together . I thought we 'd , you know {disfmarker}
Turn 453, A (Grad): I was hoping I could wave my hands . Um . So , um . So , once wa I {disfmarker} I was thinking getting {disfmarker} getting us a set of acoustic events to {disfmarker} um , to be able to distinguish between , uh , phones and words and stuff . And {vocalsound} um , once we {disfmarker} we would figure out a set of these events that can be , you know , um , hand - labeled or {disfmarker} or derived , uh , from h the hand - labeled phone targets . Um , we could take these events and , um , {vocalsound} do some cheating experiments , um , where we feed , um , these events into {pause} an SRI system , um , eh , and evaluate its performance on a Switchboard task . Uh , yeah .
Turn 454, D (Grad): Hey , Barry ? Can you give an example of an event ?
Turn 455, A (Grad): Yeah . Sure . Um , I {disfmarker} I can give you an example of {pause} twenty - odd events . Um {disfmarker} So , he In this paper , um , it 's talking about phoneme recognition using acoustic events . So , things like frication or , uh , nasality .
Turn 456, E (Professor): Whose paper is it ?
Turn 457, A (Grad): Um , this is a paper by Hubener and Cardson {pause} Benson {disfmarker} Bernds - Berndsen .
Turn 458, E (Professor): Yeah . Huh . From , uh , University of Hamburg and Bielefeld .
Turn 459, A (Grad): Mm - hmm .
Turn 460, E (Professor): OK .
Turn 461, A (Grad): Um .
Turn 462, F (PhD): Yeah . I think the {disfmarker} just to expand a little bit on the idea of acoustic event .
Turn 463, A (Grad): Mm - hmm .
Turn 464, F (PhD): There 's , um {disfmarker} in my mind , anyways , there 's a difference between , um , acoustic features and acoustic events . And I think of acoustic features as being , um , things that linguists talk about , like , um {disfmarker}
Turn 465, E (Professor): So , stuff that 's not based on data .
Turn 466, F (PhD): Stuff that 's not based on data , necessarily .
Turn 467, E (Professor): Yeah . Oh , OK . Yeah . Yeah , OK .
Turn 468, F (PhD): Right . That 's not based on , you know , acoustic data . So they talk about features for phones , like , uh , its height ,
Turn 469, A (Grad): Yeah .
Turn 470, F (PhD): its tenseness , laxness , things like that ,
Turn 471, A (Grad): Mm - hmm .
Turn 472, F (PhD): which may or may not be all that easy to measure in the acoustic signal . Versus an acoustic event , which is just {nonvocalsound} some {nonvocalsound} something in the acoustic signal {nonvocalsound} that is fairly easy to measure . Um . So it 's , um {disfmarker} it 's a little different , in {disfmarker} at least in my mind .
Turn 473, E (Professor): I mean , when we did the SPAM work {disfmarker} I mean , there we had {disfmarker} we had this notion of an , uh , auditory {disfmarker} @ @ {comment} auditory event .
Turn 474, A (Grad): Good . That 's great .
Turn 475, E (Professor): And , uh , um , called them " avents " , uh , uh , uh , with an A at the front .
Turn 476, F (PhD): Mm - hmm .
Turn 477, E (Professor): Uh . And the {disfmarker} the {disfmarker} the idea was something that occurred that is important to a bunch of neurons somewhere . So .
Turn 478, A (Grad): Mm - hmm .
Turn 479, E (Professor): Um . A sudden change or a relatively rapid change in some spectral characteristic will {disfmarker} will do sort of this . I mean , there 's certainly a bunch of {disfmarker} a bunch of places where you know that neurons are gonna fire because something novel has happened . That was {disfmarker} that was the main thing that we were focusing on there . But there 's certainly other things beyond what we talked about there that aren't just sort of rapid changes , but {disfmarker}
Turn 480, F (PhD): It 's kinda like the difference between top - down and bottom - up .
Turn 481, E (Professor): Yeah .
Turn 482, F (PhD): I think of the acoustic {disfmarker} you know , phonetic features as being top - down . You know , you look at the phone and you say this phone is supposed to be {disfmarker} you know , have this feature , this feature , and this feature . Whether tha those features show up in the acoustic signal is sort of irrelevant . Whereas , an acoustic event goes the other way . Here 's the signal . Here 's some event .
Turn 483, A (Grad): Mm - hmm .
Turn 484, F (PhD): What {disfmarker} ? And then that {disfmarker} you know , that may map to this phone sometimes , and sometimes it may not . It just depen maybe depends on the context , things like that .
Turn 485, E (Professor): Mm - hmm .
Turn 486, F (PhD): And so it 's sort of a different way of looking .
Turn 487, E (Professor): Mm - hmm .
Turn 488, A (Grad): Yeah . So . Yeah .
Turn 489, D (Grad): OK .
Turn 490, A (Grad): Mm - hmm . Um {disfmarker} Using these {disfmarker} these events , um , you know , we can {disfmarker} we can perform these {disfmarker} these , uh , cheating experiments . See how {disfmarker} how {disfmarker} how good they are , um , in , um {disfmarker} in terms of phoneme recognition or word recognition . And , um {disfmarker} and then from that point on , I would , uh , s design robust event detectors , um , in a similar , um , wa spirit that Saul has done w uh , with his graphical models , and this {disfmarker} this probabilistic AND - OR model that he uses . Um , eh , try to extend it to , um {disfmarker} to account for other {disfmarker} other phenomena like , um , CMR co - modulation release . And , um {disfmarker} and maybe also investigate ways to {disfmarker} to modify the structure of these models , um , in a data - driven way , uh , similar to the way that , uh , Jeff {disfmarker} Jeff , uh , Bilmes did his work . Um , and while I 'm {disfmarker} I 'm doing these , um , event detectors , you know , I can ma mea measure my progress by comparing , um , the error rates in clean and noisy conditions to something like , uh , neural nets . Um , and {disfmarker} So {disfmarker} so , once we have these {disfmarker} these , uh , event detectors , um , we could put them together and {disfmarker} and feed the outputs of the event detectors into {disfmarker} into the SRI , um , HMM {disfmarker} HMM system , and , um {disfmarker} and test it on {disfmarker} on Switchboard or , um , maybe even Aurora stuff . And , that 's pretty much the {disfmarker} the big picture of {disfmarker} of um , the plan .
Turn 491, E (Professor): By the way , um , there 's , uh , a couple people who are gonna be here {disfmarker} I forget if I already told you this , but , a couple people who are gonna be here for six months .
Turn 492, A (Grad): Mm - hmm .
Turn 493, E (Professor): Uh {disfmarker} uh , there 's a Professor Kollmeier , uh , from Germany who 's , uh , uh , quite big in the , uh , hearing - aid signal - processing area and , um , Michael Kleinschmidt , who 's worked with him , who also looks at {vocalsound} auditory properties inspired by various , uh , brain function things .
Turn 494, A (Grad): Hmm .
Turn 495, E (Professor): So , um , um , I think they 'll be interesting to talk to , in this sort of issue as these detectors are {disfmarker} are , uh , developing .
Turn 496, A (Grad): Hmm . OK .
Turn 497, E (Professor): So , he looks at interesting {disfmarker} interesting things in {disfmarker} in the {disfmarker} {vocalsound} different ways of looking at spectra in order to {disfmarker} to get various speech properties out . So .
Turn 498, A (Grad): OK .
Turn 499, E (Professor): OK . Well , short meeting , but that 's OK . And , uh , we might as well do our digits . And like I say , I {disfmarker} I encourage you to go ahead and meet , uh , next week with , uh , uh , Hynek . Alright , I 'll {disfmarker} I 'll start . It 's , uh , one thirty - five . seventeen OK
