{"metadata": {"meeting_name": "Bro004"}, "turns": [{"turn": 1, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 2, "name": "PhD", "id": "C", "contribution": " Oh , I don't {disfmarker}"}, {"turn": 3, "name": "PhD", "id": "A", "contribution": " I think I 'm zero ."}, {"turn": 4, "name": "Professor", "id": "B", "contribution": " Wow ! Unprecedented ."}, {"turn": 5, "name": "PhD", "id": "C", "contribution": " Hello , hello , hello , hello ."}, {"turn": 6, "name": "PhD", "id": "E", "contribution": " Ah"}, {"turn": 7, "name": "Grad", "id": "F", "contribution": " Wh - what causes the crash ?"}, {"turn": 8, "name": "PhD", "id": "A", "contribution": " Did you fix something ?"}, {"turn": 9, "name": "PhD", "id": "C", "contribution": " Hello ."}, {"turn": 10, "name": "PhD", "id": "E", "contribution": " Five , five ."}, {"turn": 11, "name": "PhD", "id": "C", "contribution": " Hello , hello ."}, {"turn": 12, "name": "Grad", "id": "F", "contribution": " Oh , maybe it 's the turning {disfmarker} turning off and turning on of the mike , right ?"}, {"turn": 13, "name": "Professor", "id": "B", "contribution": " Uh , you think that 's you ? Oh ."}, {"turn": 14, "name": "PhD", "id": "C", "contribution": " Aaa - aaa - aaa ."}, {"turn": 15, "name": "Grad", "id": "F", "contribution": " Yeah , OK , mine 's working ."}, {"turn": 16, "name": "PhD", "id": "C", "contribution": " OK . That 's me ."}, {"turn": 17, "name": "Professor", "id": "B", "contribution": " OK . OK . So , um I guess we are {pause} um {pause} gonna do the digits at the end . Uh"}, {"turn": 18, "name": "PhD", "id": "D", "contribution": " Channel {disfmarker} channel three , yeah ."}, {"turn": 19, "name": "PhD", "id": "C", "contribution": " Channel two ."}, {"turn": 20, "name": "PhD", "id": "D", "contribution": " OK ."}, {"turn": 21, "name": "PhD", "id": "E", "contribution": " Mmm , channel five ? Doesn't work ?"}, {"turn": 22, "name": "Professor", "id": "B", "contribution": " Yeah , that 's the mike number there , uh {pause} Uh , mike number five , and {pause} channel {disfmarker} channel four ."}, {"turn": 23, "name": "PhD", "id": "C", "contribution": " Two ."}, {"turn": 24, "name": "PhD", "id": "A", "contribution": " Is it written on her sheet , I believe ."}, {"turn": 25, "name": "PhD", "id": "E", "contribution": " No ? Ah ,"}, {"turn": 26, "name": "PhD", "id": "D", "contribution": " Mike four ."}, {"turn": 27, "name": "Grad", "id": "F", "contribution": " Watch this ."}, {"turn": 28, "name": "PhD", "id": "E", "contribution": " era el cuatro ."}, {"turn": 29, "name": "Grad", "id": "F", "contribution": " Yep , that 's me ."}, {"turn": 30, "name": "PhD", "id": "E", "contribution": " Yeah ."}, {"turn": 31, "name": "PhD", "id": "A", "contribution": " But , channel"}, {"turn": 32, "name": "PhD", "id": "E", "contribution": " Yeah yeah yeah ."}, {"turn": 33, "name": "Professor", "id": "B", "contribution": " This is you ."}, {"turn": 34, "name": "PhD", "id": "E", "contribution": " OK . I saw that . Ah {disfmarker} yeah , it 's OK ."}, {"turn": 35, "name": "Professor", "id": "B", "contribution": " Yeah . And I 'm channel uh two I think ,"}, {"turn": 36, "name": "PhD", "id": "C", "contribution": " Ooo ."}, {"turn": 37, "name": "Professor", "id": "B", "contribution": " or channel {disfmarker}"}, {"turn": 38, "name": "PhD", "id": "C", "contribution": " I think I 'm channel two ."}, {"turn": 39, "name": "Professor", "id": "B", "contribution": " Oh , I 'm channel {disfmarker} must be channel one . Channel one ?"}, {"turn": 40, "name": "PhD", "id": "E", "contribution": " Channel {disfmarker} {vocalsound} I decided to talk about that ."}, {"turn": 41, "name": "Professor", "id": "B", "contribution": " Yes , OK . OK . So uh {pause} I also copied uh the results that we all got in the mail I think from uh {disfmarker} {pause} from OGI and we 'll go {disfmarker} go through them also . So where are we on {disfmarker} {pause} on uh {vocalsound} {pause} our runs ?"}, {"turn": 42, "name": "PhD", "id": "D", "contribution": " Uh so . {pause} uh {disfmarker} We {disfmarker} So {pause} As I was already said , we {disfmarker} we mainly focused on uh four kind of features ."}, {"turn": 43, "name": "Professor", "id": "B", "contribution": " Excuse me ."}, {"turn": 44, "name": "PhD", "id": "D", "contribution": " The PLP , the PLP with JRASTA , the MSG , and the MFCC from the baseline Aurora ."}, {"turn": 45, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 46, "name": "PhD", "id": "D", "contribution": " Uh , and we focused for the {disfmarker} the test part on the English and the Italian . Um . We 've trained uh several neural networks on {disfmarker} so {disfmarker} on the TI - digits English {pause} and on the Italian data and also on the broad uh {pause} English uh French and uh Spanish databases . Mmm , so there 's our result tables here , for the tandem approach , and um , actually what we {disfmarker} we @ @ observed is that if the network is trained on the task data it works pretty well ."}, {"turn": 47, "name": "Professor", "id": "B", "contribution": " OK . Our {disfmarker} our uh {disfmarker} {pause} There 's a {disfmarker} {pause} We 're pausing for a photo {disfmarker}"}, {"turn": 48, "name": "PhD", "id": "C", "contribution": " Chicken on the grill . Try that corner ."}, {"turn": 49, "name": "PhD", "id": "A", "contribution": " How about over th from the front of the room ?"}, {"turn": 50, "name": "PhD", "id": "C", "contribution": " Yeah , it 's longer ."}, {"turn": 51, "name": "Professor", "id": "B", "contribution": " We 're pausing for a photo opportunity here . Uh . {vocalsound} Uh . So ."}, {"turn": 52, "name": "Grad", "id": "F", "contribution": " Oh wait wait wait wait wait . Wait ."}, {"turn": 53, "name": "PhD", "id": "C", "contribution": " Get out of the {disfmarker} Yeah ."}, {"turn": 54, "name": "Grad", "id": "F", "contribution": " Hold on . Hold on ."}, {"turn": 55, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 56, "name": "Grad", "id": "F", "contribution": " Let me give you a black screen ."}, {"turn": 57, "name": "Professor", "id": "B", "contribution": " He 's facing this way . What ? OK , this {disfmarker} this would be a {pause} good section for our silence detection ."}, {"turn": 58, "name": "Grad", "id": "F", "contribution": " OK ."}, {"turn": 59, "name": "PhD", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 60, "name": "Professor", "id": "B", "contribution": " Um Oh ."}, {"turn": 61, "name": "Grad", "id": "F", "contribution": " Musical chairs everybody !"}, {"turn": 62, "name": "Professor", "id": "B", "contribution": " OK . So um , {pause} you were saying {pause} about the training data {disfmarker} Yeah ."}, {"turn": 63, "name": "PhD", "id": "D", "contribution": " Yeah , so if the network is trained on the task data um {pause} tandem works pretty well . And uh actually we have uh , results are similar Only on ,"}, {"turn": 64, "name": "PhD", "id": "A", "contribution": " Do you mean if it 's trained only on {disfmarker} On data from just that task ,"}, {"turn": 65, "name": "PhD", "id": "D", "contribution": " yeah ."}, {"turn": 66, "name": "PhD", "id": "A", "contribution": " that language ?"}, {"turn": 67, "name": "PhD", "id": "D", "contribution": " Just that task . But actually we didn't train network on {pause} uh both types of data I mean {pause} uh {pause} phonetically ba phonetically balanced uh data and task data ."}, {"turn": 68, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 69, "name": "PhD", "id": "D", "contribution": " We only did either task {disfmarker} task data or {pause} uh broad {pause} data ."}, {"turn": 70, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 71, "name": "PhD", "id": "D", "contribution": " Um {pause} Yeah . So ,"}, {"turn": 72, "name": "Professor", "id": "B", "contribution": " So how {disfmarker} I mean {disfmarker} clearly it 's gonna be good then"}, {"turn": 73, "name": "PhD", "id": "A", "contribution": " So what 's th"}, {"turn": 74, "name": "Professor", "id": "B", "contribution": " but the question is how much {pause} worse is it {pause} if you have broad data ? I mean , {pause} my assump From what I saw from the earlier results , uh I guess last week , {pause} was that um , {pause} if you {pause} trained on one language and tested on another , say , that {pause} the results were {disfmarker} were relatively poor ."}, {"turn": 75, "name": "PhD", "id": "D", "contribution": " Mmm . Yeah ."}, {"turn": 76, "name": "Professor", "id": "B", "contribution": " But {disfmarker} but the question is if you train on one language {pause} but you have a broad coverage {pause} and then test in another , {pause} does that {disfmarker} {pause} is that improve things {pause} i c in comparison ?"}, {"turn": 77, "name": "PhD", "id": "D", "contribution": " If we use the same language ?"}, {"turn": 78, "name": "Professor", "id": "B", "contribution": " No , no , no . Different lang So {pause} um {pause} If you train on TI - digits {pause} and test on Italian digits , {pause} you do poorly , {pause} let 's say ."}, {"turn": 79, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 80, "name": "Professor", "id": "B", "contribution": " I don't have the numbers in front of me ,"}, {"turn": 81, "name": "PhD", "id": "D", "contribution": " But {disfmarker} Yeah but I did not uh do that ."}, {"turn": 82, "name": "Professor", "id": "B", "contribution": " so I 'm just imagining . E So , you didn't train on {pause} TIMIT and test on {disfmarker} {pause} on Italian digits , say ?"}, {"turn": 83, "name": "PhD", "id": "D", "contribution": " We {disfmarker} No , we did four {disfmarker} four kind of {disfmarker} of testing , actually . The first testing is {pause} with task data {disfmarker} So , with nets trained on task data . So for Italian on the Italian speech @ @ . The second test is trained on a single language um with broad database , but the same language as the t task data ."}, {"turn": 84, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 85, "name": "PhD", "id": "D", "contribution": " But for Italian we choose Spanish which {pause} we assume is close to Italian . The third test is by using , um the three language database"}, {"turn": 86, "name": "Professor", "id": "B", "contribution": " W which in {disfmarker}"}, {"turn": 87, "name": "PhD", "id": "D", "contribution": " and the fourth is"}, {"turn": 88, "name": "Professor", "id": "B", "contribution": " It has three languages . That 's including the w the {disfmarker} {pause} the {disfmarker}"}, {"turn": 89, "name": "PhD", "id": "D", "contribution": " This includes {disfmarker}"}, {"turn": 90, "name": "Professor", "id": "B", "contribution": " the one that it 's {disfmarker}"}, {"turn": 91, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 92, "name": "PhD", "id": "A", "contribution": " In"}, {"turn": 93, "name": "PhD", "id": "D", "contribution": " But {pause} not digits . I mean it 's {disfmarker}"}, {"turn": 94, "name": "PhD", "id": "A", "contribution": " The three languages {pause} is not digits ,"}, {"turn": 95, "name": "Professor", "id": "B", "contribution": " Right ."}, {"turn": 96, "name": "PhD", "id": "A", "contribution": " it 's the broad {pause} data . OK ."}, {"turn": 97, "name": "PhD", "id": "D", "contribution": " Yeah And the fourth test is uh {pause} excluding from these three languages the language {pause} that is {pause} the task language ."}, {"turn": 98, "name": "Professor", "id": "B", "contribution": " Oh , OK , yeah , so , that is what I wanted to know ."}, {"turn": 99, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 100, "name": "Professor", "id": "B", "contribution": " I just wasn't saying it very well , I guess ."}, {"turn": 101, "name": "PhD", "id": "D", "contribution": " Uh , yeah . So um {pause} for uh TI - digits for ins example {pause} uh when we go from TI - digits training to {pause} TIMIT training {pause} uh we lose {pause} uh around ten percent , uh . The error rate increase u of {disfmarker} of {disfmarker} of ten percent , relative ."}, {"turn": 102, "name": "Professor", "id": "B", "contribution": " Relative . Right ."}, {"turn": 103, "name": "PhD", "id": "D", "contribution": " So this is not so bad . And then when we jump to the multilingual data it 's uh it become worse and , well Around uh , let 's say , {pause} twenty perc twenty percent further ."}, {"turn": 104, "name": "Professor", "id": "B", "contribution": " Ab - about how much ?"}, {"turn": 105, "name": "PhD", "id": "D", "contribution": " So . Yeah ."}, {"turn": 106, "name": "Professor", "id": "B", "contribution": " Twenty percent further ?"}, {"turn": 107, "name": "PhD", "id": "D", "contribution": " Twenty to {disfmarker} to thirty percent further . Yeah ."}, {"turn": 108, "name": "PhD", "id": "A", "contribution": " And so , remind me , the multilingual stuff is just the broad data . Right ? It 's not the digits ."}, {"turn": 109, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 110, "name": "PhD", "id": "A", "contribution": " So it 's the combination of {pause} two things there . It 's {pause} removing the {pause} task specific {pause} training and {pause} it 's adding other languages ."}, {"turn": 111, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah ."}, {"turn": 112, "name": "PhD", "id": "A", "contribution": " OK ."}, {"turn": 113, "name": "PhD", "id": "D", "contribution": " But the first step is al already removing the task s specific from {disfmarker} from {disfmarker}"}, {"turn": 114, "name": "PhD", "id": "A", "contribution": " Already , right right right ."}, {"turn": 115, "name": "PhD", "id": "D", "contribution": " So ."}, {"turn": 116, "name": "PhD", "id": "A", "contribution": " So they were sort of building {pause} here ?"}, {"turn": 117, "name": "PhD", "id": "D", "contribution": " And we lose {disfmarker}"}, {"turn": 118, "name": "PhD", "id": "A", "contribution": " OK ?"}, {"turn": 119, "name": "PhD", "id": "D", "contribution": " Yeah . Uh {pause} So , basically when it 's trained on the {disfmarker} the multilingual broad data {pause} um or number {disfmarker} so , the {disfmarker} the {pause} ratio of our error rates uh with the {pause} baseline error rate is around {pause} uh one point one ."}, {"turn": 120, "name": "Professor", "id": "B", "contribution": " Yes . {vocalsound} And it 's something like one point three of {disfmarker} of the {pause} uh {disfmarker}"}, {"turn": 121, "name": "PhD", "id": "D", "contribution": " So ."}, {"turn": 122, "name": "Professor", "id": "B", "contribution": " I i if you compare everything to the first case at the baseline , you get something like one point one for the {disfmarker} for the using the same language but a different task , and something like one point three {pause} for three {disfmarker} three languages {pause} broad stuff ."}, {"turn": 123, "name": "PhD", "id": "D", "contribution": " No no no . Uh same language we are at uh {disfmarker} for at English at O point eight . So it improves , {pause} compared to the baseline . But {disfmarker} So . Le - let me ."}, {"turn": 124, "name": "Professor", "id": "B", "contribution": " I {disfmarker} I {disfmarker} I 'm sorry ."}, {"turn": 125, "name": "PhD", "id": "D", "contribution": " Tas - task data"}, {"turn": 126, "name": "Professor", "id": "B", "contribution": " I {disfmarker} I {disfmarker} I meant something different by baseline"}, {"turn": 127, "name": "PhD", "id": "D", "contribution": " we are u Yeah ."}, {"turn": 128, "name": "Professor", "id": "B", "contribution": " So let me {disfmarker} let me {disfmarker} Um , {pause} so , {pause} um {disfmarker}"}, {"turn": 129, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 130, "name": "Professor", "id": "B", "contribution": " OK , fine . Let 's {disfmarker} let 's use the conventional meaning of baseline ."}, {"turn": 131, "name": "PhD", "id": "D", "contribution": " Hmm ."}, {"turn": 132, "name": "Professor", "id": "B", "contribution": " I {disfmarker} I {disfmarker} By baseline here I meant {pause} uh using the task specific data ."}, {"turn": 133, "name": "PhD", "id": "D", "contribution": " Oh yeah , the f Yeah , OK ."}, {"turn": 134, "name": "Professor", "id": "B", "contribution": " But uh {disfmarker} {pause} uh , because that 's what you were just doing with this ten percent ."}, {"turn": 135, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 136, "name": "Professor", "id": "B", "contribution": " So I was just {disfmarker} I just trying to understand that ."}, {"turn": 137, "name": "PhD", "id": "D", "contribution": " Yeah . Sure ."}, {"turn": 138, "name": "Professor", "id": "B", "contribution": " So if we call {pause} a factor of w just one , just normalized to one , the word error rate {pause} that you have {pause} for using TI - digits as {disfmarker} as {pause} training and TI - digits as test ,"}, {"turn": 139, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 140, "name": "Professor", "id": "B", "contribution": " uh different words , I 'm sure ,"}, {"turn": 141, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 142, "name": "Professor", "id": "B", "contribution": " but {disfmarker} {pause} but uh , uh the same {pause} task and so on ."}, {"turn": 143, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 144, "name": "Professor", "id": "B", "contribution": " If we call that \" one \" , {pause} then what you 're saying is {pause} that the word error rate {pause} for the same language but using {pause} uh different training data than you 're testing on , say TIMIT and so forth , {pause} it 's one point one ."}, {"turn": 145, "name": "PhD", "id": "D", "contribution": " Mm - hmm . Yeah , it 's around one point one ."}, {"turn": 146, "name": "Professor", "id": "B", "contribution": " Right . And if it 's {disfmarker}"}, {"turn": 147, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 148, "name": "Professor", "id": "B", "contribution": " you {pause} do {pause} go to {pause} three languages including the English , {pause} it 's something like one point three . That 's what you were just saying , I think ."}, {"turn": 149, "name": "PhD", "id": "D", "contribution": " Ye Uh , more actually ."}, {"turn": 150, "name": "PhD", "id": "A", "contribution": " One point four ?"}, {"turn": 151, "name": "PhD", "id": "D", "contribution": " If I {disfmarker} Yeah ."}, {"turn": 152, "name": "PhD", "id": "A", "contribution": " So , it 's an additional thirty percent ."}, {"turn": 153, "name": "PhD", "id": "D", "contribution": " What would you say ? Around one point four"}, {"turn": 154, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 155, "name": "PhD", "id": "D", "contribution": " yeah ."}, {"turn": 156, "name": "Professor", "id": "B", "contribution": " And if you exclude {pause} English , {pause} from this combination , what 's that ?"}, {"turn": 157, "name": "PhD", "id": "D", "contribution": " If we exclude English , {pause} um {pause} there is {pause} not much difference with the {pause} data with English ."}, {"turn": 158, "name": "Professor", "id": "B", "contribution": " Aha !"}, {"turn": 159, "name": "PhD", "id": "D", "contribution": " So . Yeah ."}, {"turn": 160, "name": "Professor", "id": "B", "contribution": " That 's interesting . {pause} That 's interesting . Do you see ? Because {disfmarker} Uh ,"}, {"turn": 161, "name": "PhD", "id": "D", "contribution": " Uh ."}, {"turn": 162, "name": "Professor", "id": "B", "contribution": " so {disfmarker} No , that {disfmarker} that 's important . So what {disfmarker} what it 's saying here is just that \" yes , there is a reduction {pause} in performance , {pause} when you don't {pause} um {pause} have the s {pause} when you don't have {pause} um"}, {"turn": 163, "name": "PhD", "id": "A", "contribution": " Task data ."}, {"turn": 164, "name": "Professor", "id": "B", "contribution": " Wait a minute , th th the {disfmarker}"}, {"turn": 165, "name": "PhD", "id": "D", "contribution": " Hmm ."}, {"turn": 166, "name": "Professor", "id": "B", "contribution": " No , actually {pause} it 's interesting . So it 's {disfmarker} So when you go to a different task , there 's actually not so {pause} different . It 's when you went to these {disfmarker} So what 's the difference between two and three ? Between the one point one case and the one point four case ? I 'm confused ."}, {"turn": 167, "name": "PhD", "id": "A", "contribution": " It 's multilingual ."}, {"turn": 168, "name": "PhD", "id": "D", "contribution": " Yeah . The only difference it 's {disfmarker} is that it 's multilingual {disfmarker} Um"}, {"turn": 169, "name": "Professor", "id": "B", "contribution": " Cuz in both {disfmarker} in both {disfmarker} both of those cases , you don't have the same task ."}, {"turn": 170, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah sure ."}, {"turn": 171, "name": "Professor", "id": "B", "contribution": " So is {disfmarker} is the training data for the {disfmarker} for this one point four case {disfmarker} does it include the training data for the one point one case ?"}, {"turn": 172, "name": "PhD", "id": "D", "contribution": " Uh yeah ."}, {"turn": 173, "name": "Grad", "id": "F", "contribution": " Yeah , a fraction of it ."}, {"turn": 174, "name": "PhD", "id": "D", "contribution": " A part of it , yeah ."}, {"turn": 175, "name": "Professor", "id": "B", "contribution": " How m how much bigger is it ?"}, {"turn": 176, "name": "PhD", "id": "D", "contribution": " Um {pause} It 's two times ,"}, {"turn": 177, "name": "Grad", "id": "F", "contribution": " Yeah , um ."}, {"turn": 178, "name": "PhD", "id": "D", "contribution": " actually ? Yeah . Um . The English data {disfmarker} {pause} No , the multilingual databases are two times the {pause} broad English {pause} data . We just wanted to keep this , w well , not too huge . So ."}, {"turn": 179, "name": "Professor", "id": "B", "contribution": " So it 's two times , but it includes the {disfmarker} but it includes the broad English data ."}, {"turn": 180, "name": "PhD", "id": "D", "contribution": " I think so . Do you {disfmarker} Uh , Yeah ."}, {"turn": 181, "name": "Professor", "id": "B", "contribution": " And the broad English data is what you got this one point one {pause} with . So that 's TIMIT basically right ?"}, {"turn": 182, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 183, "name": "Grad", "id": "F", "contribution": " Mm - hmm ."}, {"turn": 184, "name": "Professor", "id": "B", "contribution": " So it 's band - limited TIMIT . This is all eight kilohertz sampling ."}, {"turn": 185, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 186, "name": "Grad", "id": "F", "contribution": " Mm - hmm ."}, {"turn": 187, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 188, "name": "Grad", "id": "F", "contribution": " Downs Right ."}, {"turn": 189, "name": "Professor", "id": "B", "contribution": " So you have band - limited TIMIT , {pause} gave you uh almost as good as a result as using TI - digits {pause} on a TI - digits test . OK ?"}, {"turn": 190, "name": "PhD", "id": "D", "contribution": " Hmm ?"}, {"turn": 191, "name": "Professor", "id": "B", "contribution": " Um {pause} and {pause} um But , {pause} when you add in more training data but keep the neural net the same size , {pause} it {pause} um performs worse on the TI - digits . OK , now all of this is {disfmarker} {pause} This is noisy {pause} TI - digits , I assume ? Both training and test ?"}, {"turn": 192, "name": "PhD", "id": "D", "contribution": ""}, {"turn": 193, "name": "Professor", "id": "B", "contribution": " Yeah . OK . Um OK . Well . {pause} We {disfmarker} we {disfmarker} we may just need to uh {disfmarker} So I mean it 's interesting that h going to a different {disfmarker} different task didn't seem to hurt us that much , and going to a different language um It doesn't seem to matter {disfmarker} The difference between three and four is not particularly great , so that means that {pause} whether you have the language in or not is not such a big deal ."}, {"turn": 194, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 195, "name": "Professor", "id": "B", "contribution": " It sounds like um {pause} uh {pause} we may need to have more {pause} of uh things that are similar to a target language or {disfmarker} I mean . {pause} You have the same number of parameters in the neural net , you haven't increased the size of the neural net , and maybe there 's just {disfmarker} {pause} just not enough {pause} complexity to it to represent {pause} the variab increased variability in the {disfmarker} in the training set . That {disfmarker} that could be . Um {pause} So , what about {disfmarker} So these are results with {pause} uh th {pause} that you 're describing now , that {pause} they are pretty similar for the different features or {disfmarker} {pause} or uh {disfmarker}"}, {"turn": 196, "name": "PhD", "id": "D", "contribution": " Uh , let me check . Uh ."}, {"turn": 197, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 198, "name": "PhD", "id": "D", "contribution": " So . This was for the PLP ,"}, {"turn": 199, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 200, "name": "PhD", "id": "D", "contribution": " Um . The {disfmarker} Yeah . For the PLP with JRASTA the {disfmarker} {pause} the {disfmarker} we {disfmarker} This is quite the same {pause} tendency , {pause} with a slight increase of the error rate , {pause} uh if we go to {disfmarker} to TIMIT . And then it 's {disfmarker} it gets worse with the multilingual . Um . Yeah . There {disfmarker} there is a difference actually with {disfmarker} b between PLP and JRASTA is that {pause} JRASTA {pause} seems to {pause} perform better with the highly mismatched {pause} condition {pause} but slightly {disfmarker} slightly worse {pause} for the well matched condition . Mmm ."}, {"turn": 201, "name": "Professor", "id": "B", "contribution": " I have a suggestion , actually , even though it 'll delay us slightly , would {disfmarker} would you mind {pause} running into the other room and making {pause} copies of this ? Cuz we 're all sort of {disfmarker} If we c if we could look at it , while we 're talking , I think it 'd be"}, {"turn": 202, "name": "PhD", "id": "D", "contribution": " Yeah , yeah . OK ."}, {"turn": 203, "name": "Professor", "id": "B", "contribution": " uh {disfmarker} {pause} Uh , I 'll {disfmarker} I 'll sing a song or dance or something while you {vocalsound} do it , too ."}, {"turn": 204, "name": "PhD", "id": "A", "contribution": " So um {disfmarker}"}, {"turn": 205, "name": "Grad", "id": "F", "contribution": " Alright ."}, {"turn": 206, "name": "PhD", "id": "A", "contribution": " Go ahead . Ah , while you 're gone I 'll ask s some of my questions ."}, {"turn": 207, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 208, "name": "PhD", "id": "A", "contribution": " Um ."}, {"turn": 209, "name": "Professor", "id": "B", "contribution": " Yeah . Uh , this way and just slightly to the left , yeah ."}, {"turn": 210, "name": "PhD", "id": "A", "contribution": " The um {disfmarker} What was {disfmarker} Was this number {pause} forty or {disfmarker} It was roughly the same as this one , {pause} he said ? When you had the two language versus the three language ?"}, {"turn": 211, "name": "Professor", "id": "B", "contribution": " Um . That 's what he was saying ."}, {"turn": 212, "name": "PhD", "id": "A", "contribution": " That 's where he removed English ,"}, {"turn": 213, "name": "Grad", "id": "F", "contribution": " Yeah ."}, {"turn": 214, "name": "PhD", "id": "A", "contribution": " right ?"}, {"turn": 215, "name": "Professor", "id": "B", "contribution": " Right ."}, {"turn": 216, "name": "Grad", "id": "F", "contribution": " It sometimes , actually , depends on what features you 're using ."}, {"turn": 217, "name": "Professor", "id": "B", "contribution": " Yeah . But {disfmarker} but i it sounds like {disfmarker}"}, {"turn": 218, "name": "Grad", "id": "F", "contribution": " Um , but {disfmarker} {vocalsound} {vocalsound} He {disfmarker} Mm - hmm ."}, {"turn": 219, "name": "Professor", "id": "B", "contribution": " I mean . That 's interesting because {pause} it {disfmarker} it seems like what it 's saying is not so much that you got hurt {pause} uh because {pause} you {pause} uh didn't have so much representation of English , because in the other case you don't get hurt any more , at least when {pause} it seemed like uh it {disfmarker} it might simply be a case that you have something that is just much more diverse ,"}, {"turn": 220, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 221, "name": "Professor", "id": "B", "contribution": " but you have the same number of parameters representing it ."}, {"turn": 222, "name": "PhD", "id": "A", "contribution": " Mm - hmm . I wonder {disfmarker} were um all three of these nets {pause} using the same output ? This multi - language {pause} uh labelling ?"}, {"turn": 223, "name": "Grad", "id": "F", "contribution": " He was using uh sixty - four phonemes from {pause} SAMPA ."}, {"turn": 224, "name": "PhD", "id": "A", "contribution": " OK , OK ."}, {"turn": 225, "name": "Grad", "id": "F", "contribution": " Yeah ."}, {"turn": 226, "name": "PhD", "id": "A", "contribution": " So this would {disfmarker} {pause} From this you would say , \" well , it doesn't really matter if we put Finnish {pause} into {pause} the training of the neural net , {pause} if there 's {pause} gonna be , {pause} you know , Finnish in the test data . \" Right ?"}, {"turn": 227, "name": "Professor", "id": "B", "contribution": " Well , it 's {disfmarker} it sounds {disfmarker} {pause} I mean , we have to be careful , cuz we haven't gotten a good result yet ."}, {"turn": 228, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 229, "name": "Professor", "id": "B", "contribution": " And comparing different bad results can be {pause} tricky ."}, {"turn": 230, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 231, "name": "Professor", "id": "B", "contribution": " But I {disfmarker} I {disfmarker} I {disfmarker} {pause} I think it does suggest that it 's not so much uh {pause} uh cross {pause} language as cross type of speech ."}, {"turn": 232, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 233, "name": "Professor", "id": "B", "contribution": " It 's {disfmarker} it 's um {disfmarker} {vocalsound} But we did {disfmarker} Oh yeah , the other thing I was asking him , though , is that I think that in the case {disfmarker} Yeah , you {disfmarker} you do have to be careful because of com compounded results . I think we got some earlier results {pause} in which you trained on one language and tested on another and you didn't have {pause} three , but you just had one {pause} language . So you trained on {pause} one type of digits and tested on another . Didn - Wasn't there something of that ? Where you , {pause} say , trained on Spanish and tested on {disfmarker} on TI - digits , or the other way around ? Something like that ?"}, {"turn": 234, "name": "PhD", "id": "E", "contribution": " No ."}, {"turn": 235, "name": "Professor", "id": "B", "contribution": " I thought there was something like that , {pause} that he showed me {pause} last week . We 'll have to wait till we get {disfmarker}"}, {"turn": 236, "name": "PhD", "id": "A", "contribution": " Yeah , that would be interesting ."}, {"turn": 237, "name": "Professor", "id": "B", "contribution": " Um , This may have been what I was asking before , Stephane , but {disfmarker} {pause} but , um , wasn't there something that you did , {pause} where you trained {pause} on one language and tested on another ? I mean no {disfmarker} no mixture but just {disfmarker}"}, {"turn": 238, "name": "Grad", "id": "F", "contribution": " I 'll get it for you ."}, {"turn": 239, "name": "PhD", "id": "D", "contribution": " Uh , no , no ."}, {"turn": 240, "name": "Professor", "id": "B", "contribution": " We 've never just trained on one lang"}, {"turn": 241, "name": "PhD", "id": "D", "contribution": " Training on a single language , you mean , and testing on the other one ?"}, {"turn": 242, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 243, "name": "PhD", "id": "D", "contribution": " Uh , no ."}, {"turn": 244, "name": "PhD", "id": "E", "contribution": " Not yet ."}, {"turn": 245, "name": "PhD", "id": "D", "contribution": " So the only {pause} task that 's similar to this is the training on two languages , and {comment} that {disfmarker}"}, {"turn": 246, "name": "Professor", "id": "B", "contribution": " But we 've done a bunch of things where we just trained on one language . Right ? I mean , you haven't {disfmarker} you haven't done all your tests on multiple languages ."}, {"turn": 247, "name": "PhD", "id": "D", "contribution": " Uh , No . Either thi this is test with {pause} uh the same language {pause} but from the broad data , or it 's test with {pause} uh different languages also from the broad data , excluding the {disfmarker} So , it 's {disfmarker} it 's three or {disfmarker} three and four ."}, {"turn": 248, "name": "PhD", "id": "E", "contribution": " The early experiment that {disfmarker}"}, {"turn": 249, "name": "PhD", "id": "A", "contribution": " Did you do different languages from digits ?"}, {"turn": 250, "name": "PhD", "id": "D", "contribution": " Uh . No . You mean {pause} training digits {pause} on one language and using the net {pause} to recognize on the other ?"}, {"turn": 251, "name": "PhD", "id": "A", "contribution": " Digits on another language ?"}, {"turn": 252, "name": "PhD", "id": "D", "contribution": " No ."}, {"turn": 253, "name": "Professor", "id": "B", "contribution": " See , I thought you showed me something like that last week . You had a {disfmarker} you had a little {disfmarker}"}, {"turn": 254, "name": "PhD", "id": "D", "contribution": " Uh , {pause} No , I don't think so ."}, {"turn": 255, "name": "Professor", "id": "B", "contribution": " Um What {disfmarker}"}, {"turn": 256, "name": "PhD", "id": "C", "contribution": " These numbers are uh {pause} ratio to baseline ?"}, {"turn": 257, "name": "Professor", "id": "B", "contribution": " So , I mean wha what 's the {disfmarker}"}, {"turn": 258, "name": "PhD", "id": "D", "contribution": " So ."}, {"turn": 259, "name": "Professor", "id": "B", "contribution": " This {disfmarker} this chart {disfmarker} this table that we 're looking at {pause} is um , show is all testing for TI - digits , or {disfmarker} ?"}, {"turn": 260, "name": "Grad", "id": "F", "contribution": " Bigger is worse ."}, {"turn": 261, "name": "PhD", "id": "D", "contribution": " So you have uh basically two {pause} uh parts ."}, {"turn": 262, "name": "Grad", "id": "F", "contribution": " This is error rate , I think ."}, {"turn": 263, "name": "PhD", "id": "C", "contribution": " Ratio ."}, {"turn": 264, "name": "Grad", "id": "F", "contribution": " No . {pause} No ."}, {"turn": 265, "name": "PhD", "id": "D", "contribution": " The upper part is for TI - digits"}, {"turn": 266, "name": "Grad", "id": "F", "contribution": " Yeah , yeah , yeah ."}, {"turn": 267, "name": "PhD", "id": "D", "contribution": " and it 's divided in three {pause} rows {pause} of four {disfmarker} four rows each ."}, {"turn": 268, "name": "Grad", "id": "F", "contribution": " Mm - hmm ."}, {"turn": 269, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 270, "name": "PhD", "id": "D", "contribution": " And the first four rows is well - matched , then the s the second group of four rows is mismatched , and {pause} finally highly mismatched . And then the lower part is for Italian and it 's the same {disfmarker} {pause} the same thing ."}, {"turn": 271, "name": "PhD", "id": "A", "contribution": " So , so the upper part is training {pause} TI - digits ?"}, {"turn": 272, "name": "PhD", "id": "D", "contribution": " So . It 's {disfmarker} it 's the HTK results , I mean . So it 's {pause} HTK training testings {pause} with different kind of features"}, {"turn": 273, "name": "PhD", "id": "A", "contribution": " Ah ."}, {"turn": 274, "name": "PhD", "id": "D", "contribution": " and what appears in the {pause} uh left column is {pause} the networks that are used for doing this ."}, {"turn": 275, "name": "Professor", "id": "B", "contribution": " Hmm ."}, {"turn": 276, "name": "PhD", "id": "D", "contribution": " So . Uh Yeah ."}, {"turn": 277, "name": "Professor", "id": "B", "contribution": " Well , What was is that i What was it that you had {pause} done {pause} last week when you showed {disfmarker} Do you remember ? Wh - when you showed me {pause} the {disfmarker} your table last week ?"}, {"turn": 278, "name": "PhD", "id": "D", "contribution": " It - It was part of these results . Mmm . Mmm ."}, {"turn": 279, "name": "PhD", "id": "A", "contribution": " So where is the baseline {pause} for the TI - digits {pause} located in here ?"}, {"turn": 280, "name": "PhD", "id": "D", "contribution": " You mean the HTK Aurora baseline ?"}, {"turn": 281, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 282, "name": "PhD", "id": "D", "contribution": " It 's uh the one hundred number . It 's , well , all these numbers are the ratio {pause} with respect to the baseline ."}, {"turn": 283, "name": "PhD", "id": "A", "contribution": " Ah ! Ah , OK , OK ."}, {"turn": 284, "name": "Professor", "id": "B", "contribution": " So this is word {disfmarker} word error rate , so a high number is bad ."}, {"turn": 285, "name": "PhD", "id": "D", "contribution": " Yeah , this is {pause} a word error rate ratio ."}, {"turn": 286, "name": "PhD", "id": "E", "contribution": " Yeah ."}, {"turn": 287, "name": "PhD", "id": "A", "contribution": " OK , I see ."}, {"turn": 288, "name": "PhD", "id": "D", "contribution": " Yeah . So , seventy point two means that {pause} we reduced the error rate uh by thirty {disfmarker} thirty percent ."}, {"turn": 289, "name": "PhD", "id": "A", "contribution": " OK , OK , gotcha ."}, {"turn": 290, "name": "PhD", "id": "D", "contribution": " So ."}, {"turn": 291, "name": "Professor", "id": "B", "contribution": " OK , {vocalsound} so if we take"}, {"turn": 292, "name": "PhD", "id": "D", "contribution": " Hmm ."}, {"turn": 293, "name": "Professor", "id": "B", "contribution": " uh um let 's see PLP {pause} uh with on - line {pause} normalization and {pause} delta - del so that 's this thing you have circled here {pause} in the second column ,"}, {"turn": 294, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 295, "name": "Professor", "id": "B", "contribution": " um {pause} and \" multi - English \" refers to what ?"}, {"turn": 296, "name": "PhD", "id": "D", "contribution": " To TIMIT . Mmm . Then you have {pause} uh MF , {pause} MS and ME which are for French , Spanish and English . And , yeah . Actually I {disfmarker} {pause} I uh forgot to say that {pause} the multilingual net are trained {pause} on {pause} uh {pause} features without the s derivatives uh but with {pause} increased frame numbers . Mmm . And we can {disfmarker} we can see on the first line of the table that it {disfmarker} it {disfmarker} {pause} it 's slightly {disfmarker} slightly worse when we don't use delta but it 's not {disfmarker} {pause} not that much ."}, {"turn": 297, "name": "Professor", "id": "B", "contribution": " Right . So w w So , I 'm sorry . I missed that . What 's MF , MS and ME ?"}, {"turn": 298, "name": "PhD", "id": "A", "contribution": " Multi - French , Multi - Spanish"}, {"turn": 299, "name": "PhD", "id": "D", "contribution": " So . Multi - French , Multi - Spanish , and Multi - English ."}, {"turn": 300, "name": "Professor", "id": "B", "contribution": " Uh OK . So , it 's {pause} uh {pause} broader vocabulary . Then {disfmarker} And {disfmarker}"}, {"turn": 301, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 302, "name": "Professor", "id": "B", "contribution": " OK so I think what I 'm {disfmarker} what I saw in your smaller chart that I was thinking of was {disfmarker} was {pause} there were some numbers I saw , I think , that included these multiple languages and it {disfmarker} and I was seeing {pause} that it got worse . I {disfmarker} I think that was all it was . You had some very limited results that {disfmarker} at that point"}, {"turn": 303, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 304, "name": "Professor", "id": "B", "contribution": " which showed {pause} having in these {disfmarker} these other languages . In fact it might have been just this last category , {pause} having two languages broad that were {disfmarker} where {disfmarker} where English was removed . So that was cross language and the {disfmarker} and the result was quite poor . What I {disfmarker} {pause} we hadn't seen yet was that if you added in the English , it 's still poor ."}, {"turn": 305, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 306, "name": "Professor", "id": "B", "contribution": " Uh {vocalsound} {vocalsound} Um now , what 's the noise condition {pause} um {pause} of the training data {disfmarker}"}, {"turn": 307, "name": "PhD", "id": "D", "contribution": " Still poor ."}, {"turn": 308, "name": "Professor", "id": "B", "contribution": " Well , I think this is what you were explaining . The noise condition is the same {disfmarker} It 's the same uh Aurora noises uh , in all these cases {pause} for the training ."}, {"turn": 309, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah ."}, {"turn": 310, "name": "Professor", "id": "B", "contribution": " So there 's not a {pause} statistical {disfmarker} sta a strong st {pause} statistically different {pause} noise characteristic between {pause} uh the training and test"}, {"turn": 311, "name": "PhD", "id": "D", "contribution": " No these are the s s s same noises ,"}, {"turn": 312, "name": "Professor", "id": "B", "contribution": " and yet we 're seeing some kind of effect {disfmarker}"}, {"turn": 313, "name": "PhD", "id": "D", "contribution": " yeah . At least {disfmarker} at least for the first {disfmarker} {pause} for the well - matched ,"}, {"turn": 314, "name": "Grad", "id": "F", "contribution": " Well matched condition ."}, {"turn": 315, "name": "Professor", "id": "B", "contribution": " Right ."}, {"turn": 316, "name": "PhD", "id": "D", "contribution": " yeah ."}, {"turn": 317, "name": "Professor", "id": "B", "contribution": " So there 's some kind of a {disfmarker} a {disfmarker} an effect from having these {disfmarker} uh this broader coverage um Now I guess what we should try doing with this is try {pause} testing these on u this same sort of thing on {disfmarker} you probably must have this {pause} lined up to do . To try the same t {pause} with the exact same training , do testing on {pause} the other languages ."}, {"turn": 318, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 319, "name": "Professor", "id": "B", "contribution": " On {disfmarker} on um {disfmarker} So . Um , oh I well , wait a minute . You have this here , for the Italian . That 's right . OK , so , {pause} So ."}, {"turn": 320, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah , so for the Italian the results are {vocalsound} uh {pause} stranger um {pause} Mmm . So what appears is that perhaps Spanish is {pause} not very close to Italian because uh , well , {pause} when using the {disfmarker} the network trained only on Spanish it 's {disfmarker} {pause} the error rate is {pause} almost uh twice {pause} the baseline error rate ."}, {"turn": 321, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 322, "name": "PhD", "id": "D", "contribution": " Mmm . {vocalsound} Uh ."}, {"turn": 323, "name": "Professor", "id": "B", "contribution": " Well , I mean , let 's see . Is there any difference in {disfmarker} So it 's in {pause} the uh {disfmarker} So you 're saying that {pause} when you train on English {pause} and {pause} uh {pause} and {disfmarker} and test on {disfmarker}"}, {"turn": 324, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 325, "name": "Professor", "id": "B", "contribution": " No , you don't have training on English testing {disfmarker}"}, {"turn": 326, "name": "PhD", "id": "D", "contribution": " There {disfmarker} there is {disfmarker} another difference , is that the noise {disfmarker} the noises are different ."}, {"turn": 327, "name": "Professor", "id": "B", "contribution": " In {disfmarker} in what ?"}, {"turn": 328, "name": "PhD", "id": "D", "contribution": " Well , For {disfmarker} for the Italian part I mean the {pause} uh {pause} the um {pause} networks are trained with noise from {pause} Aurora {disfmarker} TI - digits ,"}, {"turn": 329, "name": "PhD", "id": "E", "contribution": " Aurora - two ."}, {"turn": 330, "name": "PhD", "id": "D", "contribution": " mmm ."}, {"turn": 331, "name": "Professor", "id": "B", "contribution": " And the noise is different in th"}, {"turn": 332, "name": "PhD", "id": "D", "contribution": " Yeah . And perhaps the noise are {pause} quite different from the noises {pause} in the speech that Italian ."}, {"turn": 333, "name": "Professor", "id": "B", "contribution": " Do we have any um {pause} test sets {pause} uh in {pause} any other language that um have the same noise as in {pause} the Aurora ?"}, {"turn": 334, "name": "PhD", "id": "D", "contribution": " And {disfmarker}"}, {"turn": 335, "name": "PhD", "id": "E", "contribution": " Mmm , no ."}, {"turn": 336, "name": "PhD", "id": "D", "contribution": " No ."}, {"turn": 337, "name": "PhD", "id": "A", "contribution": " Can I ask something real quick ? In {disfmarker} in the upper part {disfmarker} {pause} in the English {pause} stuff , {pause} it looks like the very best number is sixty point nine ? and that 's in the uh {disfmarker} {pause} the third {pause} section in the upper part under PLP JRASTA , sort of the middle column ?"}, {"turn": 338, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 339, "name": "PhD", "id": "A", "contribution": " I is that {pause} a noisy condition ?"}, {"turn": 340, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 341, "name": "PhD", "id": "A", "contribution": " So that 's matched training ? Is that what that is ?"}, {"turn": 342, "name": "PhD", "id": "D", "contribution": " It 's {disfmarker} no , the third part , so it 's uh {pause} highly mismatched . So . Training and {pause} test noise are different ."}, {"turn": 343, "name": "PhD", "id": "A", "contribution": " So {disfmarker} why do you get your best number in {disfmarker} Wouldn't you get your best number in the clean case ?"}, {"turn": 344, "name": "PhD", "id": "C", "contribution": " Well , it 's relative to the um {pause} baseline mismatching"}, {"turn": 345, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 346, "name": "PhD", "id": "A", "contribution": " Ah ,"}, {"turn": 347, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah ."}, {"turn": 348, "name": "PhD", "id": "A", "contribution": " OK so these are not {disfmarker} OK , alright , I see ."}, {"turn": 349, "name": "PhD", "id": "C", "contribution": " Yeah ."}, {"turn": 350, "name": "PhD", "id": "A", "contribution": " OK . And then {disfmarker} so , in the {disfmarker} in the um {disfmarker} {pause} in the {pause} non - mismatched clean case , {pause} your best one was under MFCC ? That sixty - one point four ?"}, {"turn": 351, "name": "PhD", "id": "D", "contribution": " Yeah . {pause} But it 's not a clean case . It 's {pause} a noisy case but {pause} uh training and test noises are the same ."}, {"turn": 352, "name": "PhD", "id": "A", "contribution": " Oh ! So this upper third ?"}, {"turn": 353, "name": "PhD", "id": "D", "contribution": " So {disfmarker} Yeah ."}, {"turn": 354, "name": "PhD", "id": "A", "contribution": " Uh that 's still noisy ?"}, {"turn": 355, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 356, "name": "PhD", "id": "A", "contribution": " Ah , OK ."}, {"turn": 357, "name": "PhD", "id": "D", "contribution": " So it 's always noisy basically ,"}, {"turn": 358, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 359, "name": "PhD", "id": "D", "contribution": " and , {pause} well , the {disfmarker}"}, {"turn": 360, "name": "PhD", "id": "A", "contribution": " I see ."}, {"turn": 361, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 362, "name": "Professor", "id": "B", "contribution": " OK ? Um {pause} So uh , I think this will take some {pause} looking at , thinking about . But , {pause} what is uh {disfmarker} what is currently running , that 's {disfmarker} uh , i that {disfmarker} just filling in the holes here or {disfmarker} or {disfmarker} ? {comment} {pause} pretty much ?"}, {"turn": 363, "name": "PhD", "id": "D", "contribution": " Uh , no we don't plan to fill the holes"}, {"turn": 364, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 365, "name": "PhD", "id": "D", "contribution": " but {pause} actually there is something important , is that {pause} um we made a lot of assumption concerning the on - line normalization and we just noticed {pause} uh recently that {pause} uh the {pause} approach that we were using {pause} was not {pause} uh {pause} leading to very good results {pause} when we {pause} used the straight features to HTK . Um {pause} {pause} Mmm . So basically d {pause} if you look at the {disfmarker} at the left of the table , {pause} the first uh row , {pause} with eighty - six , one hundred , and forty - three and seventy - five , these are the results we obtained for Italian {pause} uh with {pause} straight {pause} mmm , PLP features {pause} using on - line normalization ."}, {"turn": 366, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 367, "name": "PhD", "id": "D", "contribution": " Mmm . And the , mmm {disfmarker} what 's {pause} in the table , just {pause} at the left of the PLP twelve {pause} on - line normalization column , so , the numbers seventy - nine , fifty - four and {pause} uh forty - two {pause} are the results obtained by uh Pratibha with {pause} uh his on - line normalization {disfmarker} uh her on - line normalization approach ."}, {"turn": 368, "name": "PhD", "id": "A", "contribution": " Where is that ? seventy - nine , fifty"}, {"turn": 369, "name": "Professor", "id": "B", "contribution": " Uh , it 's just sort of sitting right on the uh {disfmarker} the column line ."}, {"turn": 370, "name": "PhD", "id": "D", "contribution": " So ."}, {"turn": 371, "name": "PhD", "id": "E", "contribution": " Fifty - one ? This {disfmarker}"}, {"turn": 372, "name": "PhD", "id": "A", "contribution": " Oh I see , OK ."}, {"turn": 373, "name": "Professor", "id": "B", "contribution": " Uh . {pause} Yeah ."}, {"turn": 374, "name": "PhD", "id": "D", "contribution": " Just {disfmarker} uh Yeah . So these are the results of {pause} OGI with {pause} on - line normalization and straight features to HTK . And the previous result , eighty - six and so on , {pause} are with our {pause} features straight to HTK ."}, {"turn": 375, "name": "Professor", "id": "B", "contribution": " Yes . Yes ."}, {"turn": 376, "name": "PhD", "id": "D", "contribution": " So {pause} what we see that {disfmarker} is {disfmarker} there is that um {pause} uh the way we were doing this was not correct , but {pause} still {pause} the networks {pause} are very good . When we use the networks {pause} our number are better that {pause} uh Pratibha results ."}, {"turn": 377, "name": "PhD", "id": "E", "contribution": " We improve ."}, {"turn": 378, "name": "Professor", "id": "B", "contribution": " So , do you know what was wrong with the on - line normalization , or {disfmarker} ?"}, {"turn": 379, "name": "PhD", "id": "D", "contribution": " Yeah . There were diff there were different things and {pause} basically , {pause} the first thing is the mmm , {pause} alpha uh {pause} value . So , the recursion {pause} uh {pause} part . um , {pause} I used point five percent , {pause} which was the default value in the {disfmarker} {pause} in the programs here . And Pratibha used five percent ."}, {"turn": 380, "name": "Professor", "id": "B", "contribution": " Uh"}, {"turn": 381, "name": "PhD", "id": "D", "contribution": " So it adapts more {pause} quickly"}, {"turn": 382, "name": "Professor", "id": "B", "contribution": " Yes . Yeah ."}, {"turn": 383, "name": "PhD", "id": "D", "contribution": " Um , but , yeah . I assume that this was not important because {pause} uh previous results from {disfmarker} from Dan and {disfmarker} show that basically {pause} the {pause} both {disfmarker} both values g give the same {disfmarker} same {pause} uh results . It was true on uh {pause} TI - digits but it 's not true on Italian ."}, {"turn": 384, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 385, "name": "PhD", "id": "D", "contribution": " Uh , second thing is the initialization of the {pause} stuff . Actually , {pause} uh what we were doing is to start the recursion from the beginning of the {pause} utterance . And using initial values that are the global mean and variances {pause} measured across the whole database ."}, {"turn": 386, "name": "Professor", "id": "B", "contribution": " Right . Right ."}, {"turn": 387, "name": "PhD", "id": "D", "contribution": " And Pratibha did something different is that he {disfmarker} uh she initialed the um values of the mean and variance {pause} by computing {pause} this on the {pause} twenty - five first frames of each utterance . Mmm . There were other minor differences , the fact that {pause} she used fifteen dissities instead s instead of thirteen , and that she used C - zero instead of log energy . Uh , but the main differences concerns the recursion . So . {pause} Uh , I changed the code uh and now we have a baseline that 's similar to the OGI baseline ."}, {"turn": 388, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 389, "name": "PhD", "id": "D", "contribution": " We {disfmarker} It {disfmarker} it 's slightly {pause} uh different because {pause} I don't exactly initialize the same way she does . Actually I start , {pause} mmm , I don't wait to a fifteen {disfmarker} twenty - five {disfmarker} twenty - five frames {pause} before computing a mean and the variance {pause} to e to {disfmarker} to start the recursion ."}, {"turn": 390, "name": "PhD", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 391, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 392, "name": "PhD", "id": "D", "contribution": " I {disfmarker} I use the on - line scheme and only start the re recursion after the twenty - five {disfmarker} {pause} twenty - fifth frame . But , well it 's similar . So {pause} uh I retrained {pause} the networks with {pause} these {disfmarker} well , the {disfmarker} the {disfmarker} the networks are retaining with these new {pause} features ."}, {"turn": 393, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 394, "name": "PhD", "id": "D", "contribution": " And , yeah ."}, {"turn": 395, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 396, "name": "PhD", "id": "D", "contribution": " So basically what I expect is that {pause} these numbers will a little bit go down but {pause} perhaps not {disfmarker} not so much"}, {"turn": 397, "name": "Professor", "id": "B", "contribution": " Right ."}, {"turn": 398, "name": "PhD", "id": "D", "contribution": " because {pause} I think the neural networks learn perhaps {pause} to {disfmarker}"}, {"turn": 399, "name": "Professor", "id": "B", "contribution": " Right ."}, {"turn": 400, "name": "PhD", "id": "D", "contribution": " even if the features are not {pause} normalized . It {disfmarker} it will learn how to normalize and {disfmarker}"}, {"turn": 401, "name": "Professor", "id": "B", "contribution": " OK , but I think that {pause} given the pressure of time we probably want to draw {disfmarker} because of that {pause} especially , we wanna draw some conclusions from this , do some reductions {pause} in what we 're looking at ,"}, {"turn": 402, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 403, "name": "Professor", "id": "B", "contribution": " and make some strong decisions for what we 're gonna do testing on before next week . So do you {disfmarker} are you {disfmarker} w did you have something going on , on the side , with uh multi - band {pause} or {disfmarker} on {disfmarker} on this ,"}, {"turn": 404, "name": "PhD", "id": "D", "contribution": " Yeah {vocalsound} I"}, {"turn": 405, "name": "Professor", "id": "B", "contribution": " or {disfmarker} ?"}, {"turn": 406, "name": "PhD", "id": "D", "contribution": " No , I {disfmarker} we plan to start this uh so , act actually we have discussed uh {pause} @ @ um , these {disfmarker} what we could do {pause} more as a {disfmarker} as a research and {disfmarker} {pause} and {pause} we were thinking perhaps that {pause} uh {pause} the way we use the tandem is not {disfmarker} Uh , well , there is basically perhaps a flaw in the {disfmarker} in the {disfmarker} the stuff because {pause} we {pause} trained the networks {disfmarker} If we trained the networks on the {disfmarker} on {pause} a language and a t or a specific {pause} task ,"}, {"turn": 407, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 408, "name": "PhD", "id": "D", "contribution": " um , what we ask is {disfmarker} to the network {disfmarker} is to put the bound the decision boundaries somewhere in the space ."}, {"turn": 409, "name": "Professor", "id": "B", "contribution": " Mmm ."}, {"turn": 410, "name": "PhD", "id": "D", "contribution": " And uh {pause} mmm and ask the network to put one , {pause} at one side of the {disfmarker} for {disfmarker} for a particular phoneme at one side of the boundary {disfmarker} decision boundary and one for another phoneme at the other side . And {pause} so there is kind of reduction of the information there that 's not correct because if we change task {pause} and if the phonemes are not in the same context in the new task , {pause} obviously the {pause} decision boundaries are not {disfmarker} {pause} should not be at the same {pause} place ."}, {"turn": 411, "name": "Professor", "id": "B", "contribution": " I di"}, {"turn": 412, "name": "PhD", "id": "D", "contribution": " But the way the feature gives {disfmarker} The {disfmarker} the way the network gives the features is that it reduce completely the {disfmarker} {pause} it removes completely the information {disfmarker} {pause} a lot of information from the {disfmarker} the features {pause} by uh {pause} uh {pause} placing the decision boundaries at {pause} optimal places for {pause} one kind of {pause} data but {pause} this is not the case for another kind of data ."}, {"turn": 413, "name": "Professor", "id": "B", "contribution": " It 's a trade - off ,"}, {"turn": 414, "name": "PhD", "id": "D", "contribution": " So {disfmarker}"}, {"turn": 415, "name": "Professor", "id": "B", "contribution": " right ? Any - anyway go ahead ."}, {"turn": 416, "name": "PhD", "id": "D", "contribution": " Yeah . So uh what we were thinking about is perhaps {pause} um one way {pause} to solve this problem is increase the number of {pause} outputs of the neural networks . Doing something like , um {pause} um phonemes within context and , well , basically context dependent phonemes ."}, {"turn": 417, "name": "Professor", "id": "B", "contribution": " Maybe . I mean , I {disfmarker} I think {pause} you could make {pause} the same argument , it 'd be just as legitimate , {pause} for hybrid systems {pause} as well . Right ."}, {"turn": 418, "name": "PhD", "id": "D", "contribution": " Yeah but , we know that {disfmarker}"}, {"turn": 419, "name": "Professor", "id": "B", "contribution": " And in fact , {pause} th things get better with context dependent {pause} versions . Right ?"}, {"turn": 420, "name": "PhD", "id": "D", "contribution": " Ye - yeah but here it 's something different . We want to have features"}, {"turn": 421, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 422, "name": "PhD", "id": "D", "contribution": " uh well , {pause} um ."}, {"turn": 423, "name": "Professor", "id": "B", "contribution": " Yeah , but it 's still true {pause} that what you 're doing {pause} is you 're ignoring {disfmarker} you 're {disfmarker} you 're coming up with something to represent , {pause} whether it 's a distribution , {pause} probability distribution or features , you 're coming up with a set of variables {pause} that are representing {pause} uh , {pause} things that vary w over context ."}, {"turn": 424, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 425, "name": "Professor", "id": "B", "contribution": " Uh , and you 're {pause} putting it all together , ignoring the differences in context . That {disfmarker} that 's true {pause} for the hybrid system , it 's true for a tandem system . So , for that reason , when you {disfmarker} in {disfmarker} in {disfmarker} in a hybrid system , {pause} when you incorporate context one way or another , {pause} you do get better scores ."}, {"turn": 426, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 427, "name": "Professor", "id": "B", "contribution": " OK ? But I {disfmarker} it 's {disfmarker} it 's a big deal {pause} to get that . I {disfmarker} I 'm {disfmarker} I 'm sort of {disfmarker} And once you {disfmarker} the other thing is that once you represent {disfmarker} start representing more and more context {pause} it is {pause} uh {pause} much more {pause} um specific {pause} to a particular task in language . So um Uh , the {disfmarker} {pause} the acoustics associated with {pause} uh a particular context , for instance you may have some kinds of contexts that will never occur {pause} in one language and will occur frequently in the other , so the qu the issue of getting enough training {pause} for a particular kind of context becomes harder . We already actually don't have a huge amount of training data um"}, {"turn": 428, "name": "PhD", "id": "D", "contribution": " Yeah , but {disfmarker} mmm , I mean , {pause} the {disfmarker} the way we {disfmarker} we do it now is that we have a neural network and {pause} basically {pause} the net network is trained almost to give binary decisions ."}, {"turn": 429, "name": "Professor", "id": "B", "contribution": " Right ."}, {"turn": 430, "name": "PhD", "id": "D", "contribution": " And {pause} uh {disfmarker} binary decisions about phonemes . Nnn {disfmarker} Uh It 's {disfmarker}"}, {"turn": 431, "name": "Professor", "id": "B", "contribution": " Almost . But I mean it {disfmarker} it {disfmarker} it does give a distribution ."}, {"turn": 432, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 433, "name": "Professor", "id": "B", "contribution": " It 's {disfmarker} and {disfmarker} and {pause} it is true that if there 's two phones that are very similar , {pause} that {pause} uh {pause} the {disfmarker} {pause} i it may prefer one but it will {pause} give a reasonably high value to the other , too ."}, {"turn": 434, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah , sure but uh {pause} So basically it 's almost binary decisions and {pause} um the idea of using more {pause} classes is {pause} to {pause} get something that 's {pause} less binary decisions ."}, {"turn": 435, "name": "Professor", "id": "B", "contribution": " Oh no , but it would still be even more of a binary decision . It {disfmarker} it 'd be even more of one . Because then you would say {pause} that in {disfmarker} that this phone in this context is a one , {pause} but the same phone in a slightly different context is a zero ."}, {"turn": 436, "name": "PhD", "id": "D", "contribution": " But {disfmarker} yeah , but {disfmarker}"}, {"turn": 437, "name": "Professor", "id": "B", "contribution": " That would be even {disfmarker} even more distinct of a binary decision . I actually would have thought you 'd wanna go the other way and have fewer classes ."}, {"turn": 438, "name": "PhD", "id": "D", "contribution": " Yeah , but if {disfmarker}"}, {"turn": 439, "name": "Professor", "id": "B", "contribution": " Uh , I mean for instance , the {disfmarker} the thing I was arguing for before , but again which I don't think we have time to try , {pause} is something in which you would modify the code so you could train to have several outputs on and use articulatory features"}, {"turn": 440, "name": "PhD", "id": "D", "contribution": " Mmm . Mm - hmm ."}, {"turn": 441, "name": "Professor", "id": "B", "contribution": " cuz then that would {disfmarker} that would go {disfmarker} {pause} that would be much broader and cover many different situations . But if you go to very very fine categories , it 's very {pause} binary ."}, {"turn": 442, "name": "PhD", "id": "D", "contribution": " Mmm . Yeah , but I think {disfmarker} Yeah , perhaps you 're right , but you have more classes so {pause} you {disfmarker} you have more information in your features . So , {vocalsound} Um {pause} You have more information in the {pause} uh"}, {"turn": 443, "name": "Professor", "id": "B", "contribution": " Mm - hmm . True ."}, {"turn": 444, "name": "PhD", "id": "D", "contribution": " posteriors vector um which means that {disfmarker} But still the information is relevant"}, {"turn": 445, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 446, "name": "PhD", "id": "D", "contribution": " because it 's {disfmarker} it 's information that helps to discriminate ,"}, {"turn": 447, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 448, "name": "PhD", "id": "D", "contribution": " if it 's possible to be able to discriminate {pause} among the phonemes in context ."}, {"turn": 449, "name": "Professor", "id": "B", "contribution": " Well it 's {disfmarker} it 's {disfmarker} {pause} it 's an interesting thought ."}, {"turn": 450, "name": "PhD", "id": "D", "contribution": " But the {disfmarker}"}, {"turn": 451, "name": "Professor", "id": "B", "contribution": " I mean we {disfmarker} we could disagree about it at length"}, {"turn": 452, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 453, "name": "Professor", "id": "B", "contribution": " but the {disfmarker} the real thing is if you 're interested in it you 'll probably try it"}, {"turn": 454, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 455, "name": "Professor", "id": "B", "contribution": " and {disfmarker} {pause} and {pause} we 'll see . But {disfmarker} but what I 'm more concerned with now , as an operational level , is {pause} uh , you know ,"}, {"turn": 456, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 457, "name": "Professor", "id": "B", "contribution": " what do we do in four or five days ? Uh , and {disfmarker} {pause} so we have {pause} to be concerned {pause} with Are we gonna look at any combinations of things , you know once the nets get retrained so you have this problem out of it ."}, {"turn": 458, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 459, "name": "Professor", "id": "B", "contribution": " Um , are we going to look at {pause} multi - band ? Are we gonna look at combinations of things ? Uh , what questions are we gonna ask , uh now that , I mean , {pause} we should probably turn shortly to this O G I note . Um , how are we going to {pause} combine {pause} with what they 've been focusing on ? Uh , {pause} Uh we haven't been doing any of the L D A RASTA sort of thing ."}, {"turn": 460, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 461, "name": "Professor", "id": "B", "contribution": " And they , although they don't talk about it in this note , um , {pause} there 's um , {pause} the issue of the {pause} um Mu law {pause} business {pause} uh {pause} versus the logarithm , um , {pause} so ."}, {"turn": 462, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 463, "name": "Professor", "id": "B", "contribution": " So what i what is going on right now ? What 's right {disfmarker} you 've got {pause} nets retraining , Are there {disfmarker} is there {disfmarker} are there any H T K {pause} trainings {disfmarker} testings going on ?"}, {"turn": 464, "name": "PhD", "id": "D", "contribution": " N"}, {"turn": 465, "name": "PhD", "id": "E", "contribution": " I {disfmarker} I {disfmarker} I 'm trying the HTK with eh , {pause} PLP twelve on - line delta - delta and MSG filter {pause} together ."}, {"turn": 466, "name": "Professor", "id": "B", "contribution": " The combination , I see ."}, {"turn": 467, "name": "PhD", "id": "E", "contribution": " The combination , yeah . But I haven't result {vocalsound} at this moment ."}, {"turn": 468, "name": "Professor", "id": "B", "contribution": " MSG and {disfmarker} and PLP ."}, {"turn": 469, "name": "PhD", "id": "E", "contribution": " Yeah ."}, {"turn": 470, "name": "Professor", "id": "B", "contribution": " And is this with the revised {pause} on - line normalization ?"}, {"turn": 471, "name": "PhD", "id": "E", "contribution": " Ye - Uh , with the old {pause} older ,"}, {"turn": 472, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 473, "name": "Professor", "id": "B", "contribution": " Old one . So it 's using all the nets for that"}, {"turn": 474, "name": "PhD", "id": "E", "contribution": " yeah ."}, {"turn": 475, "name": "Professor", "id": "B", "contribution": " but again we have the hope that it {disfmarker} {pause} We have the hope that it {disfmarker} {pause} maybe it 's not making too much difference ,"}, {"turn": 476, "name": "PhD", "id": "E", "contribution": " Yeah . But {pause} We can know soon ."}, {"turn": 477, "name": "Professor", "id": "B", "contribution": " but {disfmarker} but"}, {"turn": 478, "name": "PhD", "id": "E", "contribution": " Maybe ."}, {"turn": 479, "name": "Professor", "id": "B", "contribution": " yeah ."}, {"turn": 480, "name": "PhD", "id": "E", "contribution": " I don't know ."}, {"turn": 481, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 482, "name": "Professor", "id": "B", "contribution": " Uh , OK ."}, {"turn": 483, "name": "PhD", "id": "D", "contribution": " Uh so there is this combination , yeah . Working on combination obviously ."}, {"turn": 484, "name": "PhD", "id": "E", "contribution": " Mm - hmm ."}, {"turn": 485, "name": "PhD", "id": "D", "contribution": " Um , I will start work on multi - band . And {pause} we {pause} plan to work also on the idea of using both {pause} features {pause} and net outputs ."}, {"turn": 486, "name": "PhD", "id": "E", "contribution": ""}, {"turn": 487, "name": "PhD", "id": "D", "contribution": " Um . And {pause} we think that {pause} with this approach perhaps {pause} we could reduce the number of outputs of the neural network . Um , So , get simpler networks , because we still have the features . So we have um {pause} come up with um {pause} different kind of {pause} broad phonetic categories . And we have {disfmarker} Basically we have three {pause} types of broad phonetic classes . Well , something using place of articulation which {disfmarker} which leads to {pause} nine , I think , {pause} broad classes . Uh , another which is based on manner , which is {disfmarker} is also something like nine classes . And then , {pause} something that combine both , and we have {pause} twenty f {pause} twenty - five ?"}, {"turn": 488, "name": "Grad", "id": "F", "contribution": " Twenty - seven ."}, {"turn": 489, "name": "PhD", "id": "D", "contribution": " Twenty - seven broad classes . So like , uh , oh , I don't know , like back vowels , front vowels ."}, {"turn": 490, "name": "Professor", "id": "B", "contribution": " So what you do {disfmarker} um I just wanna understand"}, {"turn": 491, "name": "PhD", "id": "D", "contribution": " Um For the moments we do not {disfmarker} don't have nets ,"}, {"turn": 492, "name": "Professor", "id": "B", "contribution": " so {pause} You have two net or three nets ? Was this ? How many {disfmarker} how many nets do you have ? No nets ."}, {"turn": 493, "name": "PhD", "id": "D", "contribution": " I mean , {pause} It 's just {disfmarker} Were we just changing {pause} the labels to retrain nets {pause} with fewer out outputs ."}, {"turn": 494, "name": "PhD", "id": "E", "contribution": " Begin to work in this . We are @ @ ."}, {"turn": 495, "name": "Professor", "id": "B", "contribution": " Right . But {disfmarker} but I didn't understand {disfmarker}"}, {"turn": 496, "name": "PhD", "id": "D", "contribution": " And then {disfmarker} Mm - hmm ."}, {"turn": 497, "name": "Professor", "id": "B", "contribution": " Uh . {pause} the software currently just has {disfmarker} uh a {disfmarker} allows for I think , the one {disfmarker} one hot output . So you 're having multiple nets and combining them , or {disfmarker} ? Uh , how are you {disfmarker} how are you coming up with {disfmarker} If you say {pause} uh {pause} If you have a place {pause} characteristic and a manner characteristic , how do you {disfmarker}"}, {"turn": 498, "name": "PhD", "id": "D", "contribution": " It - It 's the single net ,"}, {"turn": 499, "name": "PhD", "id": "A", "contribution": " I think they have one output ."}, {"turn": 500, "name": "PhD", "id": "D", "contribution": " yeah ."}, {"turn": 501, "name": "Professor", "id": "B", "contribution": " Oh , it 's just one net ."}, {"turn": 502, "name": "PhD", "id": "D", "contribution": " It 's one net with {pause} um {pause} twenty - seven outputs"}, {"turn": 503, "name": "PhD", "id": "E", "contribution": " Yeah ."}, {"turn": 504, "name": "Grad", "id": "F", "contribution": " mm - hmm"}, {"turn": 505, "name": "PhD", "id": "D", "contribution": " if we have twenty - seven classes ,"}, {"turn": 506, "name": "Professor", "id": "B", "contribution": " I see . I see , OK ."}, {"turn": 507, "name": "PhD", "id": "D", "contribution": " yeah . So it 's {disfmarker} Well , it 's basically a standard net with fewer {pause} classes ."}, {"turn": 508, "name": "Professor", "id": "B", "contribution": " So you 're sort of going the other way of what you were saying a bit ago instead of {disfmarker} yeah ."}, {"turn": 509, "name": "PhD", "id": "D", "contribution": " Yeah , but I think {disfmarker} Yeah . B b including the features , yeah ."}, {"turn": 510, "name": "Grad", "id": "F", "contribution": " But including the features ."}, {"turn": 511, "name": "PhD", "id": "E", "contribution": " Yeah ."}, {"turn": 512, "name": "PhD", "id": "D", "contribution": " I don't think this {pause} will work {pause} alone . I think it will get worse because Well , I believe the effect that {disfmarker} of {disfmarker} of too reducing too much the information is {pause} basically {disfmarker} basically what happens"}, {"turn": 513, "name": "Professor", "id": "B", "contribution": " Uh - huh ."}, {"turn": 514, "name": "PhD", "id": "D", "contribution": " and {disfmarker}"}, {"turn": 515, "name": "Professor", "id": "B", "contribution": " But you think if you include that {pause} plus the other features ,"}, {"turn": 516, "name": "PhD", "id": "D", "contribution": " but {disfmarker} Yeah , because {pause} there is perhaps one important thing that the net {pause} brings , and OGI show showed that , is {pause} the distinction between {pause} sp speech and silence Because these nets are trained on well - controlled condition . I mean the labels are obtained on clean speech , and we add noise after . So this is one thing And But perhaps , something intermediary using also {pause} some broad classes could {disfmarker} could bring so much more information . Uh ."}, {"turn": 517, "name": "Professor", "id": "B", "contribution": " So {disfmarker} so again then we have these broad classes and {disfmarker} well , somewhat broad . I mean , it 's twenty - seven instead of sixty - four , {pause} basically . And you have the original features ."}, {"turn": 518, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 519, "name": "Professor", "id": "B", "contribution": " Which are PLP , or something ."}, {"turn": 520, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 521, "name": "Professor", "id": "B", "contribution": " And then uh , just to remind me , all of that goes {pause} into {disfmarker} uh , that all of that is transformed by uh , uh , K - KL or something , or {disfmarker} ?"}, {"turn": 522, "name": "PhD", "id": "D", "contribution": " Mm - hmm . There will probably be ,"}, {"turn": 523, "name": "PhD", "id": "E", "contribution": " Mu ."}, {"turn": 524, "name": "PhD", "id": "D", "contribution": " yeah , one single KL to transform everything"}, {"turn": 525, "name": "Professor", "id": "B", "contribution": " Right ."}, {"turn": 526, "name": "PhD", "id": "D", "contribution": " or {vocalsound} {pause} uh ,"}, {"turn": 527, "name": "PhD", "id": "E", "contribution": " No transform the PLP"}, {"turn": 528, "name": "PhD", "id": "D", "contribution": " per"}, {"turn": 529, "name": "PhD", "id": "E", "contribution": " and only transform the other I 'm not sure ."}, {"turn": 530, "name": "Professor", "id": "B", "contribution": " Well no ,"}, {"turn": 531, "name": "PhD", "id": "D", "contribution": " This is {pause} still something {pause} that"}, {"turn": 532, "name": "Professor", "id": "B", "contribution": " I think {disfmarker} I see ."}, {"turn": 533, "name": "PhD", "id": "D", "contribution": " yeah , we {pause} don't know {disfmarker}"}, {"turn": 534, "name": "Professor", "id": "B", "contribution": " So there 's a question of whether you would {disfmarker}"}, {"turn": 535, "name": "PhD", "id": "E", "contribution": " Two e @ @ it 's one ."}, {"turn": 536, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 537, "name": "Professor", "id": "B", "contribution": " Right . Whether you would transform together or just one . Yeah . Might wanna try it both ways . But that 's interesting . So that 's something that you 're {disfmarker} you haven't trained yet but are preparing to train , and {disfmarker}"}, {"turn": 538, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 539, "name": "Professor", "id": "B", "contribution": " Yeah . Um {pause} {pause} Yeah , so I think Hynek will be here Monday ."}, {"turn": 540, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 541, "name": "Professor", "id": "B", "contribution": "  Monday or Tuesday . So"}, {"turn": 542, "name": "PhD", "id": "D", "contribution": " Uh , yeah ."}, {"turn": 543, "name": "Professor", "id": "B", "contribution": " So I think , you know , we need to {pause} choose the {disfmarker} choose the experiments carefully , so we can get uh key {disfmarker} {pause} key questions answered {pause} uh before then"}, {"turn": 544, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 545, "name": "Professor", "id": "B", "contribution": " and {pause} leave other ones aside even if it {pause} leaves incomplete {pause} tables {vocalsound} {pause} someplace , uh {pause} uh , it 's {disfmarker} it 's really time to {disfmarker} {pause} time to choose ."}, {"turn": 546, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 547, "name": "Professor", "id": "B", "contribution": " Um , let me pass this out , {pause} by the way . Um These are {disfmarker} Did {disfmarker} did {disfmarker} {pause} did I interrupt you ?"}, {"turn": 548, "name": "PhD", "id": "E", "contribution": " Yeah , I have one ."}, {"turn": 549, "name": "Professor", "id": "B", "contribution": " Were there other things that you wanted to {disfmarker}"}, {"turn": 550, "name": "PhD", "id": "D", "contribution": " Uh , no . I don't think so ."}, {"turn": 551, "name": "PhD", "id": "E", "contribution": ""}, {"turn": 552, "name": "PhD", "id": "D", "contribution": " Yeah , I have one ."}, {"turn": 553, "name": "Grad", "id": "G", "contribution": " Oh , thanks ."}, {"turn": 554, "name": "Professor", "id": "B", "contribution": " Ah ! {pause} OK . {pause} OK , we have {pause} lots of them ."}, {"turn": 555, "name": "PhD", "id": "E", "contribution": " We have one ."}, {"turn": 556, "name": "Professor", "id": "B", "contribution": " OK , so {vocalsound} um , Something I asked {disfmarker} So they 're {disfmarker} they 're doing {pause} the {disfmarker} the VAD I guess they mean voice activity detection So again , it 's the silence {disfmarker} So they 've just trained up a net {pause} which has two outputs , I believe . Um {vocalsound} I asked uh {pause} Hynek whether {disfmarker} I haven't talked to Sunil {disfmarker} I asked Hynek whether {pause} they compared that to {pause} just taking the nets we already had {pause} and summing up the probabilities ."}, {"turn": 557, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 558, "name": "Professor", "id": "B", "contribution": " Uh . {pause} To get the speech {disfmarker} voice activity detection , or else just using the silence , {pause} if there 's only one {pause} silence output . Um {pause} And , he didn't think they had , um . But on the other hand , maybe they can get by with a smaller net and {pause} maybe {pause} sometimes you don't run the other , maybe there 's a computational advantage to having a separate net , anyway ."}, {"turn": 559, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 560, "name": "Professor", "id": "B", "contribution": " So um Their uh {disfmarker} {pause} the results look pretty good . Um , {pause} I mean , not uniformly ."}, {"turn": 561, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 562, "name": "Professor", "id": "B", "contribution": " I mean , there 's a {disfmarker} an example or two {pause} that you can find , where it made it slightly worse , but {pause} uh in {disfmarker} in all but a couple {pause} examples ."}, {"turn": 563, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 564, "name": "Professor", "id": "B", "contribution": " Uh ."}, {"turn": 565, "name": "PhD", "id": "E", "contribution": " But they have a question of the result . Um how are trained the {disfmarker} the LDA filter ? How obtained the LDA filter ?"}, {"turn": 566, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 567, "name": "Professor", "id": "B", "contribution": " I I 'm sorry . I don't understand your question ."}, {"turn": 568, "name": "PhD", "id": "E", "contribution": " Yes , um the LDA filter {pause} needs some {pause} training set {pause} to obtain the filter . Maybe I don't know exactly how {pause} they are obtained ."}, {"turn": 569, "name": "Professor", "id": "B", "contribution": " It 's on {pause} training ."}, {"turn": 570, "name": "PhD", "id": "E", "contribution": " Training , with the training test of each {disfmarker} You understand me ?"}, {"turn": 571, "name": "Professor", "id": "B", "contribution": " No ."}, {"turn": 572, "name": "PhD", "id": "E", "contribution": " Yeah , uh for example , {pause} LDA filter {pause} need a set of {disfmarker} {pause} a set of training {pause} to obtain the filter ."}, {"turn": 573, "name": "Professor", "id": "B", "contribution": " Yes ."}, {"turn": 574, "name": "PhD", "id": "E", "contribution": " And maybe {pause} for the Italian , for the TD {pause} TE on for Finnish , these filter are {disfmarker} are obtained with their own training set ."}, {"turn": 575, "name": "Professor", "id": "B", "contribution": " Yes , I don't know . That 's {disfmarker} that 's {disfmarker} so that 's a {disfmarker} that 's a very good question , then {disfmarker} now that it {disfmarker} {pause} I understand it . It 's \" yeah , where does the LDA come from ? \" In the {disfmarker} In {pause} earlier experiments , they had taken LDA {pause} from a completely different database , right ?"}, {"turn": 576, "name": "PhD", "id": "E", "contribution": " Yeah . Yeah , because maybe it the same situation that the neural network training with their own"}, {"turn": 577, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 578, "name": "PhD", "id": "E", "contribution": " set ."}, {"turn": 579, "name": "Professor", "id": "B", "contribution": " So that 's a good question . Where does it come from ? Yeah , I don't know . Um , {pause} but uh to tell you the {pause} truth , I wasn't actually looking at the LDA so much when I {disfmarker} I was looking at it I was {pause} mostly thinking about the {disfmarker} {pause} the VAD . And um , it ap {pause} it ap Oh what does {disfmarker} what does ASP ? Oh that 's {disfmarker}"}, {"turn": 580, "name": "PhD", "id": "D", "contribution": " The features , yeah . Yeah ."}, {"turn": 581, "name": "PhD", "id": "E", "contribution": " I don't understand also"}, {"turn": 582, "name": "Professor", "id": "B", "contribution": " It says \" baseline ASP \" ."}, {"turn": 583, "name": "PhD", "id": "E", "contribution": " what is {disfmarker} {pause} what is the difference between ASP and uh baseline over ?"}, {"turn": 584, "name": "PhD", "id": "C", "contribution": " ASP ."}, {"turn": 585, "name": "PhD", "id": "D", "contribution": " Yeah , I don't know ."}, {"turn": 586, "name": "PhD", "id": "E", "contribution": " This is {disfmarker}"}, {"turn": 587, "name": "Professor", "id": "B", "contribution": " Anybody know {pause} any {disfmarker}"}, {"turn": 588, "name": "PhD", "id": "C", "contribution": " Oh . There it is ."}, {"turn": 589, "name": "Professor", "id": "B", "contribution": " Um Cuz there 's \" baseline Aurora \" {pause} above it ."}, {"turn": 590, "name": "PhD", "id": "C", "contribution": " Mm - hmm ."}, {"turn": 591, "name": "Professor", "id": "B", "contribution": " And it 's {disfmarker} This is mostly better than baseline , although in some cases it 's a little worse , in a couple cases ."}, {"turn": 592, "name": "PhD", "id": "C", "contribution": " Well , it says baseline ASP is twenty - three mill {pause} minus thirteen ."}, {"turn": 593, "name": "PhD", "id": "E", "contribution": " Yeah ."}, {"turn": 594, "name": "Professor", "id": "B", "contribution": " Yeah , it says what it is . But I don't how that 's different {pause} from {disfmarker}"}, {"turn": 595, "name": "PhD", "id": "C", "contribution": " From the baseline . {comment} OK ."}, {"turn": 596, "name": "Professor", "id": "B", "contribution": " I think this was {disfmarker} {pause} I think this is the same point we were at when {disfmarker} when we were up in Oregon ."}, {"turn": 597, "name": "PhD", "id": "E", "contribution": " Yeah ."}, {"turn": 598, "name": "PhD", "id": "D", "contribution": " I think {disfmarker} {pause} I think it 's the C - zero {disfmarker} using C - zero instead of log energy ."}, {"turn": 599, "name": "PhD", "id": "E", "contribution": " Ah , OK , mm - hmm ."}, {"turn": 600, "name": "PhD", "id": "D", "contribution": " Yeah , it 's this ."}, {"turn": 601, "name": "Professor", "id": "B", "contribution": " Oh . OK ."}, {"turn": 602, "name": "PhD", "id": "E", "contribution": " yeah ."}, {"turn": 603, "name": "PhD", "id": "D", "contribution": " It should be that , yeah ."}, {"turn": 604, "name": "PhD", "id": "A", "contribution": " They s they say in here that the VAD is not used as an additional feature ."}, {"turn": 605, "name": "Professor", "id": "B", "contribution": " Shouldn't it be {disfmarker}"}, {"turn": 606, "name": "PhD", "id": "D", "contribution": " Because {disfmarker}"}, {"turn": 607, "name": "PhD", "id": "A", "contribution": " Does {disfmarker} does anybody know how they 're using it ?"}, {"turn": 608, "name": "Professor", "id": "B", "contribution": " Yeah . So {disfmarker} so what they 're doing here is , {pause} i"}, {"turn": 609, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 610, "name": "Professor", "id": "B", "contribution": " if you look down at the block diagram , {pause} um , {pause} they estimate {disfmarker} they get a {disfmarker} {pause} they get an estimate {pause} of whether it 's speech or silence ,"}, {"turn": 611, "name": "PhD", "id": "A", "contribution": " But that {disfmarker}"}, {"turn": 612, "name": "Professor", "id": "B", "contribution": " and then they have a median filter of it ."}, {"turn": 613, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 614, "name": "Professor", "id": "B", "contribution": " And so um , {pause} basically they 're trying to find stretches . The median filter is enforcing a {disfmarker} i it having some continuity ."}, {"turn": 615, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 616, "name": "Professor", "id": "B", "contribution": " You find stretches where the {pause} combination of the {pause} frame wise VAD and the {disfmarker} {pause} the median filter say that there 's a stretch of silence . And then it 's going through and just throwing the data away ."}, {"turn": 617, "name": "PhD", "id": "C", "contribution": " Hmm ."}, {"turn": 618, "name": "Professor", "id": "B", "contribution": " Right ? So um {disfmarker}"}, {"turn": 619, "name": "PhD", "id": "A", "contribution": " So it 's {disfmarker} it 's {disfmarker} I don't understand . You mean it 's throwing out frames ? Before {disfmarker}"}, {"turn": 620, "name": "Professor", "id": "B", "contribution": " It 's throwing out chunks of frames , yeah . There 's {disfmarker} the {disfmarker} the median filter is enforcing that it 's not gonna be single cases of frames , or isolated frames ."}, {"turn": 621, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 622, "name": "Professor", "id": "B", "contribution": " So it 's throwing out frames and the thing is {pause} um , {pause} what I don't understand is how they 're doing this with H T"}, {"turn": 623, "name": "PhD", "id": "A", "contribution": " Yeah , that 's what I was just gonna ask ."}, {"turn": 624, "name": "Professor", "id": "B", "contribution": " This is {disfmarker}"}, {"turn": 625, "name": "PhD", "id": "A", "contribution": " How can you just throw out frames ?"}, {"turn": 626, "name": "Professor", "id": "B", "contribution": " Yeah . Well , you {disfmarker} you can ,"}, {"turn": 627, "name": "PhD", "id": "D", "contribution": " i"}, {"turn": 628, "name": "Professor", "id": "B", "contribution": " right ? I mean y you {disfmarker} you {disfmarker}"}, {"turn": 629, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 630, "name": "Professor", "id": "B", "contribution": " it stretches again . For single frames I think it would be pretty hard ."}, {"turn": 631, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 632, "name": "Professor", "id": "B", "contribution": " But if you say speech starts here , speech ends there ."}, {"turn": 633, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 634, "name": "Professor", "id": "B", "contribution": " Right ?"}, {"turn": 635, "name": "PhD", "id": "C", "contribution": " Huh ."}, {"turn": 636, "name": "PhD", "id": "D", "contribution": " Yeah . Yeah , you can basically remove the {disfmarker} the frames from the feature {disfmarker} feature files ."}, {"turn": 637, "name": "Professor", "id": "B", "contribution": " Yeah . Yeah , so I mean in the {disfmarker} i i in the {disfmarker} in the decoding , you 're saying that we 're gonna decode from here to here ."}, {"turn": 638, "name": "PhD", "id": "D", "contribution": " I t"}, {"turn": 639, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 640, "name": "Professor", "id": "B", "contribution": " I think they 're {disfmarker} they 're {disfmarker} they 're treating it , {pause} you know , like uh {disfmarker} well , it 's not isolated word , but {disfmarker} but connected , you know , the {disfmarker} the {disfmarker}"}, {"turn": 641, "name": "PhD", "id": "A", "contribution": " In the text they say that this {disfmarker} this is a tentative block diagram of a possible configuration we could think of . So that sort of sounds like they 're not doing that yet ."}, {"turn": 642, "name": "Professor", "id": "B", "contribution": " Well . {pause} No they {disfmarker} they have numbers though , right ? So I think they 're {disfmarker} they 're doing something like that . I think that they 're {disfmarker} they 're {disfmarker} I think what I mean by tha that is they 're trying to come up with a block diagram that 's plausible for the standard . In other words , it 's {disfmarker} uh {disfmarker} I mean from the point of view of {disfmarker} of uh reducing the number of bits you have to transmit it 's not a bad idea to detect silence anyway ."}, {"turn": 643, "name": "PhD", "id": "A", "contribution": " Yeah . Yeah . I 'm just wondering what exactly did they do up in this table if it wasn't this ."}, {"turn": 644, "name": "Professor", "id": "B", "contribution": " Um . But it 's {disfmarker} the thing is it 's that {disfmarker} that {disfmarker} that 's {disfmarker} that 's I {disfmarker} I {disfmarker} Certainly it would be tricky about it intrans in transmitting voice , {pause} uh uh for listening to , is that these kinds of things {pause} uh cut {pause} speech off a lot ."}, {"turn": 645, "name": "PhD", "id": "A", "contribution": " Mm - hmm ."}, {"turn": 646, "name": "Professor", "id": "B", "contribution": " Right ? And so {pause} um"}, {"turn": 647, "name": "PhD", "id": "A", "contribution": " Plus it 's gonna introduce delays ."}, {"turn": 648, "name": "Professor", "id": "B", "contribution": " It does introduce delays but they 're claiming that it 's {disfmarker} it 's within the {disfmarker} {pause} the boundaries of it ."}, {"turn": 649, "name": "PhD", "id": "A", "contribution": " Mmm ."}, {"turn": 650, "name": "Professor", "id": "B", "contribution": " And the LDA introduces delays , and b {pause} what he 's suggesting this here is a parallel path so that it doesn't introduce {pause} uh , any more delay . I it introduces two hundred milliseconds of delay but at the same {pause} time the LDA {pause} down here {disfmarker} I don't know {disfmarker} Wh what 's the difference between TLDA and SLDA ?"}, {"turn": 651, "name": "PhD", "id": "C", "contribution": " Temporal and spectral ."}, {"turn": 652, "name": "Professor", "id": "B", "contribution": " Ah , thank you ."}, {"turn": 653, "name": "PhD", "id": "E", "contribution": " Temporal LDA ."}, {"turn": 654, "name": "Professor", "id": "B", "contribution": " Yeah , you would know that ."}, {"turn": 655, "name": "PhD", "id": "C", "contribution": " Yeah"}, {"turn": 656, "name": "Professor", "id": "B", "contribution": " So um . The temporal LDA does in fact include the same {disfmarker} so that {disfmarker} I think he {disfmarker} well , by {disfmarker} by saying this is a b a tentative block di diagram I think means {pause} if you construct it this way , this {disfmarker} this delay would work in that way"}, {"turn": 657, "name": "PhD", "id": "A", "contribution": " Ah ."}, {"turn": 658, "name": "Professor", "id": "B", "contribution": " and then it 'd be OK . They {disfmarker} they clearly did actually remove {pause} silent sections in order {disfmarker} because they {pause} got these {pause} word error rate {pause} results . So um I think that it 's {disfmarker} it 's nice to do that in this because in fact , it 's gonna give a better word error result and therefore will help within an evaluation . Whereas to whether this would actually be in a final standard , I don't know . Um . Uh , as you know , part of the problem with evaluation right now is that the {pause} word models are pretty bad and nobody wants {disfmarker} {pause} has {disfmarker} has approached improving them . So {pause} it 's possible that a lot of the problems {pause} with so many insertions and so forth would go away if they were better word models {pause} to begin with . So {pause} this might just be a temporary thing . But {disfmarker} But , on the other hand , and maybe {disfmarker} maybe it 's a decent idea . So um The question we 're gonna wanna go {pause} through next week when Hynek shows up I guess is given that we 've been {disfmarker} if you look at what we 've been trying , we 're uh looking at {pause} uh , by then I guess , combinations of features and multi - band Uh , and we 've been looking at {pause} cross - language , cross {pause} task {pause} issues . And they 've been not so much looking at {pause} the cross task uh multiple language issues . But they 've been looking at uh {disfmarker} {pause} at these issues . At the on - line normalization and the uh {pause} voice activity detection . And I guess when he comes here we 're gonna have to start deciding about {pause} um what do we choose {pause} from what we 've looked at {pause} to um blend with {pause} some group of things in what they 've looked at And once we choose that , {pause} how do we split up the {pause} effort ? Uh , because we still have {disfmarker} even once we choose , {pause} we 've still got {pause} uh another {pause} month or so , I mean there 's holidays in the way , but {disfmarker} but uh {pause} I think the evaluation data comes January thirty - first so there 's still a fair amount of time {pause} to do things together it 's just that they probably should be somewhat more coherent between the two sites {pause} in that {disfmarker} that amount of time ."}, {"turn": 659, "name": "PhD", "id": "A", "contribution": " When they removed the silence frames , did they insert some kind of a marker so that the recognizer knows it 's {disfmarker} {pause} knows when it 's time to back trace or something ?"}, {"turn": 660, "name": "Professor", "id": "B", "contribution": " Well , see they , I {disfmarker} I think they 're Um . I don't know the {disfmarker} {pause} the specifics of how they 're doing it . They 're {disfmarker} {pause} they 're getting around the way the recognizer works because they 're not allowed to {pause} um , change the scripts {pause} for the recognizer , {pause} I believe ."}, {"turn": 661, "name": "PhD", "id": "A", "contribution": " Oh , right . Maybe they 're just inserting some nummy frames or something ?"}, {"turn": 662, "name": "Professor", "id": "B", "contribution": " So . Uh . Uh , you know that 's what I had thought . But I don't {disfmarker} I don't think they are ."}, {"turn": 663, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 664, "name": "Professor", "id": "B", "contribution": " I mean that 's {disfmarker} sort of what {disfmarker} the way I had imagined would happen is that on the other side , yeah you p put some low level noise or something . Probably don't want all zeros ."}, {"turn": 665, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 666, "name": "Professor", "id": "B", "contribution": " Most recognizers don't like zeros but {vocalsound} but {pause} you know , {pause} put some epsilon in or some rand"}, {"turn": 667, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 668, "name": "Professor", "id": "B", "contribution": " sorry epsilon random variable {pause} in or something ."}, {"turn": 669, "name": "PhD", "id": "A", "contribution": " Some constant vector . I mean i w Or something {disfmarker}"}, {"turn": 670, "name": "Professor", "id": "B", "contribution": " Maybe not a constant but it doesn't , uh {disfmarker} don't like to divide by the variance of that , but I mean it 's"}, {"turn": 671, "name": "PhD", "id": "A", "contribution": " That 's right . But something that {disfmarker} what I mean is something that is {pause} very distinguishable from {pause} speech ."}, {"turn": 672, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 673, "name": "PhD", "id": "A", "contribution": " So that the {disfmarker} the silence model in HTK will always pick it up ."}, {"turn": 674, "name": "Professor", "id": "B", "contribution": " Yeah . So I {disfmarker} I {disfmarker} that 's what I thought they would do . or else , uh {pause} uh maybe there is some indicator to tell it to start and stop , I don't know ."}, {"turn": 675, "name": "PhD", "id": "A", "contribution": " Hmm ."}, {"turn": 676, "name": "Professor", "id": "B", "contribution": " But whatever they did , I mean they have to play within the rules of this specific evaluation ."}, {"turn": 677, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 678, "name": "Professor", "id": "B", "contribution": " We c we can find out ."}, {"turn": 679, "name": "PhD", "id": "A", "contribution": " Cuz you gotta do something . Otherwise , if it 's just a bunch of speech , stuck together {disfmarker}"}, {"turn": 680, "name": "Professor", "id": "B", "contribution": " No they 're {disfmarker}"}, {"turn": 681, "name": "PhD", "id": "A", "contribution": " Yeah ."}, {"turn": 682, "name": "Professor", "id": "B", "contribution": " It would do badly"}, {"turn": 683, "name": "PhD", "id": "A", "contribution": " Yeah , right ."}, {"turn": 684, "name": "Professor", "id": "B", "contribution": " and it didn't so badly , right ? So they did something ."}, {"turn": 685, "name": "PhD", "id": "A", "contribution": " Yeah , yeah ."}, {"turn": 686, "name": "Professor", "id": "B", "contribution": " Yeah . Uh . So , OK , So I think {pause} this brings me up to date a bit . It hopefully brings other {pause} people up to date a bit . And um Um {pause} I think {disfmarker} Uh , I wanna look at these numbers off - line a little bit and think about it and {disfmarker} {pause} and talk with everybody uh , {pause} outside of this meeting . Um , but uh No I mean it sounds like {disfmarker} I mean {pause} there {disfmarker} there {disfmarker} there are the usual number of {disfmarker} of {pause} little {disfmarker} little problems and bugs and so forth but it sounds like they 're getting ironed out . And now we 're {pause} seem to be kind of in a position to actually {pause} uh , {pause} look at stuff and {disfmarker} and {disfmarker} and compare things . So I think that 's {disfmarker} that 's pretty good . Um {pause} I don't know what the {disfmarker} One of the things I wonder about , {pause} coming back to the first results you talked about , is {disfmarker} is {pause} how much , {pause} uh {pause} things could be helped {pause} by more parameters . And uh {disfmarker} {pause} And uh how many more parameters we can afford to have , {vocalsound} {pause} in terms of the uh computational limits . Because anyway when we go to {pause} twice as much data {pause} and have the same number of parameters , particularly when it 's twice as much data and it 's quite diverse , um , I wonder if having twice as many parameters would help ."}, {"turn": 687, "name": "PhD", "id": "D", "contribution": " Mm - hmm ."}, {"turn": 688, "name": "Professor", "id": "B", "contribution": " Uh , just have a bigger hidden layer . Uh But {disfmarker} I doubt it would {pause} help by forty per cent . But {vocalsound} {pause} but uh"}, {"turn": 689, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 690, "name": "Professor", "id": "B", "contribution": " Just curious . How are we doing on the {pause} resources ? Disk , and {disfmarker}"}, {"turn": 691, "name": "PhD", "id": "D", "contribution": " I think we 're alright ,"}, {"turn": 692, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 693, "name": "PhD", "id": "D", "contribution": " um , {pause} not much problems with that ."}, {"turn": 694, "name": "Professor", "id": "B", "contribution": " Computation ?"}, {"turn": 695, "name": "PhD", "id": "D", "contribution": " It 's OK ."}, {"turn": 696, "name": "Professor", "id": "B", "contribution": " We {disfmarker}"}, {"turn": 697, "name": "PhD", "id": "D", "contribution": " Well this table took uh {pause} more than five days to get back ."}, {"turn": 698, "name": "Professor", "id": "B", "contribution": " Yeah . Yeah , well ."}, {"turn": 699, "name": "PhD", "id": "D", "contribution": " But {disfmarker} Yeah ."}, {"turn": 700, "name": "Professor", "id": "B", "contribution": " Are {disfmarker} were you folks using Gin ? That 's a {disfmarker} that just died , you know ?"}, {"turn": 701, "name": "PhD", "id": "D", "contribution": " Mmm , no . You were using Gin {comment} perhaps , yeah ? No ."}, {"turn": 702, "name": "PhD", "id": "E", "contribution": " No ."}, {"turn": 703, "name": "Professor", "id": "B", "contribution": " No ? Oh , that 's good ."}, {"turn": 704, "name": "Grad", "id": "F", "contribution": " It just died ."}, {"turn": 705, "name": "Professor", "id": "B", "contribution": " OK . Yeah , {pause} we 're gonna get a replacement {pause} server that 'll be a faster server , {pause} actually ."}, {"turn": 706, "name": "PhD", "id": "E", "contribution": " Yes ."}, {"turn": 707, "name": "Professor", "id": "B", "contribution": " That 'll be {disfmarker} It 's a {pause} seven hundred fifty megahertz uh SUN"}, {"turn": 708, "name": "PhD", "id": "D", "contribution": " Hmm . {comment} Mm - hmm ."}, {"turn": 709, "name": "Professor", "id": "B", "contribution": " uh {pause} But it won't be installed for {pause} a little while ."}, {"turn": 710, "name": "PhD", "id": "C", "contribution": " Tonic ."}, {"turn": 711, "name": "Professor", "id": "B", "contribution": " U Go ahead ."}, {"turn": 712, "name": "Grad", "id": "G", "contribution": " Do we {disfmarker} Do we have that big new IBM machine the , I think in th"}, {"turn": 713, "name": "Professor", "id": "B", "contribution": " We have the {pause} little tiny IBM machine {vocalsound} {pause} that might someday grow up to be a big {pause} IBM machine . It 's got s slots for eight , uh IBM was donating five , I think we only got two so far , processors . We had originally hoped we were getting eight hundred megahertz processors . They ended up being five fifty . So instead of having eight processors that were eight hundred megahertz , we ended up with two {pause} that are five hundred and fifty megahertz . And more are supposed to come soon and there 's only a moderate amount of dat of memory . So I don't think {pause} anybody has been sufficiently excited by it to {pause} spend much time {pause} uh {pause} with it , but uh {vocalsound} Hopefully , {pause} they 'll get us some more {pause} parts , soon and {disfmarker} Uh , yeah , I think that 'll be {disfmarker} once we get it populated , {pause} that 'll be a nice machine . I mean we will ultimately get eight processors in there . And uh {disfmarker} and uh a nice amount of memory . Uh so it 'll be a pr pretty fast Linux machine ."}, {"turn": 714, "name": "Grad", "id": "G", "contribution": " And if we can do things on Linux , {pause} some of the machines we have going already , like Swede ?"}, {"turn": 715, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 716, "name": "Grad", "id": "G", "contribution": " Um It seems pretty fast ."}, {"turn": 717, "name": "Professor", "id": "B", "contribution": " Mm - hmm ."}, {"turn": 718, "name": "Grad", "id": "G", "contribution": " But {disfmarker} I think Fudge is pretty fast too ."}, {"turn": 719, "name": "Professor", "id": "B", "contribution": " Yeah , I mean you can check with uh {pause} Dave Johnson . I mean , it {disfmarker} it 's {disfmarker} {pause} I think the machine is just sitting there . And it does have two processors , you know and {disfmarker} {pause} Somebody could do {disfmarker} {pause} you know , uh , check out {pause} uh the multi - threading {pause} libraries . And {pause} I mean i it 's possible that the {disfmarker} I mean , I guess the prudent thing to do would be for somebody to do the work on {disfmarker} {pause} on getting our code running {pause} on that machine with two processors {pause} even though there aren't five or eight . There 's {disfmarker} there 's {disfmarker} there 's gonna be debugging hassles and then we 'd be set for when we did have five or eight , to have it really be useful . But . {pause} Notice how I said somebody and {vocalsound} turned my head your direction . That 's one thing you don't get in these recordings . You don't get the {disfmarker} {pause} don't get the visuals but {disfmarker}"}, {"turn": 720, "name": "Grad", "id": "G", "contribution": " I is it um {pause} mostly um the neural network trainings that are {pause} um slowing us down or the HTK runs that are slowing us down ?"}, {"turn": 721, "name": "Professor", "id": "B", "contribution": " Uh , I think yes . Uh , {vocalsound} Isn't that right ? I mean I think you 're {disfmarker} you 're sort of held up by both , right ? If the {disfmarker} if the neural net trainings were a hundred times faster {pause} you still wouldn't {pause} be anything {disfmarker} running through these a hundred times faster because you 'd {pause} be stuck by the HTK trainings ,"}, {"turn": 722, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 723, "name": "Professor", "id": "B", "contribution": " right ?"}, {"turn": 724, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 725, "name": "Professor", "id": "B", "contribution": " But if the HTK {disfmarker} I mean I think they 're both {disfmarker} It sounded like they were roughly equal ? Is that about right ?"}, {"turn": 726, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 727, "name": "Professor", "id": "B", "contribution": " Yeah ."}, {"turn": 728, "name": "Grad", "id": "G", "contribution": " Because , um {pause} I think that 'll be running Linux , and Sw - Swede and Fudge are already running Linux so , {pause} um I could try to get {pause} um the train the neural network trainings or the HTK stuff running under Linux , and to start with I 'm {pause} wondering which one I should pick first ."}, {"turn": 729, "name": "Professor", "id": "B", "contribution": " Uh , probably the neural net cuz it 's probably {disfmarker} it {disfmarker} it 's {disfmarker} {pause} it 's um {disfmarker} Well , I {disfmarker} I don't know . They both {disfmarker} HTK we use for {pause} um {pause} this Aurora stuff Um {pause} Um , I think {pause} It 's not clear yet what we 're gonna use {pause} for trainings uh {disfmarker} Well , {pause} there 's the trainings uh {disfmarker} is it the training that takes the time , or the decoding ? Uh , is it about equal {pause} between the two ? For {disfmarker} for Aurora ?"}, {"turn": 730, "name": "PhD", "id": "D", "contribution": " For HTK ?"}, {"turn": 731, "name": "Professor", "id": "B", "contribution": " For {disfmarker} Yeah . For the Aurora ?"}, {"turn": 732, "name": "PhD", "id": "D", "contribution": " Uh Training is longer ."}, {"turn": 733, "name": "Professor", "id": "B", "contribution": " OK ."}, {"turn": 734, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 735, "name": "Professor", "id": "B", "contribution": " OK . Well , I don't know how we can {disfmarker} I don't know how to {disfmarker} Do we have HTK source ? Is that {disfmarker} Yeah ."}, {"turn": 736, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 737, "name": "Professor", "id": "B", "contribution": " You would think that would fairly trivially {disfmarker} the training would , anyway , th the testing {pause} uh I don't {disfmarker} I don't {pause} think would {pause} parallelize all that well . But I think {pause} that {pause} you could {pause} certainly do d um , {pause} distributed , sort of {disfmarker} {pause} Ah , no , it 's the {disfmarker} {pause} each individual {pause} sentence is pretty tricky to parallelize . But you could split up the sentences in a test set ."}, {"turn": 738, "name": "PhD", "id": "A", "contribution": " They have a {disfmarker} they have a thing for doing that and th they have for awhile , in H T And you can parallelize the training ."}, {"turn": 739, "name": "Professor", "id": "B", "contribution": " Yeah ?"}, {"turn": 740, "name": "PhD", "id": "A", "contribution": " And run it on several machines"}, {"turn": 741, "name": "Professor", "id": "B", "contribution": " Aha !"}, {"turn": 742, "name": "PhD", "id": "A", "contribution": " and it just basically keeps counts . And there 's something {disfmarker} {pause} a final {pause} thing that you run and it accumulates all the counts together ."}, {"turn": 743, "name": "Professor", "id": "B", "contribution": " I see ."}, {"turn": 744, "name": "PhD", "id": "D", "contribution": " Mmm ."}, {"turn": 745, "name": "PhD", "id": "A", "contribution": " I don't what their scripts are {pause} set up to do for the Aurora stuff , but {disfmarker}"}, {"turn": 746, "name": "PhD", "id": "D", "contribution": " Yeah ."}, {"turn": 747, "name": "Professor", "id": "B", "contribution": " Something that we haven't really settled on yet is other than {pause} this Aurora stuff , {pause} uh what do we do , large vocabulary {pause} training slash testing {pause} for uh tandem systems . Cuz we hadn't really done much with tandem systems for larger stuff . Cuz we had this one collaboration with CMU and we used SPHINX . Uh , we 're also gonna be collaborating with SRI and we have their {disfmarker} have theirs . Um {pause} So {pause} I don't know Um . So I {disfmarker} I think the {disfmarker} the advantage of going with the neural net thing is that we 're gonna use the neural net trainings , no matter what , for a lot of the things we 're doing ,"}, {"turn": 748, "name": "Grad", "id": "G", "contribution": " OK ."}, {"turn": 749, "name": "Professor", "id": "B", "contribution": " whereas , w exactly which HMM {disfmarker} Gaussian - mixture - based HMM thing we use is gonna depend uh So with that , maybe we should uh {vocalsound} go to our {nonvocalsound} digit recitation task . And , it 's about eleven fifty . Canned . Uh , I can {disfmarker} I can start over here . Great , uh , could you give Adam a call . Tell him to He 's at two nine seven seven ."}, {"turn": 750, "name": "Grad", "id": "F", "contribution": " Oh ."}, {"turn": 751, "name": "Professor", "id": "B", "contribution": " OK . I think we can {vocalsound} @ @ You know Herve 's coming tomorrow , right ? Herve will be giving a talk , yeah , talk at eleven . Did uh , did everybody sign these consent Er everybody Has everyone signed a consent form before , on previous meetings ? You don't have to do it again each time Yes . microphones off"}]}