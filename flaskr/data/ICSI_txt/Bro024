Turn 0, F (PhD): And we 're on .
Turn 1, D (Professor): OK . Might wanna {vocalsound} close the door so that {disfmarker} Uh , Stephane will {disfmarker}
Turn 2, F (PhD): I 'll get it .
Turn 3, D (Professor): Yeah
Turn 4, F (PhD): Hey Dave ? Could you go ahead and turn on , uh , Stephane 's {disfmarker}
Turn 5, C (Grad): Mm - hmm .
Turn 6, D (Professor): So that 's the virtual Stephane over there .
Turn 7, F (PhD): OK .
Turn 8, G (Professor): Do you use a PC for recording ? Or {disfmarker}
Turn 9, F (PhD): Uh , yeah , a Linux box . Yeah . It 's got , uh , like sixteen channels going into it .
Turn 10, G (Professor): Uh - huh . Uh - huh . The quality is quite good ? Or {disfmarker} ?
Turn 11, F (PhD): Mm - hmm . Yeah , so far , it 's been pretty good .
Turn 12, G (Professor): Mm - hmm .
Turn 13, D (Professor): Yeah . So , uh , yeah {disfmarker} the suggestion was to have these guys start to {disfmarker}
Turn 14, F (PhD): OK . Why don't you go ahead , Dave ?
Turn 15, C (Grad): OK . Um , so , yeah , the {disfmarker} this past week I 've been main mainly occupied with , um , getting some results , u from the SRI system trained on this short Hub - five training set for the mean subtraction method . And , um , I ran some tests last night . But , um , c the results are suspicious . Um , it 's , um , {vocalsound} cuz they 're {disfmarker} the baseline results are worse than , um , Andreas {disfmarker} than results Andreas got previously . And {vocalsound} it could have something to do with , um {disfmarker}
Turn 16, F (PhD): That 's on digits ?
Turn 17, C (Grad): That 's on digits . It c it {disfmarker} it could h it could have something to do with , um , downsampling .
Turn 18, F (PhD): Hmm .
Turn 19, C (Grad): That 's {disfmarker} that 's worth looking into . Um , d and , um , ap ap apart from that , I guess the {disfmarker} the main thing I have t ta I have to talk is , um , where I 'm planning to go over the next week . Um . So I 've been working on integrating this mean subtraction approach into the SmartKom system . And there 's this question of , well , so , um , in my tests before with HTK I found it worked {disfmarker} it worked the best with about twelve seconds of data used to estimate the mean , but , we 'll often have less {comment} in the SmartKom system . Um . So I think we 'll use as much data as we have {pause} at a particular time , and we 'll {disfmarker} {vocalsound} we 'll concatenate utterances together , um , to get as much data as we possibly can from the user . But , {vocalsound} um , {vocalsound} there 's a question of how to set up the models . So um , we could train the models . If we think twelve seconds is ideal we could train the models using twelve seconds to calculate the mean , to mean subtract the training data . Or we could , um , use some other amount . So {disfmarker} like I did an experiment where I , um , was using six seconds in test , um , but , for {disfmarker} I tried twelve seconds in train . And I tried , um , um , the same in train {disfmarker} I 'm a I tried six seconds in train . And six seconds in train {vocalsound} was about point three percent better . Um , and {disfmarker} {vocalsound} um , it 's not clear to me yet whether that 's {vocalsound} something significant . So I wanna do some tests and , um , {vocalsound} actually make some plots of , um {disfmarker} for a particular amount of data and test what happens if you vary the amount of data in train .
Turn 20, F (PhD): Mm - hmm .
Turn 21, D (Professor): Uh , Guenter , I don't know if you t {vocalsound} followed this stuff but this is , uh , {vocalsound} a uh , uh , long - term {disfmarker} long - term window F F Yeah . Yeah , he {disfmarker} you talked about it .
Turn 22, G (Professor): Yeah , we {disfmarker} we spoke about it already ,
Turn 23, D (Professor): Oh , OK . So you know what he 's doing .
Turn 24, G (Professor): yeah .
Turn 25, D (Professor): Alright .
Turn 26, C (Grad): y s so I was {disfmarker} I actually ran the experiments mostly and I {disfmarker} I was {disfmarker} I was hoping to have the plots with me today . I just didn't get to it . But , um {disfmarker} yeah , I wou I would be curious about people 's feedback on this cuz I 'm {disfmarker} {vocalsound} @ @ {comment} I p I think there are some I think it 's {disfmarker} it 's kind of like a {disfmarker} a bit of a tricky engineering problem . I 'm trying to figure out what 's the optimal way to set this up . So , um , {vocalsound} I 'll try to make the plots and then put some postscript up on my {disfmarker} on my web page . And I 'll mention it in my status report if people wanna take a look .
Turn 27, D (Professor): You could clarify something for me . You 're saying point three percent , you take a point three percent hit , {vocalsound} when the training and testing links are {disfmarker} don't match or something ?
Turn 28, E (PhD): Hello .
Turn 29, D (Professor): Is that what it is ?
Turn 30, C (Grad): w Well , it c
Turn 31, D (Professor): Or {disfmarker} ?
Turn 32, C (Grad): I {disfmarker} I don't think it {disfmarker} it 's {vocalsound} just for any mismatch {vocalsound} you take a hit .
Turn 33, D (Professor): Yeah .
Turn 34, C (Grad): i In some cases it might be u better to have a mismatch . Like I think I saw something like {disfmarker} like if you only have two seconds in test , or , um , maybe it was something like four seconds , you actually do a little better if you , um , {vocalsound} train on six seconds than if you train on four seconds .
Turn 35, D (Professor): Yeah . Right .
Turn 36, C (Grad): Um , but the case , uh {disfmarker} with the point three percent hit was {vocalsound} using six seconds in test , um , comparing train on twelve seconds {comment} versus train on six seconds .
Turn 37, D (Professor): And which was worse ?
Turn 38, C (Grad): The train on twelve seconds .
Turn 39, D (Professor): OK . But point three percent , uh , w from what to what ? That 's point three percent {disfmarker}
Turn 40, C (Grad): On {disfmarker} The {disfmarker} the {disfmarker} the accuracies {vocalsound} w went from {disfmarker} it was something vaguely like ninety - five point six accuracy , um , improved to ninety - five point nine wh when I {disfmarker}
Turn 41, D (Professor): So four point four to four point one .
Turn 42, C (Grad): OK .
Turn 43, D (Professor): So {disfmarker} yeah . So about a {disfmarker} about an eight percent , uh , seven or eight percent relative ?
Turn 44, C (Grad): OK .
Turn 45, D (Professor): Uh , Yeah . Well , I think in a p You know , if {disfmarker} if you were going for an evaluation system you 'd care . But if you were doing a live system that people were actually using nobody would notice . It 's {disfmarker} uh , I think the thing is to get something that 's practical , that {disfmarker} that you could really use .
Turn 46, C (Grad): Huh . That 's {disfmarker} that 's interesting . Alright , the e uh , I see your point . I guess I was thinking of it as , um , {vocalsound} an interesting research problem . The {disfmarker} how to g I was thinking that for the ASRU paper we could have a section saying , {vocalsound} " For SmartKom , we {disfmarker} we d in {disfmarker} we tried this approach in , uh , {vocalsound} interactive system " , which I don't think has been done before .
Turn 47, D (Professor): Yeah . Mm - hmm .
Turn 48, C (Grad): And {disfmarker} and then there was two research questions from that .
Turn 49, D (Professor): Mm - hmm .
Turn 50, C (Grad): And one is the k does it still work if you just use the past history ?
Turn 51, D (Professor): Mm - hmm .
Turn 52, C (Grad): Alright , and the other was this question of , um what I was just talking about now . So I guess that 's why I thought it was interesting .
Turn 53, D (Professor): I mean , a short - time FFT {disfmarker} short - time cepstrum calculation , uh , mean {disfmarker} u mean calculation work that people have in commercial systems , they do this all the time . They {disfmarker} the {disfmarker} they calculate it from previous utterances and then use it , you know .
Turn 54, C (Grad): Yeah , um .
Turn 55, D (Professor): But {disfmarker} but , uh , as you say , there hasn't been that much with this long {disfmarker} long - time , uh , spectra work .
Turn 56, C (Grad): Oh , o Oh , OK .
Turn 57, D (Professor): Uh ,
Turn 58, C (Grad): So that 's {disfmarker} that 's {disfmarker} that 's standard . Um {disfmarker}
Turn 59, D (Professor): Yeah . Pretty common .
Turn 60, C (Grad): OK .
Turn 61, D (Professor): Yeah . Um , but , u uh , yes . No , it is interesting . And the other thing is , I mean , there 's two sides to these really small , uh , gradations in performance . Um , I mean , on the one hand in a practical system if something is , uh , four point four percent error , four point one percent error , people won't really tell {disfmarker} be able to tell the difference . On the other hand , when you 're doing , uh , research , you may , eh {disfmarker} you might find that the way that you build up a change from a ninety - five percent accurate system to a ninety - eight percent accurate system is through ten or twelve little things that you do that each are point three percent . So {disfmarker} so the {disfmarker} they {disfmarker} they {disfmarker} it 's {disfmarker} I don't mean to say that they 're {disfmarker} they 're irrelevant . Uh , they are relevant . But , um , {vocalsound} i for a demo , you won't see it .
Turn 62, C (Grad): Mm - hmm . Right . OK .
Turn 63, D (Professor): Yeah .
Turn 64, C (Grad): And , um , Let 's {disfmarker} l let 's see . Um , OK . And then there 's um , another thing I wanna start looking at , um , {vocalsound} wi is , um , the choice of the analysis window length . So I 've just been using two seconds just because that 's what Carlos did before . Uh , I wrote to him asking about he chose the two seconds . And it seemed like he chose it a bit informally . So , um , with the {disfmarker} with the HTK set - up I should be able to do some experiments , on just varying that length , say between one and three seconds , in a few different reverberation conditions , um , say this room and also a few of the artificial impulse responses we have for reverberation , just , um , making some plots and seeing how they look . And , um , so , with the {disfmarker} the sampling rate I was using , one second or two seconds or four seconds is at a power of two um , number of samples and , um , I 'll {disfmarker} I 'll jus f for the ones in between I guess I 'll just zero - pad .
Turn 65, D (Professor): Mm - hmm . I guess one thing that might also be an issue , uh , cuz part of what you 're doing is you 're getting a {disfmarker} a spectrum over a bunch of different kinds of speech sounds . Um , and so it might matter how fast someone was talking for instance .
Turn 66, C (Grad): Oh .
Turn 67, D (Professor): You know , if you {disfmarker} if {disfmarker} if {disfmarker} if there 's a lot of phones in one second maybe you 'll get a {disfmarker} a really good sampling of all these different things , and {disfmarker} {vocalsound} and , uh , on the other hand if someone 's talking slowly maybe you 'd need more . So {disfmarker}
Turn 68, C (Grad): Huh .
Turn 69, D (Professor): I don't know if you have some samples of faster or slower speech but it might make a difference . I don't know .
Turn 70, C (Grad): Uh , yeah , I don't {disfmarker} I don't think the TI - digits data that I have , um , {vocalsound} i is {disfmarker} would be appropriate for that .
Turn 71, D (Professor): Yeah , probably not . Yeah .
Turn 72, C (Grad): But what do you {disfmarker} What about if I w I fed it through some kind of , um , speech processing algorithm that changed the speech rate ?
Turn 73, D (Professor): Yeah , but then you 'll have the degradation of {disfmarker} of , uh , whatever you do uh , added onto that . But maybe . Yeah , maybe if you get something that sounds {disfmarker} that {disfmarker} that 's {disfmarker} does a pretty job at that .
Turn 74, C (Grad): Yeah . Well , uh , just if you think it 's worth looking into .
Turn 75, D (Professor): You could imagine that .
Turn 76, C (Grad): I mean , it {disfmarker} it is getting a little away from reverberation .
Turn 77, D (Professor): Um , yeah . It 's just that you 're making a choice {disfmarker} uh , I was thinking more from the system aspect , if you 're making a choice for SmartKom , that {disfmarker} that {disfmarker} that it might be that it 's {disfmarker} it c the optimal number could be different , depending on {disfmarker}
Turn 78, C (Grad): Yeah . Right .
Turn 79, D (Professor): Could be . I don't know .
Turn 80, C (Grad): And {disfmarker} and th the third thing , um , uh , is , um , Barry explained LDA filtering to me yesterday . And so , um , Mike Shire in his thesis um , {vocalsound} did a {disfmarker} a series of experiments , um , training LDA filters in d on different conditions . And you were interested in having me repeat this for {disfmarker} for this mean subtraction approach ? Is {disfmarker} is that right ? Or for these long analysis windows , I guess , is the right way to put it .
Turn 81, D (Professor): I guess , the {disfmarker} the {disfmarker} the issue I was {disfmarker} the general issue I was bringing up was that if you 're {disfmarker} have a moving {disfmarker} {vocalsound} moving window , uh , a wa a {disfmarker} a set of weights times things that , uh , move along , shift along in time , that you have in fact a linear time invariant filter . And you just happened to have picked a particular one by setting all the weights to be equal . And so the issue is what are some other filters that you could use , uh , in that sense of " filter " ?
Turn 82, C (Grad): Mm - hmm .
Turn 83, D (Professor): And , um , as I was saying , I think the simplest thing to do is not to train anything , but just to do some sort of , uh , uh , hamming or Hanning , uh , kind of window , kind of thing ,
Turn 84, C (Grad): Right . Mm - hmm .
Turn 85, D (Professor): just sort of to de - emphasize the jarring . So I think that would sort of be the first thing to do . But then , yeah , the LDA i uh , is interesting because it would sort of say well , suppose you actually trained this up to do the best you could by some criterion , what would the filter look like then ?
Turn 86, C (Grad): Uh - huh .
Turn 87, D (Professor): Uh , and , um , that 's sort of what we 're doing in this Aur - Aurora stuff . And , uh , it 's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you 've trained up , because you always have the problem that it 's trained up for one condition and it isn't quite right for another . So . uh {disfmarker} that 's {disfmarker} that 's why {disfmarker} that 's why RASTA filter has actually ended up lasting a long time , people still using it quite a bit , because y you don't change it . So doesn't get any worse . Uh ,
Turn 88, C (Grad): Huh .
Turn 89, D (Professor): Anyway .
Turn 90, C (Grad): o OK . So , um , a actually I was just thinking about what I was asking about earlier , wi which is about having {vocalsound} less than say twelve seconds in the SmartKom system to do the mean subtraction . You said in {vocalsound} systems where you use cepstral mean subtraction , they concatenate utterances and , {vocalsound} do you know how they address this issue of , um , testing versus training ? Can {disfmarker}
Turn 91, D (Professor): Go ahead .
Turn 92, G (Professor): I think what they do is they do it always on - line , I mean , that you just take what you have from the past , that you calculate the mean of this and subtract the mean .
Turn 93, C (Grad): OK . Um {disfmarker}
Turn 94, G (Professor): And then you can {disfmarker} yeah , you {disfmarker} you can increase your window whi while you get {disfmarker} while you are getting more samples .
Turn 95, C (Grad): OK , um , and , um , so {disfmarker} so in tha in that case , wh what do they do when they 're t um , performing the cepstral mean subtraction on the training data ? So {disfmarker} because you 'd have hours and hours of training data . So do they cut it off and start over ? At intervals ? Or {disfmarker} ?
Turn 96, G (Professor): So do you have {disfmarker} uh , you {disfmarker} you mean you have files which are hours of hours long ? Or {disfmarker} ?
Turn 97, C (Grad): Oh , well , no . I guess not . But {disfmarker}
Turn 98, G (Professor): Yeah . I mean , usually you have in the training set you have similar conditions , I mean , file lengths are , I guess the same order or in the same size as for test data , or aren't they ?
Turn 99, C (Grad): OK . But it 's {disfmarker} OK . So if someone 's interacting with the system , though , uh , Morgan {disfmarker} uh , Morgan said that you would {vocalsound} tend to , um , {vocalsound} chain utterances together um , r
Turn 100, D (Professor): Well , I think what I was s I thought what I was saying was that , um , at any given point you are gonna start off with what you had from before .
Turn 101, C (Grad): Oh .
Turn 102, D (Professor): From {disfmarker} and so if you 're splitting things up into utterances {disfmarker} So , for instance , in a dialogue system , {comment} where you 're gonna be asking , uh , you know , th for some information , there 's some initial th something . And , you know , the first time out you {disfmarker} you might have some general average . But you {disfmarker} you d you don't have very much information yet . But at {disfmarker} after they 've given one utterance you 've got something . You can compute your mean cepstra from that ,
Turn 103, C (Grad): Mm - hmm .
Turn 104, D (Professor): and then can use it for the next thing that they say , uh , so that , you know , the performance should be better that second time . Um ,  and I think the heuristics of exactly how people handle that and how they handle their training I 'm sure vary from place to place . But I think the {disfmarker} ideally , it seems to me anyway , that you {disfmarker} you would wanna do the same thing in training as you do in test . But that 's {disfmarker} that 's just , uh , a prejudice . And I think anybody working on this with some particular task would experiment .
Turn 105, C (Grad): Right . I g I guess the question I had was , um , amount of data e u was the amount of data that you 'd give it to , um {vocalsound} update this estimate . Because say you {disfmarker} if you have say five thousand utterances in your training set , {vocalsound} um , and you {disfmarker} you keep the mean from the last utterance , by the time it gets to the five thousandth utterance {disfmarker}
Turn 106, D (Professor): No , but those are all different people with different {disfmarker} I mean , i in y So for instance , in {disfmarker} in the {disfmarker} in a telephone task , these are different phone calls . So you don't wanna @ @ {comment} chain it together from a {disfmarker} from a different phone call .
Turn 107, C (Grad): OK , so {disfmarker} so {disfmarker} so they would {disfmarker} g s
Turn 108, D (Professor): So it 's within speaker , within phone call ,
Turn 109, G (Professor): Yeah .
Turn 110, D (Professor): if it 's a dialogue system , it 's within whatever this characteristic you 're trying to get rid of is expected to be consistent over ,
Turn 111, G (Professor): Hmm .
Turn 112, C (Grad): r and it {disfmarker}
Turn 113, D (Professor): right ?
Turn 114, C (Grad): right . OK , so you 'd {disfmarker} you {disfmarker} and so in training you would start over at {disfmarker} at every new phone call or at every {vocalsound} new speaker . Yeah ,
Turn 115, D (Professor): Yeah .
Turn 116, C (Grad): OK .
Turn 117, D (Professor): Yeah . Now , {vocalsound} you know , maybe you 'd use something from the others just because at the beginning of a call you don't know anything , and so you might have some kind of general thing that 's your best guess to start with . But {disfmarker} So , s I {disfmarker} I {disfmarker} you know , a lot of these things are proprietary so we 're doing a little bit of guesswork here . I mean , what do comp what do people do who really face these problems in the field ? Well , they have companies and they don't tell other people exactly what they do .
Turn 118, C (Grad): R right .
Turn 119, D (Professor): But {disfmarker} but I mean , when you {disfmarker} the {disfmarker} the hints that you get from what they {disfmarker} when they talk about it are that they do {disfmarker} they all do something like this .
Turn 120, C (Grad): Right , OK . I see . Bec - because I {disfmarker} so this SmartKom task first off , it 's this TV and movie information system .
Turn 121, D (Professor): Yeah , but you might have somebody who 's using it
Turn 122, C (Grad): And {disfmarker} Yeah .
Turn 123, D (Professor): and then later you might have somebody else who 's using it .
Turn 124, C (Grad): Yeah . Right . Right . I {disfmarker} I see .
Turn 125, D (Professor): And so you 'd wanna set some {disfmarker}
Turn 126, C (Grad): I was {disfmarker} I was about to say . So if {disfmarker} if you ask it " What {disfmarker} what movies are on TV tonight ? " ,
Turn 127, D (Professor): Yeah . Yeah .
Turn 128, C (Grad): if I look at my wristwatch when I say that it 's about two seconds . The way I currently have the mean subtraction , um , set up , the {disfmarker} the analysis window is two seconds .
Turn 129, D (Professor): Yeah .
Turn 130, C (Grad): So what you just said , about what do you start with , raises a question of {vocalsound} what do I start with then ?
Turn 131, D (Professor): Mm - hmm .
Turn 132, C (Grad): I guess it {disfmarker} because {disfmarker}
Turn 133, D (Professor): Well , w OK , so in that situation , though , th maybe what 's a little different there , is I think you 're talking about {disfmarker} there 's only one {disfmarker} it {disfmarker} it {disfmarker} it also depends {disfmarker} we 're getting a little off track here .
Turn 134, C (Grad): Oh , right .
Turn 135, D (Professor): r But {disfmarker} but {disfmarker} but {disfmarker} Uh , there 's been some discussion about whether the work we 're doing in that project is gonna be for the kiosk or for the mobile or for both . And I think for this kind of discussion it matters . If it 's in the kiosk , then the physical situation is the same . It 's gonna {disfmarker} you know , the exact interaction of the microphone 's gonna differ depending on the person and so forth . But at least the basic acoustics are gonna be the same . So f if it 's really in one kiosk , then I think that you could just chain together and {disfmarker} and you know , as much {disfmarker} as much speech as possible to {disfmarker} because what you 're really trying to get at is the {disfmarker} is the reverberation characteristic .
Turn 136, C (Grad): Yeah .
Turn 137, D (Professor): But in {disfmarker} in the case of the mobile , uh , {comment} presumably the acoustic 's changing all over the place .
Turn 138, C (Grad): Right .
Turn 139, D (Professor): And in that case you probably don't wanna have it be endless because you wanna have some sort of {disfmarker} it 's {disfmarker} it 's not a question of how long do you think it 's {disfmarker} you can get an approximation to a stationary something , given that it 's not really stationary .
Turn 140, C (Grad):  Right . Right .
Turn 141, D (Professor): So .
Turn 142, G (Professor): Hmm .
Turn 143, C (Grad): And I {disfmarker} I g I guess I s just started thinking of another question , which is , {vocalsound} for {disfmarker} for the very first frame , w what {disfmarker} what do I do if I 'm {disfmarker} if I take {disfmarker} if I use that frame to calculate the mean , then I 'm just gonna get n nothing .
Turn 144, D (Professor): Mm - hmm .
Turn 145, C (Grad): Um ,
Turn 146, D (Professor): Right .
Turn 147, C (Grad): so I should probably have some kind of default {vocalsound} mean for the first f couple of frames ?
Turn 148, D (Professor): Yeah . Yeah .
Turn 149, C (Grad): OK .
Turn 150, D (Professor): Yeah . Or subtract nothing . I mean , it 's {disfmarker}
Turn 151, C (Grad): Or subtract nothing . And {disfmarker} and that 's {disfmarker} that 's {disfmarker} I guess that 's something that 's p people have figured out how to deal with in cepstral mean subtraction as well ?
Turn 152, D (Professor): Yeah , yeah . Yeah , people do something . They {disfmarker} they , uh , they have some , um , uh , in {disfmarker} in cepstral mean subtraction , for short - term window {disfmarker} analysis windows , as is usually done , you 're trying to get rid of some very general characteristic . And so , uh , if you have any other information about what a general kind of characteristic would be , then you {disfmarker} you can do it there .
Turn 153, F (PhD): You can also {disfmarker} you can also reflect the data . So you take , uh {disfmarker} you know , I 'm not sure how many frames you need .
Turn 154, C (Grad): Uh - huh .
Turn 155, F (PhD): But you take that many from the front and flip it around to {disfmarker} a as the negative value .
Turn 156, D (Professor): Yeah , that 's {disfmarker} Yeah .
Turn 157, F (PhD): So you can always {disfmarker}
Turn 158, D (Professor): The other thing is that {disfmarker} and {disfmarker} and {disfmarker} I {disfmarker} I remember B B N doing this , is that if you have a multi - pass system , um , if the first pass ta it takes most of the computation , the second and the third pass could be very , very quick ,
Turn 159, C (Grad): Mmm .
Turn 160, D (Professor): just looking at a relatively small n small , uh , space of hypotheses .
Turn 161, C (Grad): Uh - huh .
Turn 162, D (Professor): Then you can do your first pass {vocalsound} without any subtraction at all .
Turn 163, C (Grad): Oh .
Turn 164, D (Professor): And then your second pass , uh , uh , eliminates those {disfmarker} most of those hypotheses by , uh {disfmarker} by having an improved {disfmarker} improved version o of the analysis .
Turn 165, C (Grad): OK . OK .
Turn 166, D (Professor): So .
Turn 167, C (Grad): OK . So that was all I had , for now .
Turn 168, D (Professor): Yeah .
Turn 169, F (PhD): Do you wanna go , Barry ?
Turn 170, A (Grad): Yeah , OK . Um , so for the past , {vocalsound} uh , week an or two , I 've been just writing my , uh , formal thesis proposal . Um , so I 'm taking {vocalsound} this qualifier exam that 's coming up in two weeks . And I {disfmarker} I finish writing a proposal and submit it to the committee . Um . And uh , should I {disfmarker} should I explain , uh , more about what {disfmarker} what I 'm proposing to do , and s and stuff ?
Turn 171, D (Professor): Yes , briefly .
Turn 172, F (PhD): Yeah briefly .
Turn 173, A (Grad): OK . Um , so briefly , {vocalsound} I 'm proposing to do a n a new p approach to speech recognition using um , a combination of , uh , multi - band ideas and ideas , um , {vocalsound} {vocalsound} {comment} about the uh , acoustic phonec phonetic approach to speech recognition . Um , so I will be using {vocalsound} these graphical models that {disfmarker} um , that implement the multi - band approach {vocalsound} to recognize a set of intermediate categories that might involve , uh , things like phonetic features {vocalsound} or other {disfmarker} other f feature things that are more closely related to the acoustic signal itself . Um , and the hope in all of this is that by going multi - band and by going into these , {vocalsound} um intermediate classifications , {vocalsound} that we can get a system that 's more robust to {disfmarker} to unseen noises , and situations like that . Um , and so , some of the research issues involved in this are , {vocalsound} um , {vocalsound} {comment} one , what kind of intermediate categories do we need to classify ? Um , another one is {vocalsound} um , what {disfmarker} what other types of structures in these multi - band graphical models should we consider in order to um , combine evidence from {vocalsound} the sub - bands ? And , uh , the third one is how do we {disfmarker} how do we merge all the , uh , information from the individual uh , multi - band classifiers to come up with word {disfmarker} word recognition or {disfmarker} or phone recognition things . Um , so basically that 's {disfmarker} that 's what I 've been doing . And ,
Turn 174, F (PhD): So you 've got two weeks , huh ?
Turn 175, A (Grad): I got two weeks to brush up on d um , presentation stuff and , um ,
Turn 176, D (Professor): Oh , I thought you were finishing your thesis in two weeks .
Turn 177, A (Grad): But . Oh , that too .
Turn 178, D (Professor): Yeah .
Turn 179, A (Grad): Yeah .
Turn 180, F (PhD): Are you gonna do any dry runs for your thing ,
Turn 181, A (Grad): Yes .
Turn 182, F (PhD): or are you just gonna {disfmarker}
Turn 183, A (Grad): Yes . I , um {disfmarker} I 'm {disfmarker} I 'm gonna do some . Would you be interested ? To help out ?
Turn 184, F (PhD): Sure .
Turn 185, A (Grad): OK .
Turn 186, F (PhD): Sure .
Turn 187, A (Grad): Thanks . Yeah .
Turn 188, F (PhD): Is that it ?
Turn 189, A (Grad): That 's it .
Turn 190, F (PhD): Hhh . OK . Uh . Hhh . Let 's see . So we 've got forty minutes left , and it seems like there 's a lot of material . An - any suggestions about where we {disfmarker} where we should go next ?
Turn 191, B (PhD): Mmm , @ @ .
Turn 192, F (PhD): Uh . Do you wanna go , Sunil ? Maybe we 'll just start with you .
Turn 193, B (PhD): Yeah . But I actually stuck most of this in our m last meeting with Guenter . Um , but I 'll just {disfmarker} Um , so the last week , uh , I showed some results with only SpeechDat - Car which was like some fifty - six percent . And , uh , I didn't h I mean , I {disfmarker} I found that the results {disfmarker} I mean , I wasn't getting that r results on the TI - digit . So I was like looking into " why , what is wrong with the TI - digits ? " . Why {disfmarker} why I was not getting it . And I found that , the noise estimation is a reason for the TI - digits to perform worse than the baseline . So , uh , I actually , picked th I mean , the first thing I did was I just scaled the noise estimate by a factor which is less than one to see if that {disfmarker} because I found there are a lot of zeros in the spectrogram for the TI - digits when I used this approach . So the first thing I did was I just scaled the noise estimate . And I found {disfmarker} So the {disfmarker} the results that I 've shown here are the complete results using the new {disfmarker} Well , the n the new technique is nothing but the noise estimate scaled by a factor of point five . So it 's just an ad - hoc {disfmarker} I mean , some intermediate result , because it 's not optimized for anything . So the results {disfmarker} The trend {disfmarker} the only trend I could see from those results was like the {disfmarker} the p the current noise estimation or the , uh , noise composition scheme is working good for like the car noise type of thing . Because I 've {disfmarker} the only {disfmarker} only {disfmarker} p very good result in the TI - digits is the noise {disfmarker} car noise condition for their test - A , which is like the best I could see that uh , for any non - stationary noise like " Babble " or " Subway " or any {disfmarker} " Street " , some " Restaurant " noise , it 's like {disfmarker} it 's not performing w very well . So , the {disfmarker} {vocalsound} So that {disfmarker} that 's the first thing I c uh , I could make out from this stuff . And {disfmarker}
Turn 194, G (Professor): Yeah , I think what is important to see is that there is a big difference between the training modes .
Turn 195, B (PhD): Yeah .
Turn 196, G (Professor): Uh - huh . If you have clean training , you get also a fifty percent improvement .
Turn 197, B (PhD): Yeah .
Turn 198, G (Professor): But if you have muddy condition training you get only twenty percent .
Turn 199, B (PhD): Yeah . Yeah .
Turn 200, E (PhD): Mm - hmm .
Turn 201, B (PhD): Uh , and in that twenty percent @ @ it 's very inconsistent across different noise conditions .
Turn 202, G (Professor): Mm - hmm . Mmm .
Turn 203, B (PhD): So I have like a forty - five {vocalsound} percent for " Car noise " and then there 's a minus five percent for the " Babble " ,
Turn 204, G (Professor): Mmm .
Turn 205, B (PhD): and there 's this thirty - three for the " Station " . And so {vocalsound} it 's {disfmarker} it 's not {disfmarker} it 's not actually very consistent across . So . The only correlation between the SpeechDat - Car and this performance is the c stationarity of the noise that is there in these conditions and the SpeechDat - Car .
Turn 206, G (Professor): Mm - hmm .
Turn 207, B (PhD): And , uh {disfmarker} so {disfmarker} so the overall result is like in the last page , which is like forty - seven , which is still very imbalanced because there are like fifty - six percent on the SpeechDat - Car and thirty - five percent on the TI - digits . And {disfmarker} uh , ps the fifty - six percent is like comparable to what the French Telecom gets , but the thirty - five percent is way off .
Turn 208, D (Professor): I 'm sort of confused but {disfmarker} this {disfmarker} I 'm looking on the second page ,
Turn 209, B (PhD): Oh , yep .
Turn 210, D (Professor): and it says " fifty percent " {disfmarker} looking in the lower right - hand corner , " fifty percent relative performance " .
Turn 211, G (Professor): For the clean training .
Turn 212, D (Professor): Is that {disfmarker}
Turn 213, G (Professor): u And if you {disfmarker} if you look {disfmarker}
Turn 214, D (Professor): is that fifty percent improvement ?
Turn 215, B (PhD): Yeah . For {disfmarker} that 's for the clean training and the noisy testing for the TI - digits .
Turn 216, G (Professor): Yeah .
Turn 217, D (Professor): So it 's improvement over the baseline mel cepstrum ?
Turn 218, B (PhD): Yeah . Yeah .
Turn 219, D (Professor): But the baseline mel cepstrum under those training doesn't do as well I {disfmarker} I 'm {disfmarker} I 'm trying to understand why it 's {disfmarker} it 's eighty percent {disfmarker} That 's an accuracy number , I guess ,
Turn 220, B (PhD): Yeah , yeah , yeah .
Turn 221, D (Professor): right ? So that 's not as good as the one up above .
Turn 222, B (PhD): No .
Turn 223, D (Professor): But the fifty is better than the one up above ,
Turn 224, B (PhD): Yeah .
Turn 225, D (Professor): so I 'm confused .
Turn 226, B (PhD): Uh , actually the noise compensation whatever , uh , we are put in it works very well for the high mismatch condition . I mean , it 's consistent in the SpeechDat - Car and in the clean training also it gives it {disfmarker} But this fifty percent is {disfmarker} is that the {disfmarker} the high mismatch performance {disfmarker} equivalent to the high mismatch performance in the speech .
Turn 227, F (PhD): So n s So since the high mismatch performance is much worse to begin with , it 's easier to get a better relative improvement .
Turn 228, B (PhD): Yeah . Yeah . I do . Yeah , yeah . So by putting this noise {disfmarker}
Turn 229, E (PhD): Yeah . Yeah , if we look at the figures on the right , we see that the reference system is very bad .
Turn 230, D (Professor): Oh .
Turn 231, B (PhD): Yeah . The reference drops like a very fast {disfmarker}
Turn 232, D (Professor): Oh , oh , oh , oh , oh , oh .
Turn 233, E (PhD): Like for clean {disfmarker} clean training condition .
Turn 234, D (Professor): I see .
Turn 235, B (PhD): Yeah .
Turn 236, D (Professor): I see .
Turn 237, E (PhD): Nnn .
Turn 238, D (Professor): This is {disfmarker} this is TI digits {comment} we 're looking at ?
Turn 239, B (PhD): Yeah . Yeah . Oh {disfmarker}
Turn 240, D (Professor): This whole page is TI - digits
Turn 241, B (PhD): Oh . Yeah .
Turn 242, D (Professor): or this is {disfmarker} ?
Turn 243, B (PhD): It 's not written anywhere . Yeah , it 's TI - digits . The first r spreadsheet is TI - digits .
Turn 244, D (Professor): Mmm . How does clean training do for the , uh , " Car "
Turn 245, G (Professor): Hmm .
Turn 246, B (PhD): The " Car " ?
Turn 247, D (Professor): stuff ?
Turn 248, B (PhD): Oh . Still {disfmarker} it still , uh {disfmarker} that {disfmarker} that 's still consistent . I mean , I get the best performance in the case of " Car " , which is the third column in the A condition .
Turn 249, D (Professor): No . I mean , this is added noise . I mean , this is TI - digits . I 'm sorry . I meant {disfmarker} in {disfmarker} in the {disfmarker} in the , uh , multi - language , uh , uh , Finnish and {disfmarker}
Turn 250, B (PhD): Uh {disfmarker}
Turn 251, G (Professor): This is next {disfmarker} next page .
Turn 252, B (PhD): That 's the next {disfmarker} next spreadsheet , is {disfmarker}
Turn 253, G (Professor): Hmm .
Turn 254, B (PhD): So that is the performance for Italian , Finnish and Spanish .
Turn 255, D (Professor): " Training condition " {disfmarker} Oh , right . So " clean " corresponds to " high mismatch " .
Turn 256, B (PhD): Yeah .
Turn 257, D (Professor): And " increase " , That 's increase e
Turn 258, G (Professor): Improvement .
Turn 259, B (PhD): Improvement . That 's {disfmarker} " Percentage increase " is the percentage improvement over the baseline .
Turn 260, G (Professor): Yeah . It 's {disfmarker} it 's a {disfmarker}
Turn 261, B (PhD): So that 's {disfmarker}
Turn 262, D (Professor): Which means decrease in word error rate ?
Turn 263, B (PhD): Yeah .
Turn 264, D (Professor): OK , so " percentage increase " means decrease ?
Turn 265, B (PhD): Yeah , yeah .
Turn 266, D (Professor): OK .
Turn 267, G (Professor): Yeah . The {disfmarker} the w there was a very long discussion about this on {disfmarker} on the {disfmarker} on the , uh , Amsterdam meeting .
Turn 268, D (Professor): Yeah .
Turn 269, G (Professor): How to {disfmarker} how to calculate it then .
Turn 270, B (PhD): Yeah . There 's {disfmarker} there 's a {disfmarker}
Turn 271, G (Professor): I {disfmarker} I {disfmarker} I guess you are using finally this {disfmarker} the scheme which they {disfmarker}
Turn 272, B (PhD): Which is there in the spreadsheet .
Turn 273, G (Professor): OK .
Turn 274, B (PhD): I 'm not changing anything in there .
Turn 275, G (Professor): Mmm .
Turn 276, D (Professor): Alright .
Turn 277, B (PhD): So . Uh , yeah . So all the hi H M numbers are w very good , in the sense , they are better than what the French Telecom gets . So . But the {disfmarker} the only number that 's still {disfmarker} I mean , which Stephane also got in his result was that medium mismatch of the Finnish , which is very {disfmarker} {vocalsound} which is a very strange situation where we used the {disfmarker} we changed the proto for initializing the HMM {disfmarker} I mean , this {disfmarker} this is basically because it gets stuck in some local minimum in the training . That seventy - five point seven nine in the Finnish mismatch which is that {disfmarker} the eleven point nine six what we see .
Turn 278, D (Professor): Uh - huh .
Turn 279, G (Professor): Mmm .
Turn 280, B (PhD): Yeah .
Turn 281, D (Professor): So we have to jiggle it somehow ?
Turn 282, B (PhD): Yeah {disfmarker} so we start with that different proto and it becomes eighty - eight , which is like some fifty percent improvement .
Turn 283, D (Professor): S Wait a minute . Start with a different what ?
Turn 284, B (PhD): Different prototype , which is like a different initialization for the , uh , s transition probabilities . It 's just that right now , the initialization is to stay more in the current state , which is point four point six , right ? Yeah .
Turn 285, E (PhD): Yeah .
Turn 286, B (PhD): And if it changes to point five point five , which is equal @ @ for transition and self loop where it becomes eighty - eight percent .
Turn 287, F (PhD): Well , but that involves mucking with the back - end ,
Turn 288, B (PhD): Yeah . We can't do it .
Turn 289, F (PhD): which is not allowed .
Turn 290, B (PhD): Yeah .
Turn 291, E (PhD): Mmm .
Turn 292, F (PhD): Yeah .
Turn 293, B (PhD): So .
Turn 294, G (Professor): I mean , it uh , like , i i i It is well known , this {disfmarker} this medium match condition of the Finnish data has some strange effects .
Turn 295, B (PhD): Very s
Turn 296, E (PhD): Yeah .
Turn 297, B (PhD): It has a very few at {disfmarker} uh , actually , c uh , tran I mean , words also .
Turn 298, G (Professor): I mean , that is {disfmarker} Yeah ,
Turn 299, B (PhD): It 's a very , very small set , actually .
Turn 300, G (Professor): that too . Yeah . Uh - huh .
Turn 301, B (PhD): So there is {disfmarker}
Turn 302, G (Professor): There is a l a {disfmarker} There is a lot of {disfmarker} Uh , there are a lot of utterances with music in {disfmarker} with music in the background .
Turn 303, B (PhD): Yeah . Yeah , yeah , yeah . Yeah .
Turn 304, G (Professor): Mmm .
Turn 305, D (Professor): Uh - huh .
Turn 306, B (PhD): Yeah . It has some music also . I mean , very horrible music like like I know .
Turn 307, D (Professor): So maybe for that one you need a much smarter VAD ? Mmm ,
Turn 308, B (PhD): Uh {disfmarker}
Turn 309, D (Professor): if it 's music .
Turn 310, B (PhD): So , that {disfmarker} that 's the {disfmarker} that 's about the results . And , uh , the summary is like {disfmarker} OK . So there are {disfmarker} the other thing what I tried was , which I explained in the last meeting , is using the channel zero for , uh , for both dropping and estimating the noise . And that 's like just to f n get a feel of how good it is . I guess the fifty - six percent improvement in the SpeechDat - Car becomes like sixty - seven percent . Like ten percent better . But that 's {disfmarker} that 's not a {disfmarker} that 's a cheating experiment . So . That 's just {disfmarker} So , m w
Turn 311, G (Professor): But the {disfmarker} but the , uh , forty - seven point nine percent which you have now , that 's already a remarkable improvement in comparison to the first proposal .
Turn 312, B (PhD): Yeah . So we had forty - four percent in the first proposal .
Turn 313, G (Professor): OK .
Turn 314, B (PhD): Yeah .
Turn 315, G (Professor): Mm - hmm .
Turn 316, B (PhD): We have f a big im So {vocalsound} the major improvement that we got was in all the high mismatch cases , because all those numbers were in sixties and seventies because we never had any noise compensations .
Turn 317, G (Professor): Mmm .
Turn 318, B (PhD): So that 's where the biggest improvement came up . Not much in the well match and the medium match and TI - digits also right now . So this is still at three or four percent improvement over the first proposal .
Turn 319, G (Professor): Mmm . Mmm .
Turn 320, D (Professor): Yeah , so that 's good .
Turn 321, B (PhD): Yeah . So .
Turn 322, D (Professor): Then if we can improve the noise estimation , then it should get better .
Turn 323, G (Professor): Yeah , I {disfmarker} I started thinking about also {disfmarker} I mean yeah , uh , {vocalsound} I discovered the same problem when I started working on {disfmarker} uh , on this Aurora task {vocalsound} almost two years ago , that you have the problem with this mulit a at the beginning we had only this multi condition training of the TI - digits .
Turn 324, B (PhD): Yeah .
Turn 325, G (Professor): And , uh , I {disfmarker} I found the same problem . Just taking um , what we were used to u {vocalsound} use , I mean , uh , some type of spectral subtraction , {comment} y {vocalsound} you get even worse results than {vocalsound} the basis
Turn 326, B (PhD): Yeah . Yeah ,
Turn 327, G (Professor): and uh {disfmarker}
Turn 328, B (PhD): yeah .
Turn 329, G (Professor): I {disfmarker} I tried to find an explanation for it ,
Turn 330, D (Professor): Mmm .
Turn 331, G (Professor): so {disfmarker}
Turn 332, B (PhD): So . Yes . Stephane also has the same experience of using the spectral subtraction right ?
Turn 333, G (Professor): Mmm .
Turn 334, E (PhD): Mm - hmm .
Turn 335, B (PhD): Yeah . So here {disfmarker} here I mean , I found that it 's {disfmarker} if I changed the noise estimate I could get an improvement .
Turn 336, E (PhD): Yeah .
Turn 337, B (PhD): So that 's {disfmarker} so it 's something which I can actually pursue , is the noise estimate .
Turn 338, G (Professor): Mm - hmm .
Turn 339, B (PhD): And {disfmarker}
Turn 340, G (Professor): Yeah , I think what you do is in {disfmarker} when {disfmarker} when you have the {disfmarker} the {disfmarker} this multi - condition training mode , um then you have {disfmarker} then you can train models for the speech , for the words , as well as for the pauses where you really have all information about the noise available .
Turn 341, B (PhD): Yeah .
Turn 342, G (Professor): And it was surprising {disfmarker} At the beginning it was not surprising to me that you get really the best results on doing it this way , I mean , in comparison to any type of training on clean data and any type of processing . But it was {disfmarker} So , u u it {disfmarker} it seems to be the best what {disfmarker} wh wh what {disfmarker} what we can do in this moment is multi - condition training . And every when we now start introducing some {disfmarker} some noise reduction technique we {disfmarker} we introduce also somehow artificial distortions .
Turn 343, B (PhD): Yeah .
Turn 344, G (Professor): And these artificial distortions {disfmarker} uh , I have the feeling that they are the reason why {disfmarker} why we have the problems in this multi - condition training . That means the H M Ms we trained , they are {disfmarker} they are based on Gaussians ,
Turn 345, B (PhD): Yeah .
Turn 346, G (Professor): and on modeling Gaussians . And if you {disfmarker} Can I move a little bit with this ? Yeah . And if we introduce now this {disfmarker} this u spectral subtraction , or Wiener filtering stuff {disfmarker} So , usually what you have is maybe , um {disfmarker} I 'm {disfmarker} I 'm showing now an envelope um maybe you 'll {disfmarker} f for this time . So usually you have {disfmarker} maybe in clean condition you have something which looks like this . And if it is noisy it is somewhere here . And then you try to subtract it or Wiener filter or whatever . And what you get is you have always these problems , that you have this {disfmarker} these {disfmarker} these {disfmarker} these zeros in there .
Turn 347, B (PhD): Yeah .
Turn 348, G (Professor): And you have to do something if you get these negative values . I mean , this is your noise estimate and you somehow subtract it or do whatever . Uh , and then you have {disfmarker} And then I think what you do is you introduce some {disfmarker} some artificial distribution in this uh in {disfmarker} in the models . I mean , i you {disfmarker} you train it also this way but , i somehow there is {disfmarker} u u there is no longer a {disfmarker} a Gaussian distribution . It is somehow a strange distribution which we introduce with these {vocalsound} artificial distortions . And {disfmarker} and I was thinking that {disfmarker} that might be the reason why you get these problems in the {disfmarker} especially in the multi - condition training mode .
Turn 349, B (PhD): Yeah , yeah .
Turn 350, D (Professor): Mm - hmm .
Turn 351, B (PhD): Th - That 's true . Yeah {disfmarker} the c the models are not complex enough to absorb that additional variability that you 're introducing .
Turn 352, G (Professor): s
Turn 353, F (PhD): Thanks Adam .
Turn 354, G (Professor): Yeah . Yes .
Turn 355, B (PhD): Well , that 's {disfmarker} Yeah . So {disfmarker}
Turn 356, E (PhD): I also have the feeling that um , the reason ye why it doesn't work is {disfmarker} yeah , that the models are much {disfmarker} are t um , not complex enough . Because I {disfmarker} actually I als always had a good experience with spectral subtraction , just a straight spectral subtraction algorithm when I was using neural networks , big neural networks , which maybe are more able to model strange distributions and {disfmarker}
Turn 357, G (Professor): Mm - hmm .
Turn 358, E (PhD): But {disfmarker} Yeah . Then I tried the same {disfmarker} exactly the same spectral subtraction algorithm on these Aurora tasks and it simply doesn't work . It 's even {disfmarker} it , uh , hurts even .
Turn 359, G (Professor): Hmm .
Turn 360, E (PhD): So .
Turn 361, D (Professor): We probably should at some point here try the tandem {disfmarker} the {disfmarker} the {disfmarker} the system - two kind of stuff with this , with the spectral subtraction for that reason .
Turn 362, G (Professor): Hmm .
Turn 363, D (Professor): Cuz {vocalsound} again , it should do a transformation to a domain where it maybe {disfmarker} looks more Gaussian .
Turn 364, G (Professor): Mm - hmm .
Turn 365, E (PhD): Mm - hmm .
Turn 366, G (Professor): Hmm . Yeah , y I {disfmarker} I was {disfmarker} whe w w just yesterday when I was thinking about it {vocalsound} um w what {disfmarker} what we could try to do , or do about it {disfmarker} I mean , if you {disfmarker} if you get at this {disfmarker} in this situation that you get this {disfmarker} this negative values and you simply set it to zero or to a constant or whatever {vocalsound} if we {disfmarker} if we would use there a somehow , um {disfmarker} a random generator which {disfmarker} which has a certain distribution , u not a certain {disfmarker} {comment} yeah , a special distribution we should see {disfmarker} we {disfmarker} we have to think about it .
Turn 367, B (PhD):  It 's {disfmarker}
Turn 368, G (Professor): And that we , so , introduce again some natural behavior in this trajectory .
Turn 369, D (Professor): Mm - hmm .
Turn 370, B (PhD): Mm - hmm . Very different from speech . Still , I mean , it shouldn't confuse the {disfmarker}
Turn 371, G (Professor): Yeah , I mean , similar to what {disfmarker} what you see really u in {disfmarker} in the real um noisy situation .
Turn 372, B (PhD): OK . Mm - hmm .
Turn 373, G (Professor): Or i in the clean situation . But {disfmarker} but somehow a {disfmarker} a natural distribution .
Turn 374, D (Professor): But isn't that s again sort of the idea of the additive thing , if it {disfmarker} as {disfmarker} as we had in the J stuff ? I mean , basically if {disfmarker} {vocalsound} if you have random data , um , in {disfmarker} in the time domain , then when you look at the s spectrum it 's gonna be pretty flat . And {disfmarker} and ,
Turn 375, G (Professor): Mm - hmm .
Turn 376, D (Professor): uh , so just add something everywhere rather than just in those places . It 's just a constant , right ?
Turn 377, E (PhD): Mm - hmm .
Turn 378, G (Professor): Yeah . I think {disfmarker} e yeah . It 's {disfmarker} it 's just especially in these segments , I mean , you introduce , um , very artificial behavior .
Turn 379, D (Professor): Yeah . Yeah .
Turn 380, G (Professor): And {disfmarker}
Turn 381, D (Professor): Well , see if you add something everywhere , it has almost no effect up {disfmarker} up {disfmarker} up on {disfmarker} on top . And it {disfmarker} and it {disfmarker} and it has significant effect down there .
Turn 382, E (PhD): Mm - hmm .
Turn 383, D (Professor): That was , sort of the idea .
Turn 384, G (Professor): Mm - hmm .
Turn 385, B (PhD): Hmm . Yeah the {disfmarker} that 's true . That {disfmarker} those {disfmarker} those regions are the cause for this @ @ {disfmarker} those negative values or whatever you get .
Turn 386, G (Professor): I Mm - hmm . Mm - hmm .
Turn 387, B (PhD): Yeah . So .
Turn 388, G (Professor): I mean , we {disfmarker} we could trit uh , we {disfmarker} we could think how w what {disfmarker} what we could try .
Turn 389, B (PhD): Yeah . Yeah , yeah .
Turn 390, G (Professor): I mean , {vocalsound} it {disfmarker} it was just an idea .
Turn 391, E (PhD): Mm - hmm .
Turn 392, G (Professor): I mean , we {disfmarker}
Turn 393, D (Professor): I think when it 's noisy people should just speak up .
Turn 394, G (Professor): to {disfmarker} Mmm .
Turn 395, B (PhD): So {disfmarker}
Turn 396, E (PhD): If we look at the France Telecom proposal , they use some kind of noise addition . They have a random number generator , right ? And they add noise on the trajectory of , uh , the log energy only , right ?
Turn 397, D (Professor): Oh , they do !
Turn 398, B (PhD): Yep .
Turn 399, D (Professor): Oh .
Turn 400, B (PhD): C - z C - zero and log energy also , yeah .
Turn 401, E (PhD): Yeah . Um , But I don't know how much effect it {disfmarker} this have , but they do that .
Turn 402, B (PhD): Now ?
Turn 403, E (PhD): Yeah .
Turn 404, B (PhD): Oh .
Turn 405, G (Professor): Uh - huh .
Turn 406, D (Professor): Hmm .
Turn 407, G (Professor): So it {disfmarker} it {disfmarker} it {disfmarker} it {disfmarker} it is l somehow similar to what {disfmarker}
Turn 408, E (PhD): I think because they have th log energy , yeah , and then just generate random number . They have some kind of mean and variance , and they add this number to {disfmarker} to the log energy simply . Um {disfmarker}
Turn 409, B (PhD): Yeah {disfmarker} the {disfmarker} the log energy , the {disfmarker} after the clean {disfmarker} cleaning up .
Turn 410, D (Professor): To the l
Turn 411, B (PhD): So they add a random {disfmarker} random noise to it .
Turn 412, E (PhD): Mm - hmm .
Turn 413, D (Professor): To the {disfmarker} just the energy , or to the mel {disfmarker} uh , to the mel filter ?
Turn 414, B (PhD): No . On - only to the log energy .
Turn 415, E (PhD): Only {disfmarker} Yeah .
Turn 416, D (Professor): Oh .
Turn 417, G (Professor): Uh - huh .
Turn 418, D (Professor): So it {disfmarker} Cuz I mean , I think this is most interesting for the mel filters . Right ?
Turn 419, G (Professor): Uh - huh .
Turn 420, D (Professor): Or {disfmarker} or F F one or the other .
Turn 421, G (Professor): But {disfmarker} but they do not apply filtering of the log energy or what {disfmarker}
Turn 422, B (PhD): Like , uh {disfmarker} I mean {disfmarker}
Turn 423, G (Professor): like {disfmarker} like a spectral subtraction or {disfmarker}
Turn 424, B (PhD): No {disfmarker} their filter is not M domain . S so they did filter their time signal
Turn 425, G (Professor): Yeah . I kn
Turn 426, B (PhD): and then what @ @ {disfmarker} u
Turn 427, G (Professor): And then they calculate from this , the log energy
Turn 428, B (PhD): Yeah {disfmarker} then after that it is s almost the same as the baseline prop system .
Turn 429, G (Professor): or {disfmarker} ? Mm - hmm .
Turn 430, B (PhD): And then the final log energy that they {disfmarker} that they get , that {disfmarker} to the {disfmarker} to that they add some random noise .
Turn 431, D (Professor): Yeah , but again , that 's just log energy as opposed to {vocalsound} filter bank energy .
Turn 432, B (PhD): Yeah . So it 's not the mel .
Turn 433, G (Professor): Mmm .
Turn 434, B (PhD): You know , it 's not the mel filter bank output .
Turn 435, D (Professor): Yeah .
Turn 436, B (PhD): These are log energy computed from the time s domain signal ,
Turn 437, G (Professor): Mm - hmm .
Turn 438, E (PhD): Mm - hmm .
Turn 439, B (PhD): not from the mel filter banks . So {disfmarker} did {disfmarker}
Turn 440, D (Professor): Hmm .
Turn 441, E (PhD): Maybe it 's just a way to decrease the importance of this particular parameter in the {disfmarker} in the world feature vector cu if you add noise to one of the parameters , you widen the distributions
Turn 442, D (Professor): Hmm .
Turn 443, B (PhD): Becomes flat . The variance , yeah , reduces ,
Turn 444, E (PhD): and {disfmarker}
Turn 445, B (PhD): so . Hmm , yeah .
Turn 446, E (PhD): Eee - sss - uh .
Turn 447, D (Professor): So it could reduce the dependence on the amplitude and so on . Yeah .
Turn 448, E (PhD): Yeah .
Turn 449, B (PhD): Yeah . Although {disfmarker}
Turn 450, D (Professor): Maybe .
Turn 451, E (PhD): Mm - hmm .
Turn 452, F (PhD): So is , uh {disfmarker} Is that about it ?
Turn 453, B (PhD): Uh , so the {disfmarker}
Turn 454, F (PhD): Or {disfmarker} ?
Turn 455, B (PhD): OK . So the other thing is the {disfmarker} I 'm just looking at a little bit on the delay issue where the delay of the system is like a hundred and eighty millisecond . So {vocalsound} I just {disfmarker} just tried another sk system {disfmarker} I mean , another filter which I 've like shown at the end . Which is very similar to the existing uh , filter . Only {disfmarker} Uh , only thing is that the phase is {disfmarker} is like a totally nonlinear phase because it 's a {disfmarker} it 's not a symmetric filter anymore .
Turn 456, F (PhD): This is for the LDA ?
Turn 457, B (PhD): Yeah {disfmarker} so {disfmarker} so this {disfmarker} this is like {disfmarker} So this makes the delay like zero for LDA because it 's completely causal .
Turn 458, F (PhD): Oh .
Turn 459, B (PhD): So {disfmarker} So I got actually just the results for the Italian for that and that 's like {disfmarker} So the fifty - one point O nine has become forty - eight point O six , which is like three percent relative degradation . So I have like the fifty - one point O nine
Turn 460, E (PhD): Mm - hmm .
Turn 461, B (PhD): and {disfmarker} So . I don't know it f fares for the other conditions . So it 's just like {disfmarker} it 's like a three percent relative degradation , with the {disfmarker}
Turn 462, G (Professor): But {disfmarker} but is there {disfmarker} is there a problem with the one hundred eighty milliseconds ? Or {disfmarker} ?
Turn 463, B (PhD): u Uh , may
Turn 464, D (Professor): Th - Well , this is {disfmarker}
Turn 465, G (Professor): Yeah , I mean , I talked to {disfmarker} to {disfmarker} uh , I ta Uh , I talked , uh , about it with {disfmarker} with Hynek . I mean , there is {disfmarker}
Turn 466, D (Professor): This is {disfmarker} So {disfmarker} So , basically our {disfmarker} our position is {vocalsound} that , um , we shouldn't be unduly constraining the latency at this point because we 're all still experimenting with trying to make the performance better in the presence of noise . Uh , there is a minority in that group who is a arguing {disfmarker} who are arguing for {vocalsound} um , uh , having a further constraining of the latency . So we 're s just continuing to keep aware of what the trade - offs are and , you know , what {disfmarker} what do we gain from having longer or shorter latencies ?
Turn 467, G (Professor): Mmm .
Turn 468, D (Professor): But since we always seem to at least get something out of longer latencies not being so constrained , we 're tending to go with that if we 're not told we can't do it .
Turn 469, F (PhD): What {disfmarker} where was the , um {disfmarker} the smallest latency of all the systems last time ?
Turn 470, G (Professor): Mm - hmm .
Turn 471, B (PhD): The French Telecom .
Turn 472, D (Professor): Well , France Telecom was {disfmarker} was {disfmarker} was very short latency
Turn 473, G (Professor): It 's {disfmarker}
Turn 474, D (Professor): and they had a very good result .
Turn 475, F (PhD): What {disfmarker} what was it ?
Turn 476, D (Professor): It was thirty - five .
Turn 477, G (Professor): It was in the order of thirty milliseconds
Turn 478, D (Professor): Yeah .
Turn 479, G (Professor): or {disfmarker}
Turn 480, F (PhD): Thirteen ?
Turn 481, D (Professor): th th
Turn 482, G (Professor): Thirty .
Turn 483, F (PhD): Thirty .
Turn 484, B (PhD): Thirty - four .
Turn 485, D (Professor): Yeah .
Turn 486, G (Professor): Yeah .
Turn 487, D (Professor): Yeah , so it 's possible to get very short latency .
Turn 488, G (Professor): 
Turn 489, D (Professor): But , again , we 're {disfmarker} the {disfmarker} the approaches that we 're using are ones that {vocalsound} take advantage of {disfmarker}
Turn 490, F (PhD): Yeah . I was just curious about where we are compared to , you know , the shortest that people have done .
Turn 491, G (Professor): But {disfmarker} but I think this thirty milliseconds {disfmarker} they {disfmarker} they did {disfmarker} it did not include the {disfmarker} the delta calculation .
Turn 492, B (PhD): Yeah . Yeah . Yeah .
Turn 493, G (Professor): And this is included now ,
Turn 494, B (PhD): Yeah . Yeah .
Turn 495, G (Professor): you know ?
Turn 496, B (PhD): So if they include the delta , it will be an additional forty millisecond .
Turn 497, E (PhD): Mm - hmm .
Turn 498, D (Professor): Yeah .
Turn 499, G (Professor): Yeah . I {disfmarker} I don't remember the {disfmarker} i th They were not using the HTK delta ?
Turn 500, B (PhD): No , they 're using a nine - point window , which is like a four on either side ,
Turn 501, G (Professor): Nine - point .
Turn 502, B (PhD): which is like {disfmarker}
Turn 503, G (Professor): OK .
Turn 504, B (PhD): f so {disfmarker}
Turn 505, G (Professor): Mmm .
Turn 506, B (PhD): they didn't include that .
Turn 507, D (Professor): Yeah .
Turn 508, G (Professor): Mm - hmm .
Turn 509, B (PhD): So {disfmarker}
Turn 510, E (PhD): Where does the comprish compression in decoding delay comes from ?
Turn 511, F (PhD): OK .
Turn 512, E (PhD): 
Turn 513, B (PhD): That 's the way the {disfmarker} the {disfmarker} the frames are packed , like you have to wait for one more frame to pack . Because it 's {disfmarker} the CRC is computed for two frames always .
Turn 514, D (Professor): Well , that {disfmarker} the they would need that forty milliseconds also .
Turn 515, E (PhD): Mm - hmm .
Turn 516, B (PhD): No . They actually changed the compression scheme altogether .
Turn 517, D (Professor): Right ?
Turn 518, E (PhD): Mm - hmm .
Turn 519, B (PhD): So they have their own compression and decoding scheme and they {disfmarker} I don't know what they have .
Turn 520, D (Professor): Oh .
Turn 521, B (PhD): But they have coded zero delay for that . Because they ch I know they changed it , their compression . They have their own CRC , their {disfmarker} their own {vocalsound} error correction mechanism .
Turn 522, D (Professor): Oh .
Turn 523, B (PhD): So they don't have to wait more than one more frame to know whether the current frame is in error .
Turn 524, D (Professor): Oh , OK .
Turn 525, B (PhD): So they changed the whole thing so that there 's no delay for that compression and {disfmarker} part also .
Turn 526, D (Professor): Hmm .
Turn 527, G (Professor): Mm - hmm .
Turn 528, B (PhD): Even you have reported actually zero delay for the {pause} compression . I thought maybe you also have some different {disfmarker}
Turn 529, G (Professor): Mmm . Mmm . No , I think I {disfmarker} I used this scheme as it was before .
Turn 530, B (PhD): OK . Ah . Mm - hmm .
Turn 531, F (PhD): OK , we 've got twenty minutes so we should {vocalsound} probably try to move along . Uh , did you wanna go next , Stephane ?
Turn 532, E (PhD): I can go next . Yeah . Mmm .
Turn 533, D (Professor): Oh . Wait a minute . It 's {disfmarker}
Turn 534, E (PhD): It 's {disfmarker} Yeah , we have to take {disfmarker}
Turn 535, D (Professor): Wait a minute . I think {vocalsound} I 'm confused .
Turn 536, E (PhD): Well {disfmarker} OK .
Turn 537, D (Professor): Alright .
Turn 538, E (PhD): So you have w w one sheet ? This one is {disfmarker} you don't need it , alright .
Turn 539, D (Professor): Uh {disfmarker}
Turn 540, E (PhD): So you have to take the whole {disfmarker} the five . There should be five sheets .
Turn 541, D (Professor): OK ,
Turn 542, E (PhD): 
Turn 543, D (Professor): I have four now because I left one with Dave because I thought I was dropping one off and passing the others on . So , no , we 're not . OK .
Turn 544, B (PhD): Thanks .
Turn 545, H (PhD): Please give me one .
Turn 546, D (Professor): Ah , we need one more over here .
Turn 547, E (PhD): OK , maybe there 's not enough for everybody .
Turn 548, F (PhD): I can share with Barry .
Turn 549, A (Grad): Yeah .
Turn 550, D (Professor): Oh , OK .
Turn 551, E (PhD): But {disfmarker} Can we look at this ?
Turn 552, G (Professor): OK .
Turn 553, C (Grad): Yeah .
Turn 554, E (PhD): So , yeah , there are two figures showing actually the , mmm , um , performance of the current VAD . So it 's a n neural network based on PLP parameters , uh , which estimate silence probabilities , and then I just put a median filtering on this to smooth the probabilities , right ? Um {disfmarker} I didn't use the {disfmarker} the scheme that 's currently in the proposal because {vocalsound} I don't want to {disfmarker} In the proposal {disfmarker} Well , in {disfmarker} in the system we want to add like speech frame before every word and a little bit of {disfmarker} of , uh , s a couple of frames after also . Uh , but to estimate the performance of the VAD , we don't want to do that , because it would artificially increase the um {disfmarker} the false alarm rate of speech detection . Right ? Um , so , there is u normally a figure for the Finnish and one for Italian . And maybe someone has two for the Italian because I 'm missing one figure here .
Turn 555, B (PhD): No .
Turn 556, E (PhD): Well {disfmarker} Well , whatever . Uh {disfmarker} Yeah , so one surprising thing that we can notice first is that apparently the speech miss rate is uh , higher than the false alarm rate . So . It means {disfmarker}
Turn 557, G (Professor): So {disfmarker} so what is the lower curve and the upper curve ?
Turn 558, E (PhD): Mm - hmm . Yeah , there are two curves . One curve 's for the close - talking microphone , which is the lower curve .
Turn 559, G (Professor): Yeah .
Turn 560, E (PhD): And the other one is for the distant microphone
Turn 561, G (Professor): Ah , OK .
Turn 562, E (PhD): which has more noise so , it 's logical that {vocalsound} it performs worse . So as I was saying , the miss rate is quite important uh , which means that we tend to label speech as {disfmarker} as a silence . And , uh , I didn't analyze further yet , but {vocalsound} I think it 's {disfmarker} it may be due to the fricative sounds which may be {disfmarker} in noisy condition maybe label {disfmarker} labelled as silence . And it may also be due to the alignment because {disfmarker} well , the reference alignment . Because right now I just use an alignment obtained from {disfmarker} from a system trained on channel zero . And I checked it a little bit but there might be alignment errors . Um , yeah , e like the fact that {vocalsound} {vocalsound} the {disfmarker} the models tend to align their first state on silence and their last state o on silence also . So the reference {disfmarker} reference alignment would label as speech some silence frame before speech and after speech . This is something that we already noticed before when {disfmarker} mmm , So this cus this could also explain , uh , the high miss rate maybe . Uh {disfmarker}
Turn 563, G (Professor): And {disfmarker} and this {disfmarker} this curves are the average over the whole database , so .
Turn 564, E (PhD): Yeah . Right .
Turn 565, G (Professor): Mmm .
Turn 566, E (PhD): Um {disfmarker} Yeah , and the different points of the curves are for five uh , thresholds on the probability {comment} uh from point three to point seven .
Turn 567, B (PhD): So that threshold {disfmarker}
Turn 568, E (PhD): Mm - hmm . Yeah .
Turn 569, B (PhD): OK . S OK {disfmarker} so d the detection threshold is very {disfmarker}
Turn 570, E (PhD): So the v
Turn 571, B (PhD): Yeah , yeah .
Turn 572, E (PhD): The VAD ? Yeah . There first , a threshold on the probability {comment} @ @ {comment} That puts all the values to zero or one .
Turn 573, B (PhD): Mmm .
Turn 574, E (PhD): And then the median filtering .
Turn 575, B (PhD): Yeah , so the median filtering is fixed . You just change the threshold ?
Turn 576, E (PhD): Yeah . It 's fixed ,
Turn 577, B (PhD): Yeah .
Turn 578, E (PhD): yeah . Mm - hmm . So , going from channel zero to channel one , uh , almost double the error rate . Um , Yeah . Well , so it 's a reference performance that we can {disfmarker} you know , if we want to {disfmarker} to work on the VAD , {comment} we can work on this basis
Turn 579, H (PhD): 
Turn 580, B (PhD): Mm - hmm .
Turn 581, E (PhD): and {disfmarker}
Turn 582, B (PhD): OK .
Turn 583, A (Grad): Is this {disfmarker} is this VAD a MLP ?
Turn 584, E (PhD): Yeah .
Turn 585, A (Grad): OK . How {disfmarker} how big is it ?
Turn 586, E (PhD): It 's a very big one . I don't remember .
Turn 587, B (PhD): So three {disfmarker} three hundred and fifty inputs ,
Turn 588, E (PhD): m
Turn 589, B (PhD): uh , six thousand hidden nodes and two outputs . t t
Turn 590, A (Grad): OK .
Turn 591, B (PhD): Yeah .
Turn 592, E (PhD): Mm - hmm .
Turn 593, D (Professor): Middle - sized one .
Turn 594, B (PhD): Yeah .
Turn 595, E (PhD): Mm - hmm .
Turn 596, B (PhD): 
Turn 597, E (PhD): Yeah . Uh , ppp . I don't know , you have questions about that , or suggestions ?
Turn 598, B (PhD): Mmm . S so {disfmarker}
Turn 599, E (PhD): It seems {disfmarker} the performance seems worse in Finnish , which {disfmarker}
Turn 600, B (PhD): Well , it 's not trained on Finnish .
Turn 601, E (PhD): uh {disfmarker}
Turn 602, H (PhD): It 's worse .
Turn 603, E (PhD): It 's not trained on Finnish , yeah .
Turn 604, D (Professor): What 's it trained on ?
Turn 605, B (PhD): I mean , the MLP 's not trained on Finnish .
Turn 606, D (Professor): Right , what 's it trained on ?
Turn 607, B (PhD): Oh {disfmarker} oh . Sorry . Uh , it 's Italian TI - digits .
Turn 608, D (Professor): Yeah . Oh , it 's trained on Italian ?
Turn 609, B (PhD): Yeah .
Turn 610, D (Professor): Yeah , OK .
Turn 611, E (PhD): Mm - hmm . And {disfmarker}
Turn 612, B (PhD): That 's right .
Turn 613, D (Professor): OK .
Turn 614, E (PhD): And also there are like funny noises on Finnish more than on Italian . I mean , like music
Turn 615, G (Professor): Mm - hmm .
Turn 616, B (PhD): Yeah . Yeah , the {disfmarker} Yeah , it 's true .
Turn 617, E (PhD): and {vocalsound} um {disfmarker} So , yeah , we were looking at this . But for most of the noises , noises are {disfmarker} um , I don't know if we want to talk about that . But , well , the {disfmarker} the " Car " noises are below like five hundred hertz . And we were looking at the " Music " utterances and in this case the noise is more about two thousand hertz .
Turn 618, B (PhD): Yeah .
Turn 619, E (PhD): Well , the music energy 's very low apparently . Uh , uh , from zero to two {disfmarker} two thousand hertz . So maybe just looking at this frequency range for {disfmarker} from five hundred to two thousand would improve somewhat the VAD
Turn 620, B (PhD): Mmm .
Turn 621, E (PhD): and {disfmarker}
Turn 622, B (PhD): Yeah .
Turn 623, E (PhD): Mmm {disfmarker}
Turn 624, B (PhD): So there are like some {disfmarker} some s some parameters you wanted to use or something ?
Turn 625, E (PhD): Yeah , but {disfmarker} Yes .
Turn 626, B (PhD): Or {disfmarker} Yeah .
Turn 627, E (PhD): Mm - hmm . Uh , the next , um {disfmarker} Oh , it 's there .
Turn 628, G (Professor): So is the {disfmarker} is the {disfmarker} is the training {disfmarker} is the training based on these labels files which you take as reference here ?
Turn 629, E (PhD): Yeah .
Turn 630, G (Professor): Wh - when you train the neural net y y you {disfmarker}
Turn 631, E (PhD): No . It 's not . It 's {disfmarker} it was trained on some alignment obtained um , uh {disfmarker} For the Italian data , I think we trained the neural network on {disfmarker} with embedded training . So re - estimation of the alignment using the neural network , I guess . That 's right ?
Turn 632, B (PhD): Yeah . We actually trained , uh , the {disfmarker} on the Italian training part .
Turn 633, E (PhD): Yeah .
Turn 634, B (PhD): We {disfmarker} we had another {vocalsound} system with u
Turn 635, E (PhD): So it was a f f a phonetic classification system for the Italian Aurora data .
Turn 636, B (PhD): Yeah . It must be somewhere . Yeah .
Turn 637, E (PhD): For the Aurora data that it was trained on , it was different . Like , for TI - digits you used a {disfmarker} a previous system that you had , I guess .
Turn 638, B (PhD): What {disfmarker}  No it {disfmarker} Yeah , yeah . That 's true .
Turn 639, E (PhD): So the alignments from the different database that are used for training came from different system .
Turn 640, B (PhD): Syste Yeah .
Turn 641, E (PhD): Then we put them tog together . Well , you put them together and trained the VAD on them .
Turn 642, B (PhD): Yeah .
Turn 643, E (PhD): Mmm .
Turn 644, B (PhD): Yeah .
Turn 645, G (Professor): Hmm .
Turn 646, E (PhD): Uh , But did you use channel {disfmarker} did you align channel one also ? Or {disfmarker}
Turn 647, B (PhD): I just took their entire Italian training part .
Turn 648, E (PhD): Yeah .
Turn 649, B (PhD): So it was both channel zero plus channel one .
Turn 650, E (PhD): So di Yeah . So the alignments might be wrong then on channel one , right ?
Turn 651, B (PhD): On one . Possible .
Turn 652, E (PhD): So we might ,
Turn 653, B (PhD): We can do a realignment .
Turn 654, E (PhD): yeah ,
Turn 655, B (PhD): That 's true .
Turn 656, E (PhD): at least want to retrain on these alignments , which should be better because they come from close - talking microphone .
Turn 657, G (Professor): Yeah , the {disfmarker} that was my idea . I mean , if {disfmarker} if it ha if it is not the same labeling which is taking the spaces .
Turn 658, B (PhD): Yeah .
Turn 659, E (PhD): OK .
Turn 660, B (PhD): Yeah , possible .
Turn 661, E (PhD): Yeah .
Turn 662, G (Professor): Mmm .
Turn 663, B (PhD): I mean , it {disfmarker} so the system {disfmarker}
Turn 664, E (PhD): Yeah .
Turn 665, B (PhD): so the VAD was trained on maybe different set of labels for channel zero and channel one
Turn 666, E (PhD): Mm - hmm .
Turn 667, B (PhD): and {disfmarker}
Turn 668, G (Professor): Mm - hmm .
Turn 669, B (PhD): was the alignments were w were different for {disfmarker} s certainly different because they were independently trained .
Turn 670, E (PhD): Mm - hmm .
Turn 671, B (PhD): We didn't copy the channel zero alignments to channel one .
Turn 672, E (PhD): Mm - hmm .
Turn 673, G (Professor): Mm - hmm .
Turn 674, B (PhD): Yeah .
Turn 675, E (PhD): Yeah .
Turn 676, B (PhD): But for the new alignments what you generated , you just copied the channel zero to channel one , right ? Yeah .
Turn 677, E (PhD): Right . Yeah . Um . And eh , hhh actually when we look at {disfmarker} at the VAD , {vocalsound} for some utterances it 's almost perfect , I mean , it just dropped one frame , the first frame of speech or {disfmarker} So there are some utterances where it 's almost one hundred percent VAD performance .
Turn 678, G (Professor): Hmm .
Turn 679, E (PhD): Uh , but {disfmarker} Yeah . Mmm {disfmarker} Yep . So the next thing is um , I have the spreadsheet for three different system . But for this you only have to look right now on the SpeechDat - Car performance uh , because I didn't test {disfmarker} so {disfmarker} I didn't test the spectral subtraction on TI - digits yet . Uh , so you have three she sheets . One is the um proposal - one system . Actually , it 's not exe exactly proposal - one . It 's the system that Sunil just described . Um , but with uh , Wiener filtering from um , France Telecom included . Um , so this gives like fifty - seven point seven percent , uh , s uh , error rate reduction on the SpeechDat - Car data . Mmm , and then I have two sheets where it 's for a system where {disfmarker} uh , so it 's again the same system . But in this case we have spectral subtraction with a maximum overestimation factor of two point five . Uh , there is smoothing of the gain trajectory with some kind of uh , low - pass filter , which has forty milliseconds latency . And then , after subtraction um , I add a constant to the energies and I have two cases d where {disfmarker} The first case is where the constant is twenty - five DB below the mean speech energy and the other is thirty DB below . Um , and for these s two system we have like fifty - five point , uh , five - percent improvement , and fifty - eight point one . So again , it 's around fifty - six , fifty - seven . Uh {disfmarker}
Turn 680, D (Professor): Cuz I notice the TI - digits number is exactly the same for these last two ?
Turn 681, E (PhD): Yeah , because I didn't {disfmarker} For the France Telecom uh , spectral subtraction included in the {disfmarker} our system , the TI - digits number are the right one , but not for the other system because I didn't test it yet {disfmarker} this system , including {disfmarker} with spectral subtraction on the TI - digits data . I just tested it on SpeechDat - Car .
Turn 682, D (Professor): Ah ! So {disfmarker} so that means the only thing {disfmarker}
Turn 683, G (Professor): Mm - hmm . So {disfmarker} so {disfmarker} so these numbers are simply {disfmarker}
Turn 684, E (PhD): This , we have to {disfmarker} Yeah .
Turn 685, B (PhD): But this number .
Turn 686, D (Professor): Yeah .
Turn 687, G (Professor): Yeah .
Turn 688, D (Professor): So you {disfmarker} so you just should look at that fifty - eight perc point O nine percent and so on .
Turn 689, E (PhD): Yes .
Turn 690, G (Professor): OK .
Turn 691, E (PhD): Right . Right .
Turn 692, D (Professor): OK . Good .
Turn 693, E (PhD): Mm - hmm . Um , Yeah .
Turn 694, B (PhD): So this {disfmarker} So by {disfmarker} uh , by {disfmarker} by reducing the noise a {disfmarker} a decent threshold like minus thirty DB , it 's like {disfmarker} Uh , you are like r r reducing the floor of the noisy regions , right ?
Turn 695, G (Professor): s
Turn 696, E (PhD): Yeah . Yeah . The floor is lower . Um ,
Turn 697, B (PhD): Uh - huh .
Turn 698, E (PhD): mm - hmm .
Turn 699, D (Professor): I 'm sorry . So when you say minus twenty - five or minus thirty DB , with respect to what ?
Turn 700, E (PhD): To the average um , speech energy which is estimated on the world database .
Turn 701, D (Professor): OK , so basically you 're creating a signal - to - noise ratio of twenty - five or thirty DB ?
Turn 702, E (PhD): Yeah .
Turn 703, D (Professor): uh r
Turn 704, E (PhD): But it 's not {disfmarker}
Turn 705, G (Professor): I {disfmarker} I {disfmarker} I think what you do is this .
Turn 706, E (PhD): it {disfmarker} it 's {disfmarker}
Turn 707, G (Professor): i When {disfmarker} when you have this , {vocalsound} after you subtracted it , I mean , then you get something w w with this , uh , where you set the values to zero and then you simply add an additive constant again .
Turn 708, E (PhD): Yeah .
Turn 709, G (Professor): So you shift it somehow . This {disfmarker} this whole curve is shifted again .
Turn 710, D (Professor): But did you do that before the thresholding to zero ,
Turn 711, E (PhD): Right . It 's {disfmarker}
Turn 712, D (Professor): or {disfmarker} ?
Turn 713, E (PhD): But , it 's after the thresholding .
Turn 714, G (Professor): 
Turn 715, D (Professor): Oh ,
Turn 716, E (PhD): So ,
Turn 717, D (Professor): so you 'd really want to do it before ,
Turn 718, E (PhD): maybe {disfmarker}
Turn 719, D (Professor): right ?
Turn 720, E (PhD): maybe we might do it before ,
Turn 721, D (Professor): Yeah , because then the {disfmarker} then you would have less of that phenomenon .
Turn 722, E (PhD): yeah . Yeah .
Turn 723, D (Professor): I think .
Turn 724, E (PhD): Uh {disfmarker}
Turn 725, G (Professor): E Hhh .
Turn 726, E (PhD): Yeah .
Turn 727, D (Professor): c
Turn 728, E (PhD): But still , when you do this and you take the log after that , it {disfmarker} it reduce the {disfmarker} the variance .
Turn 729, D (Professor): Yeah , it {disfmarker} it {disfmarker} Right .
Turn 730, E (PhD): But {disfmarker} Mmm ,
Turn 731, D (Professor): Yeah , that will reduce the variance . That 'll help . But maybe if you does {disfmarker} do it before you get less of these funny - looking things he 's drawing .
Turn 732, G (Professor): Mm - hmm .
Turn 733, E (PhD): Um ,
Turn 734, B (PhD): So before it 's like adding this , col to the {disfmarker} to the {disfmarker} o exi original {disfmarker}
Turn 735, G (Professor): But {disfmarker} but {disfmarker}
Turn 736, E (PhD): We would {disfmarker}
Turn 737, D (Professor): Right at the point where you 've done the subtraction .
Turn 738, B (PhD): OK .
Turn 739, D (Professor): Um , essentially you 're adding a constant into everything .
Turn 740, E (PhD): Mm - hmm .
Turn 741, G (Professor): But the way Stephane did it , it is exactly the way I have implemented in the phone , so .
Turn 742, D (Professor): Oh , yeah , better do it different , then . Yeah .
Turn 743, E (PhD): Um .
Turn 744, D (Professor): Just you {disfmarker} you just ta you just set it for a particular signal - to - noise ratio that you want ?
Turn 745, E (PhD): Yeah .
Turn 746, G (Professor): Yeah I {disfmarker} I made s similar investigations like Stephane did here , just uh , adding this constant and {disfmarker} and looking how dependent is it on the value of the constant
Turn 747, D (Professor): Yeah . Yeah .
Turn 748, E (PhD): Mm - hmm .
Turn 749, G (Professor): and then , must choose them somehow {vocalsound} to give on average the best results for a certain range of the signal - to - noise ratios .
Turn 750, D (Professor): Uh - huh .
Turn 751, E (PhD): Yeah . Mm - hmm .
Turn 752, G (Professor): So {disfmarker}
Turn 753, E (PhD): Oh , it 's clear . I should have gi given other results . Also it 's clear when you don't add noise , it 's much worse . Like , around five percent worse I guess .
Turn 754, D (Professor): Uh - huh .
Turn 755, E (PhD): And if you add too much noise it get worse also . And it seems that {vocalsound} right now this {disfmarker} this is c a constant that does not depend on {disfmarker} {comment} on anything that you can learn from the utterance . It 's just a constant noise addition . Um . And I {disfmarker} I think w w
Turn 756, D (Professor): I {disfmarker} I 'm sorry . Then {disfmarker} then I 'm confused .
Turn 757, E (PhD): I think {disfmarker}
Turn 758, D (Professor): I thought {disfmarker} you 're saying it doesn't depend on the utterance but I thought you were adding an amount that was twenty - five DB down from the signal energy .
Turn 759, E (PhD): Yeah , so the way I did that , {comment} i I just measured the average speech energy of the {disfmarker} all the Italian data .
Turn 760, D (Professor): Oh !
Turn 761, E (PhD): And then {disfmarker} I {disfmarker} I have {disfmarker} I used this as mean speech energy . Mm - hmm .
Turn 762, D (Professor): Oh , it 's just a constant amount over all .
Turn 763, E (PhD): Yeah . And {disfmarker}
Turn 764, B (PhD): OK .
Turn 765, E (PhD): wha what I observed is that for Italian and Spanish , {comment} when you go to thirty and twenty - five DB , {comment} uh it {disfmarker} it 's good .
Turn 766, B (PhD): Oh .
Turn 767, E (PhD): It stays {disfmarker} In this range , it 's , uh , the p u well , the performance of the {disfmarker} this algorithm is quite good . But for Finnish , {vocalsound} you have a degradation already when you go from thirty - five to thirty and then from thirty to twenty - five . And {disfmarker} I have the feeling that maybe it 's because just Finnish has a mean energy that 's lower than {disfmarker} than the other databases . And due to this the thresholds should be {disfmarker}
Turn 768, D (Professor): Yeah .
Turn 769, E (PhD): the {disfmarker} the a the noise addition should be lower
Turn 770, D (Professor): But in {disfmarker} I mean , in the real thing you 're not gonna be able to measure what people are doing over half an hour or an hour , or anything , right ?
Turn 771, E (PhD): and {disfmarker}
Turn 772, D (Professor): So you have to come up with this number from something else .
Turn 773, E (PhD): Yeah . So {disfmarker}
Turn 774, G (Professor): Uh , but you are not doing it now language dependent ? Or {disfmarker} ?
Turn 775, E (PhD): It 's not . It 's just something that 's fixed .
Turn 776, G (Professor): No . It 's overall .
Turn 777, E (PhD): Yeah .
Turn 778, G (Professor): OK .
Turn 779, E (PhD): Mm - hmm . Um {disfmarker}
Turn 780, D (Professor): But what he is doing language dependent is measuring what that number i reference is that he comes down twenty - five down from .
Turn 781, E (PhD): Yeah , so I g No . It {disfmarker} No .
Turn 782, D (Professor): No ?
Turn 783, E (PhD): Because I did it {disfmarker} I started working on Italian . I obtained this average energy
Turn 784, D (Professor): Yeah .
Turn 785, E (PhD): and then I used this one .
Turn 786, B (PhD): For all the languages . OK .
Turn 787, E (PhD): Yeah .
Turn 788, D (Professor): So it 's sort of arbitrary .
Turn 789, B (PhD): Yeah .
Turn 790, D (Professor): I mean , so if y if {disfmarker} Yeah .
Turn 791, E (PhD): Yep .
Turn 792, D (Professor): Yeah .
Turn 793, E (PhD): Um , yeah , so the next thing is to use this as {disfmarker} as maybe initialization
Turn 794, D (Professor): Uh - huh .
Turn 795, E (PhD): and then use something on - line .
Turn 796, D (Professor): Something more adaptive ,
Turn 797, E (PhD): But {disfmarker} {vocalsound} And I expect improvement at least in Finnish because eh {disfmarker} the way {disfmarker}
Turn 798, D (Professor): yeah . OK .
Turn 799, E (PhD): Well , um , for Italian and Spanish it 's {disfmarker} th this value works good but not necessarily for Finnish . Mmm . But unfortunately there is , like , this forty millisecond latency and , um {disfmarker} Yeah , so I would try to somewhat reduce this @ @ . I already know that if I completely remove this latency , so . {vocalsound} um , {comment} it {disfmarker} um there is a three percent hit on Italian .
Turn 800, G (Professor): Mm - hmm .
Turn 801, B (PhD): d Does latency {disfmarker}
Turn 802, G (Professor): i
Turn 803, B (PhD): Sorry . Go ahead .
Turn 804, G (Professor): Yeah . Your {disfmarker} your smoothing was @ @ {comment} uh , over this s so to say , the {disfmarker} the factor of the Wiener . And then it 's , uh {disfmarker} What was it ? This {disfmarker}
Turn 805, E (PhD): Mm - hmm .
Turn 806, G (Professor): this smoothing , it was over the subtraction factor , so to say .
Turn 807, E (PhD): It 's a smoothing over the {disfmarker} the gain of the subtraction algorithm .
Turn 808, G (Professor): Was this done {disfmarker} Mm - hmm . And {disfmarker} and you are looking into the future , into the past .
Turn 809, E (PhD): Right .
Turn 810, G (Professor): And smoothing .
Turn 811, E (PhD): So , to smooth this {pause} thing .
Turn 812, G (Professor): Mm - hmm .
Turn 813, E (PhD): Yeah . Um {disfmarker}
Turn 814, G (Professor): And did {disfmarker} did you try simply to smooth um to smooth the {disfmarker} the {disfmarker} t to {disfmarker} to smooth stronger the {disfmarker} the envelope ?
Turn 815, E (PhD): Um , no , I did not .
Turn 816, G (Professor): Mmm .
Turn 817, E (PhD): Mmm .
Turn 818, G (Professor): Because I mean , it should have a similar effect if you {disfmarker}
Turn 819, E (PhD): Yeah .
Turn 820, G (Professor): I mean , you {disfmarker} you have now several stages of smoothing , so to say . You start up . As far as I remember you {disfmarker} you smooth somehow the envelope , you smooth somehow the noise estimate ,
Turn 821, E (PhD): Mm - hmm . Mmm {disfmarker}
Turn 822, G (Professor): and {disfmarker} {vocalsound} and later on you smooth also this subtraction factor .
Turn 823, E (PhD): Uh , no , it 's {disfmarker} it 's just the gain that 's smoothed actually
Turn 824, B (PhD): Uh , actually I d I do all the smoothing .
Turn 825, E (PhD): but it 's smoothed {disfmarker}
Turn 826, G (Professor): Ah . Oh , it w it was you .
Turn 827, B (PhD): Yeah , yeah .
Turn 828, E (PhD): Uh {disfmarker} Yeah .
Turn 829, G (Professor): Yeah .
Turn 830, E (PhD): Yeah . No , in this case it 's just the gain .
Turn 831, G (Professor): Yeah .
Turn 832, E (PhD): And {disfmarker}
Turn 833, G (Professor): Uh - huh .
Turn 834, E (PhD): But the way it 's done is that um , for low gain , there is this non nonlinear smoothing actually . For low gains um , I use the smoothed sm uh , smoothed version but {disfmarker} for high gain @ @ {comment} it 's {disfmarker} I don't smooth .
Turn 835, G (Professor): Uh . Mm - hmm . I just , uh {disfmarker} it {disfmarker} Experience shows you , if {disfmarker} if you do the {disfmarker}  The best is to do the smoo smoothing as early as possible .
Turn 836, E (PhD): Uh - huh .
Turn 837, G (Professor): So w when you start up . I mean , you start up with the {disfmarker} with the {disfmarker} somehow with the noisy envelope .
Turn 838, E (PhD): Mm - hmm .
Turn 839, G (Professor): And , best is to smooth this somehow .
Turn 840, E (PhD): Mm - hmm . Uh , yeah , I could try this . Um .
Turn 841, G (Professor): And {disfmarker}
Turn 842, B (PhD): So , before estimating the SNR , @ @ smooth the envelope .
Turn 843, G (Professor): Yeah . Yeah . Uh - huh .
Turn 844, E (PhD): Mm - hmm . But {disfmarker} Yeah . Then I {disfmarker} I would need to find a way to like smooth less also when there is high energy . Cuz I noticed that it {disfmarker} it helps a little bit to s like smooth more during low energy portions and less during speech ,
Turn 845, G (Professor): Yes , y
Turn 846, E (PhD): because if you smooth then y you kind of distort the speech .
Turn 847, G (Professor): Yeah . Yeah .
Turn 848, E (PhD): Um .
Turn 849, G (Professor): Right .
Turn 850, E (PhD): Mm - hmm .
Turn 851, G (Professor): Yeah , I think when w you {disfmarker} you could do it in this way that you say , if you {disfmarker} if I 'm {disfmarker} you have somehow a noise estimate ,
Turn 852, E (PhD): Mm - hmm .
Turn 853, G (Professor): and , if you say I 'm {disfmarker} I 'm {disfmarker} with my envelope I 'm close to this noise estimate ,
Turn 854, E (PhD): Yeah .
Turn 855, G (Professor): then you have a bad signal - to - noise ratio and then you {disfmarker} you would like to have a stronger smoothing .
Turn 856, E (PhD): Mm - hmm .
Turn 857, G (Professor): So you could {disfmarker} you could base it on your estimation of the signal - to - noise ratio on your actual {disfmarker}
Turn 858, E (PhD): Mm - hmm . Mm - hmm . Mmm .
Turn 859, B (PhD): Yeah , or some silence probability from the VAD if you have {disfmarker}
Turn 860, E (PhD): Um , yeah , but I don't trust {vocalsound} the current VAD . So .
Turn 861, B (PhD): Yeah , uh , so not {disfmarker} not right now maybe .
Turn 862, E (PhD): Well , maybe .
Turn 863, D (Professor): The VAD later will be much better .
Turn 864, E (PhD): Maybe .
Turn 865, D (Professor): Yeah . So . I see .
Turn 866, F (PhD): So is {pause} that it ?
Turn 867, E (PhD): Uh , fff {comment} I think that 's it . Yeah . Uh .
Turn 868, G (Professor): s So to summarize the performance of these , SpeechDat - Car results is similar than {disfmarker} than yours so to say .
Turn 869, B (PhD): Yeah , so the fifty - eight is like the be some fifty - six point {disfmarker}
Turn 870, E (PhD): Yeah .
Turn 871, G (Professor): Y you have {disfmarker} you have fifty - six point four
Turn 872, B (PhD): Yeah , that 's true .
Turn 873, G (Professor): and {disfmarker} and {disfmarker} {vocalsound} and dependent on this additive constant , it is s better or {disfmarker} or worse .
Turn 874, E (PhD): Yeah .
Turn 875, B (PhD): Slightly better .
Turn 876, E (PhD): Mm - hmm .
Turn 877, H (PhD):  
Turn 878, B (PhD): Yeah .
Turn 879, E (PhD): Mm - hmm .
Turn 880, G (Professor): Yeah .
Turn 881, E (PhD): And , {vocalsound} yeah , i i i the condition where it 's better than your approach , it 's {disfmarker} it {disfmarker} just because maybe it 's better on well matched and that the weight on well matched is {disfmarker} is bigger ,
Turn 882, B (PhD): Yeah . Yeah , you {disfmarker} you caught up .
Turn 883, E (PhD): because {disfmarker}
Turn 884, B (PhD): Yep , that 's true .
Turn 885, E (PhD): if you don't weigh differently the different condition , you can see that your {disfmarker} well , the win the two - stage Wiener filtering is maybe better or {disfmarker}
Turn 886, B (PhD): Yeah .
Turn 887, E (PhD): It 's better for high mismatch , right ?
Turn 888, B (PhD): Yeah , it 's better for high mismatch .
Turn 889, E (PhD): Mm - hmm . But a little bit worse for well matched .
Turn 890, B (PhD): So over all it gets , yeah , worse for the well matched condition , so y
Turn 891, E (PhD): Uh - huh .
Turn 892, F (PhD): So we need to combine these two .
Turn 893, B (PhD): Uh , that 's {disfmarker} that 's the best thing , is like the French Telecom system is optimized for the well matched condition . They c
Turn 894, E (PhD): Mm - hmm .
Turn 895, B (PhD): Yeah . So they know that the weighting is good for the well matched , and so there 's {disfmarker} everywhere the well matched 's s s performance is very good for the French Telecom .
Turn 896, E (PhD): Yeah .
Turn 897, G (Professor): Mm - hmm .
Turn 898, E (PhD): Mm - hmm .
Turn 899, B (PhD): T we are {disfmarker} we may also have to do something similar @ @ .
Turn 900, E (PhD): Mm - hmm .
Turn 901, D (Professor): Well , our tradition here has always been to focus on the mismatched .
Turn 902, B (PhD): Um the {disfmarker}
Turn 903, D (Professor): Cuz it 's more interesting .
Turn 904, G (Professor): Mu - my {disfmarker} mine was it too , I mean .
Turn 905, D (Professor): Yeah .
Turn 906, G (Professor): Before I started working on this Aurora .
Turn 907, D (Professor): Yeah .
Turn 908, G (Professor): so .
Turn 909, D (Professor): Yeah . Yeah . OK .
Turn 910, F (PhD): Carmen ? Do you , uh {disfmarker}
Turn 911, H (PhD): Well , I only say that the {disfmarker} this is , a summary of the {disfmarker} of all the VTS experiments and say that the result in the last {comment} um , for Italian {disfmarker} the last experiment for Italian , {vocalsound} are bad . I make a mistake when I write . Up at D I copy {vocalsound} one of the bad result .
Turn 912, B (PhD): So you {disfmarker}
Turn 913, H (PhD): And {disfmarker} There . {vocalsound} You know , this . Um , well . If we put everything , we improve a lot u the spectral use of the VTS but the final result {vocalsound} are not still mmm , good {vocalsound} like the Wiener filter for example . I don't know . Maybe it 's {disfmarker} @ @ {comment} it 's possible to {disfmarker} to have the same result .
Turn 914, B (PhD): That 's somewhere {disfmarker}
Turn 915, H (PhD): I don't know exactly . Mmm . Because I have , {vocalsound} mmm , {comment} worse result in medium mismatch and high mismatch .
Turn 916, B (PhD): You s you have a better r Yeah . You have some results that are good for the high mismatch .
Turn 917, H (PhD): And {disfmarker} Yeah . I someti are more or less similar but {disfmarker} but are worse . And still I don't have the result for TI - digits . The program is training . Maybe for this weekend I will have result TI - digits and I can complete that s like this . Well .
Turn 918, D (Professor): Uh . Right .
Turn 919, H (PhD): One thing that I {comment} note are not here in this result {vocalsound} but are speak {disfmarker} are spoken before with Sunil I {disfmarker} I improve my result using clean LDA filter .
Turn 920, B (PhD): Mm - hmm .
Turn 921, D (Professor): Mm - hmm .
Turn 922, H (PhD): If I use , {vocalsound} eh , the LDA filter that are training with the noisy speech , {vocalsound} that hurts the res my results .
Turn 923, D (Professor): So what are these numbers here ? Are these with the clean or with the noisy ?
Turn 924, H (PhD): This is with the clean .
Turn 925, D (Professor): OK .
Turn 926, H (PhD): With the noise I have worse result , that if I doesn't use it .
Turn 927, D (Professor): Uh - huh .
Turn 928, H (PhD): But m that may be because {vocalsound} with this technique {vocalsound} we are using really {disfmarker} really clean speech . The speech {disfmarker} the {comment} representation that go to the HTK is really clean speech because it 's from the dictionary , the code book and maybe from that . I don't know .
Turn 929, E (PhD): Mm - hmm .
Turn 930, H (PhD): Because I think that you {disfmarker} did some experiments using the two {disfmarker} the two LDA filter , clean and noi and noise ,
Turn 931, E (PhD): It 's {disfmarker}
Turn 932, H (PhD): and it doesn't matter too much .
Turn 933, E (PhD): Um , yeah , I did that but it doesn't matter on SpeechDat - Car , but , it matters , uh , a lot on TI - digits .
Turn 934, B (PhD): Using the clean filter .
Turn 935, H (PhD): It 's better to use clean .
Turn 936, E (PhD): Yeah , d uh , it 's much better when you {disfmarker} we used the clean derived LDA filter .
Turn 937, H (PhD): Mm - hmm . Maybe you can do d also this .
Turn 938, B (PhD): Yeah .
Turn 939, H (PhD): To use clean speech .
Turn 940, B (PhD): Yeah , I 'll try .
Turn 941, E (PhD): Uh , but , yeah , Sunil in {disfmarker} in your result it 's {disfmarker}
Turn 942, B (PhD): I {disfmarker} I 'll try the cle No , I {disfmarker} I {disfmarker} my result is with the noisy {disfmarker} noisy LDA .
Turn 943, E (PhD): It 's with the noisy one . Yeah .
Turn 944, B (PhD): Yeah .
Turn 945, D (Professor): Oh !
Turn 946, B (PhD): It 's with the noisy . Yeah . It 's {disfmarker} it 's not the clean LDA .
Turn 947, E (PhD): So {disfmarker}
Turn 948, D (Professor): Um {disfmarker}
Turn 949, B (PhD): It 's {disfmarker} In {disfmarker} in the front sheet , I have like {disfmarker} like the summary . Yeah .
Turn 950, D (Professor): And {disfmarker} and your result {comment} is with the {disfmarker}
Turn 951, E (PhD): It 's with the clean LDA .
Turn 952, B (PhD): Oh . This is {disfmarker} Your results are all with the clean LDA result ?
Turn 953, H (PhD): Yeah , with the clean LDA .
Turn 954, B (PhD): OK . @ @ .
Turn 955, E (PhD): Yeah .
Turn 956, B (PhD): 
Turn 957, E (PhD): And in your case it 's all {disfmarker} all noisy ,
Turn 958, H (PhD): Is that the reason ?
Turn 959, B (PhD): All noisy , yeah .
Turn 960, E (PhD): yeah . But {disfmarker}
Turn 961, H (PhD): And {disfmarker}
Turn 962, B (PhD): Uh {disfmarker} 
Turn 963, E (PhD): Yeah .
Turn 964, B (PhD): Uh {disfmarker}
Turn 965, E (PhD): But I observe my case it 's in , uh , uh , at least on SpeechDat - Car it doesn't matter but TI - digits it 's like two or three percent absolute , uh , {comment} better .
Turn 966, B (PhD): On TI - digits this matters . Absolute . Uh {disfmarker}
Turn 967, D (Professor): So you really might wanna try the clean I think .
Turn 968, E (PhD): So if {disfmarker}
Turn 969, B (PhD): Yeah , I {disfmarker} I {disfmarker} I will have to look at it . Yeah , that 's true .
Turn 970, D (Professor): Yeah . Yeah , that could be sizeable right there .
Turn 971, H (PhD): And this is everything .
Turn 972, G (Professor): Yeah .
Turn 973, D (Professor): OK .
Turn 974, G (Professor): Maybe you {disfmarker} you are leaving in {disfmarker} in about two weeks Carmen . No ?
Turn 975, H (PhD): Yeah .
Turn 976, G (Professor): Yeah . So I mean , if {disfmarker} if {disfmarker} if I would put it {disfmarker} put on the head of a project mana manager {disfmarker} I {disfmarker} I {disfmarker} I I would say , uh , um {disfmarker} I mean there is not so much time left now .
Turn 977, D (Professor): Be my guest .
Turn 978, G (Professor): I mean , if {disfmarker} {vocalsound} um , what {disfmarker} what I would do is I {disfmarker} I {disfmarker} I would pick @ @ {comment} the best consolation , which you think , and {vocalsound} c create {disfmarker} create all the results for the whole database that you get to the final number as {disfmarker} as Sunil did it
Turn 979, H (PhD): And prepare at the s
Turn 980, G (Professor): and {vocalsound} um and maybe also to {disfmarker} to write somehow a document where you describe your approach , and what you have done .
Turn 981, H (PhD): Yeah , I was thinking to do that next week .
Turn 982, D (Professor): Yeah .
Turn 983, G (Professor): Yeah .
Turn 984, D (Professor): Yeah , I 'll {disfmarker} I 'll borrow the head back and {disfmarker} and agree . Yeah ,
Turn 985, H (PhD): Yeah , I wi I {disfmarker} I will do that next week .
Turn 986, D (Professor): that 's {disfmarker} that 's {disfmarker} Right . In fact , actually I g I guess the , uh {disfmarker} the Spanish government , uh , requires that anyway . They want some kind of report from everybody who 's in the program .
Turn 987, H (PhD): Mm - hmm .
Turn 988, D (Professor): So . And of course I 'd {disfmarker} we 'd {disfmarker} we 'd like to see it too . So ,
Turn 989, H (PhD): OK .
Turn 990, D (Professor): yeah .
Turn 991, F (PhD): So , um , what 's {disfmarker} Do you think we , uh , should do the digits or skip it ? Or what are {disfmarker} what do you think ?
Turn 992, D (Professor): Uh , we have them now ?
Turn 993, F (PhD): Yeah , got them .
Turn 994, D (Professor): Uh , why don why don't we do it ?
Turn 995, F (PhD): OK .
Turn 996, D (Professor): Just {comment} {disfmarker} just take a minute .
Turn 997, H (PhD): I can send yet .
Turn 998, F (PhD): Would you pass those down ?
Turn 999, D (Professor): Oh ! Sorry .
Turn 1000, F (PhD): OK , um , so I guess I 'll go ahead . Um ,
Turn 1001, D (Professor): Seat ?
Turn 1002, E (PhD): Dave ? Is it the channel , or the mike ? I don't remember . It 's the mike ?
Turn 1003, D (Professor): Mike ?
Turn 1004, E (PhD): It 's not four .
Turn 1005, H (PhD): This is date and time . No . On the channel , channel .
Turn 1006, G (Professor): What is this ?
Turn 1007, B (PhD): t
Turn 1008, F (PhD): OK , if you could just leave , um , your mike on top of your , uh , digit form I can fill in any information that 's missing .
Turn 1009, G (Professor): OK .
Turn 1010, F (PhD): That 's uh {disfmarker} I didn't get a chance to fill them out ahead of time . Yeah , we 're gonna have to fix that . Uh , let 's see , it starts with one here , and then goes around and ends with nine here .
Turn 1011, A (Grad): Seven . So I {disfmarker} I 'm eight ,
Turn 1012, F (PhD): So he 's eight ,
Turn 1013, A (Grad): you 're seven .
Turn 1014, F (PhD): you 're seven ,
Turn 1015, A (Grad): Yeah .
